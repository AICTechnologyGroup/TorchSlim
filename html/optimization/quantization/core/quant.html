<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>optimization.quantization.core.quant API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>optimization.quantization.core.quant</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import torch
import copy
import logging
from optimization.common import Compressor, build_module_graph
from .utils import QuantType, BN_FOLD_TAG, BN_FOLD_OP
from torch.quantization import RecordingObserver as _RecordingObserver


_logger = logging.getLogger(__name__)

def _setattr(model, name, module):
    name_list = name.split(&#39;.&#39;)
    for name in name_list[:-1]:
        model = getattr(model, name)
    setattr(model, name_list[-1], module)

class RecordingObserver(_RecordingObserver):

    def forward(self, x):
        val = x.cpu()
        super().forward(val)
        return x

class Quantizer(Compressor):

    def __init__(self, model, config_list, optimizer=None, dummy_input=None):
        &#34;&#34;&#34;
        Base quantizer for pytorch quantizer
        &#34;&#34;&#34;
        if isinstance(model, torch.nn.DataParallel):
            model = model.module
        model_copied = copy.deepcopy(model)
        self.identity_wrappers = []
        self.conv_bn_patterns = {}
        self.find_conv_bn_patterns(model, dummy_input)
        super().__init__(model, config_list, optimizer)
        self.all_shapes = {}
        self.record_shape(model_copied, dummy_input)
        self.quant_grad = QuantGrad.apply
        if self.optimizer is not None:
            self.patch_optimizer(self.step_with_optimizer)
            for wrapper in self.get_modules_wrapper():
                if &#39;weight&#39; in wrapper.config[&#39;quant_types&#39;]:
                    self.optimizer.add_param_group({&#34;params&#34;: wrapper.module.old_weight})
                if hasattr(wrapper.module, &#34;old_bias&#34;):
                    self.optimizer.add_param_group({&#34;params&#34;: getattr(wrapper.module, &#34;old_bias&#34;)})

    def quantize_weight(self, wrapper, **kwargs):
        &#34;&#34;&#34;
        quantize should overload this method to quantize weight.
        This method is effectively hooked to :meth:`forward` of the model.
        Parameters
        ----------
        wrapper : QuantizerModuleWrapper
            the wrapper for origin module
        &#34;&#34;&#34;
        raise NotImplementedError(&#39;Quantizer must overload quantize_weight()&#39;)

    def quantize_output(self, output, wrapper, **kwargs):
        &#34;&#34;&#34;
        quantize should overload this method to quantize output.
        This method is effectively hooked to :meth:`forward` of the model.
        Parameters
        ----------
        output : Tensor
            output that needs to be quantized
        wrapper : QuantizerModuleWrapper
            the wrapper for origin module
        &#34;&#34;&#34;
        raise NotImplementedError(&#39;Quantizer must overload quantize_output()&#39;)

    def quantize_input(self, inputs, wrapper, **kwargs):
        &#34;&#34;&#34;
        quantize should overload this method to quantize input.
        This method is effectively hooked to :meth:`forward` of the model.
        Parameters
        ----------
        inputs : Tensor
            inputs that needs to be quantized
        wrapper : QuantizerModuleWrapper
            the wrapper for origin module
        &#34;&#34;&#34;
        raise NotImplementedError(&#39;Quantizer must overload quantize_input()&#39;)

    def fold_bn(self, *inputs, wrapper):
        &#34;&#34;&#34;
        Simulate batch normalization folding in the training graph. Folded weight and bias are
        returned for the following operations.

        Parameters
        ----------
        inputs : tuple of torch.Tensor
            inputs for the module
        wrapper : QuantizerModuleWrapper
            the wrapper for origin module

        Returns
        -------
        Tuple of torch.Tensor
        &#34;&#34;&#34;
        module = wrapper.module
        bn_module = wrapper.bn_module
        with torch.no_grad():
            output = module(*inputs)
            _ = bn_module(output)
        running_mean = bn_module.running_mean
        running_var = torch.sqrt(bn_module.running_var + bn_module.eps)
        bn_weight = bn_module.weight
        bn_bias = bn_module.bias
        dimensions = len(module.weight.shape)
        shape = [-1] + [1] * (dimensions - 1)
        new_weight = module.old_weight * bn_weight.reshape(shape) / running_var.reshape(shape)
        if hasattr(module, &#39;old_bias&#39;):
            new_bias = bn_bias + (module.old_bias - running_mean) / running_var * bn_weight
        else:
            new_bias = bn_bias - running_mean / running_var * bn_weight
        return new_weight, new_bias

    def _wrap_modules(self, layer, config):
        &#34;&#34;&#34;
        Create a wrapper forward function to replace the original one.
        Parameters
        ----------
        layer : LayerInfo
            the layer to instrument the mask
        config : dict
            the configuration for quantization
        &#34;&#34;&#34;
        assert &#39;quant_types&#39; in config, &#39;must provide quant_types in config&#39;
        assert isinstance(config[&#39;quant_types&#39;], list), &#39;quant_types must be list type&#39;
        assert &#39;quant_bits&#39; in config, &#39;must provide quant_bits in config&#39;
        assert isinstance(config[&#39;quant_bits&#39;], int) or isinstance(config[&#39;quant_bits&#39;], dict), &#39;quant_bits must be dict type or int type&#39;

        if isinstance(config[&#39;quant_bits&#39;], dict):
            for quant_type in config[&#39;quant_types&#39;]:
                assert quant_type in config[&#39;quant_bits&#39;], &#39;bits length for %s must be specified in quant_bits dict&#39; % quant_type

        bn_module = None
        if layer.name in self.conv_bn_patterns:
            bn_module_name = self.conv_bn_patterns[layer.name]
            for name, module in self.bound_model.named_modules():
                if name == bn_module_name:
                    bn_module = module
                    break
            assert bn_module is not None, &#34;BN module corresponding to layer {} is not found&#34;.format(layer.name)
            self.identity_wrappers.append(QuantizerIdentityWrapper(bn_module, bn_module_name))
        return QuantizerModuleWrapper(layer.module, layer.name, layer.type, config, self, bn_module)

    def _wrap_model(self):
        &#34;&#34;&#34;
        wrap all modules that needed to be compressed

        &#34;&#34;&#34;
        for wrapper in reversed(self.identity_wrappers):
            _setattr(self.bound_model, wrapper.module_name, wrapper)
        super()._wrap_model()

    def _unwrap_model(self):
        &#34;&#34;&#34;
        unwrap all modules that needed to be compressed

        &#34;&#34;&#34;
        for wrapper in self.identity_wrappers:
            _setattr(self.bound_model, wrapper.module_name, wrapper.module)
        super()._unwrap_model()

    def export_model_save(self, model, model_path, calibration_config=None, calibration_path=None, onnx_path=None,
                          input_shape=None, device=None):
        &#34;&#34;&#34;
        This method helps save pytorch model, calibration config, onnx model in quantizer.

        Parameters
        ----------
        model : pytorch model
            pytorch model to be saved
        model_path : str
            path to save pytorch
        calibration_config: dict
            (optional) config of calibration parameters
        calibration_path : str
            (optional) path to save quantize parameters after calibration
        onnx_path : str
            (optional) path to save onnx model
        input_shape : list or tuple
            input shape to onnx model
        device : torch.device
            device of the model, used to place the dummy input tensor for exporting onnx file.
            the tensor is placed on cpu if ```device``` is None
        &#34;&#34;&#34;
        torch.save(model.state_dict(), model_path)
        _logger.info(&#39;Model state_dict saved to %s&#39;, model_path)
        if calibration_path is not None:
            torch.save(calibration_config, calibration_path)
            _logger.info(&#39;Mask dict saved to %s&#39;, calibration_path)
        if onnx_path is not None:
            assert input_shape is not None, &#39;input_shape must be specified to export onnx model&#39;
            # input info needed
            if device is None:
                device = torch.device(&#39;cpu&#39;)
            input_data = torch.Tensor(*input_shape)
            torch.onnx.export(self.bound_model, input_data.to(device), onnx_path)
            _logger.info(&#39;Model in onnx with input shape %s saved to %s&#39;, input_data.shape, onnx_path)

    def export_model(self, model_path, calibration_path=None, onnx_path=None, input_shape=None, device=None):
        &#34;&#34;&#34;
        Export quantized model weights and calibration parameters

        Parameters
        ----------
        model_path : str
            path to save quantized model weight
        calibration_path : str
            (optional) path to save quantize parameters after calibration
        onnx_path : str
            (optional) path to save onnx model
        input_shape : list or tuple
            input shape to onnx model
        device : torch.device
            device of the model, used to place the dummy input tensor for exporting onnx file.
            the tensor is placed on cpu if ```device``` is None

        Returns
        -------
        Dict
        &#34;&#34;&#34;
        raise NotImplementedError(&#39;Quantizer must overload export_model()&#39;)

    def load_calibration_config(self, calibration_config):
        &#34;&#34;&#34;
        This function aims to help quantizer set quantization parameters by
        loading from a calibration_config which is exported by other quantizer
        or itself. The main usage of this function is helping quantize aware training
        quantizer set appropriate initial parameters so that the training process will
        be much more flexible and converges quickly. What&#39;s more, it can also enable
        quantizer resume quantization model by loading parameters from config.

        Parameters
        ----------
        calibration_config : dict
            dict which saves quantization parameters, quantizer can export itself
            calibration config.
            eg, calibration_config = quantizer.export_model(model_path, calibration_path)
        &#34;&#34;&#34;
        raise NotImplementedError(&#39;Quantizer must overload export_model()&#39;)

    def find_conv_bn_patterns(self, model, dummy_input):
        &#34;&#34;&#34;
        Find all Conv-BN patterns, used for batch normalization folding

        Parameters
        ----------
        model : torch.nn.Module
            model to be analyzed.
        dummy_input : tupel of torch.tensor
            inputs to the model, used for generating the torchscript
        &#34;&#34;&#34;
        if dummy_input is None:
            _logger.debug(&#34;Model inputs are not given, batch normalization folding is disabled&#34;)
            return

        graph = build_module_graph(model, dummy_input)
        for node_group in graph.nodes_py.nodes_op:
            if node_group.op_type in BN_FOLD_OP:
                successors = graph.find_successors(node_group.unique_name)
                successors = [graph.name_to_node[x] for x in successors]
                for successor in successors:
                    if successor.op_type == &#39;BatchNorm2d&#39;:
                        self.conv_bn_patterns[node_group.name] = successor.name

    def record_shape(self, model, dummy_input):
        &#34;&#34;&#34;
        Record input/output&#39;s shapes of each module to be quantized

        Parameters
        ----------
        model : torch.nn.Module
            model to be recorded.
        dummy_input : tupel of torch.tensor
            inputs to the model.
        &#34;&#34;&#34;
        def _pre_forward_hook(self, inp):
            # Only record the first tensor of the input
            return self.pre_forward(inp[0])

        def _post_forward_hook(self, _, out):
            return self.post_forward(out)

        if dummy_input is None:
            return

        all_handles = []
        all_observers = {}
        modules_to_compress = self.get_modules_to_compress()
        compress_names = [layer_info[0].name for layer_info in modules_to_compress]
        for name, module in model.named_modules():
            if name in compress_names:
                all_observers[name] = {}
                all_observers[name][&#39;input_hook&#39;] = RecordingObserver()
                all_observers[name][&#39;output_hook&#39;] = RecordingObserver()
                module.add_module(&#39;pre_forward&#39;, all_observers[name][&#39;input_hook&#39;])
                module.add_module(&#39;post_forward&#39;, all_observers[name][&#39;output_hook&#39;])
                all_handles.append(module.register_forward_pre_hook(_pre_forward_hook))
                all_handles.append(module.register_forward_hook(_post_forward_hook))
        model(dummy_input)
        for name, hooks in all_observers.items():
            # only support single input
            input_val = hooks[&#39;input_hook&#39;].tensor_val
            input_shape = input_val[0].shape if input_val else None
            output_val = hooks[&#39;output_hook&#39;].tensor_val
            output_shape = output_val[0].shape if output_val else None
            shapes = [input_shape, output_shape]
            self.all_shapes[name] = shapes
        return

    def step_with_optimizer(self):
        pass


class QuantGrad(torch.autograd.Function):
    @classmethod
    def _quantize(cls, x, scale, zero_point):
        &#34;&#34;&#34;
        Reference function for quantizing x -- non-clamped.
        Parameters
        ----------
        x : Tensor
            tensor to be quantized
        scale : Tensor
            scale for quantizing x
        zero_point : Tensor
            zero_point for quantizing x
        Returns
        -------
        tensor
            quantized x without clamped
        &#34;&#34;&#34;
        return ((x / scale) + zero_point).round()

    @classmethod
    def get_bits_length(cls, config, quant_type):
        &#34;&#34;&#34;
        Get bits for quantize config
        Parameters
        ----------
        config : Dict
            the configuration for quantization
        quant_type : str
            quant type
        Returns
        -------
        int
            n-bits for quantization configuration
        &#34;&#34;&#34;
        if isinstance(config[&#34;quant_bits&#34;], int):
            return config[&#34;quant_bits&#34;]
        else:
            return config[&#34;quant_bits&#34;].get(quant_type)

    @staticmethod
    def quant_backward(tensor, grad_output, quant_type, scale, zero_point, qmin, qmax):
        &#34;&#34;&#34;
        This method should be overrided by subclass to provide customized backward function,
        default implementation is Straight-Through Estimator
        Parameters
        ----------
        tensor : Tensor
            input of quantization operation
        grad_output : Tensor
            gradient of the output of quantization operation
        scale : Tensor
            the type of quantization, it can be `QuantType.INPUT`, `QuantType.WEIGHT`,
            `QuantType.OUTPUT`, you can define different behavior for different types.
        zero_point : Tensor
            zero_point for quantizing tensor
        qmin : Tensor
            quant_min for quantizing tensor
        qmax : Tensor
            quant_max for quantizng tensor
        Returns
        -------
        tensor
            gradient of the input of quantization operation
        &#34;&#34;&#34;
        return grad_output

    @staticmethod
    def forward(ctx, tensor, quant_type, wrapper, input_tensor=None, **kwargs):
        output = quantize_helper(tensor, quant_type, wrapper, input_tensor, **kwargs)

        if hasattr(wrapper.module, &#34;layer_quant_setting&#34;):
            layer_quant_setting = wrapper.module.layer_quant_setting
            qmin, qmax = getattr(layer_quant_setting, quant_type).get_qmin_qmax()
        else:
            bits = QuantGrad.get_bits_length(wrapper.config, quant_type)
            qmin, qmax = 0, (1 &lt;&lt; bits) - 1

        scale_name, zero_point_name = quant_type.type_to_scale_zero_point_name()
        if hasattr(wrapper.module, scale_name) and hasattr(wrapper.module, zero_point_name):
            scale = getattr(wrapper.module, scale_name)
            zero_point = getattr(wrapper.module, zero_point_name)
        elif hasattr(wrapper.module, &#39;scale&#39;) and hasattr(wrapper.module, &#39;zero_point&#39;):
            scale = wrapper.module.scale
            zero_point = wrapper.module.zero_point
        else:
            scale, zero_point = None, None
        ctx.save_for_backward(tensor)
        ctx.quant_type = quant_type
        ctx.qmin, ctx.qmax = qmin, qmax
        ctx.scale = scale
        ctx.zero_point = zero_point
        return output

    @classmethod
    def backward(cls, ctx, grad_output):
        tensor = ctx.saved_variables[0]
        scale, zero_point = ctx.scale, ctx.zero_point
        quant_type = ctx.quant_type
        qmin, qmax = ctx.qmin, ctx.qmax
        output = cls.quant_backward(tensor, grad_output, quant_type, scale, zero_point, qmin, qmax)
        return output, None, None, None

def _check_weight(module):
    try:
        return isinstance(module.weight.data, torch.Tensor)
    except AttributeError:
        return False

def _check_bias(module):
    try:
        return isinstance(module.bias.data, torch.Tensor)
    except AttributeError:
        return False

def quantize_helper(tensor, quant_type, wrapper, input_tensor=None, **kwargs):
    if quant_type == QuantType.INPUT:
        output = wrapper.quantizer.quantize_input(tensor, wrapper=wrapper, **kwargs)
    elif quant_type == QuantType.WEIGHT:
        output = wrapper.quantizer.quantize_weight(wrapper, input_tensor=input_tensor, **kwargs)
    elif quant_type == QuantType.OUTPUT:
        output = wrapper.quantizer.quantize_output(tensor, wrapper, **kwargs)
    else:
        raise ValueError(&#34;unrecognized QuantType.&#34;)

    return output

class QuantForward(torch.nn.Module):
    &#34;&#34;&#34;
    Base class for executing quantization operations. This is for quantization algorithms
    that do not need to customize gradient.
    &#34;&#34;&#34;

    def forward(self, tensor, quant_type, wrapper, input_tensor=None, **kwargs):
        return quantize_helper(tensor, quant_type, wrapper, input_tensor, **kwargs)


class QuantizerModuleWrapper(torch.nn.Module):
    def __init__(self, module, module_name, module_type, config, quantizer, bn_module=None):
        &#34;&#34;&#34;
        Wrap an module to enable data parallel, forward method customization and buffer registeration.

        Parameters
        ----------
        module : pytorch module
            the module user wants to compress
        config : dict
            the configurations that users specify for compression
        module_name : str
            the name of the module to compress, wrapper module shares same name
        module_type : str
            the type of the module to compress
        quantizer ：quantizer
            the quantizer used to calculate mask
        bn_module : torch.nn.Module
            batch norm layer corresponding to current module, used for simulating batch normalization folding
        &#34;&#34;&#34;
        super().__init__()
        self.module = module
        self.name = module_name
        self.type = module_type
        self.config = config
        self.quantizer = quantizer
        self.bn_module = bn_module

        if &#39;weight&#39; in config[&#39;quant_types&#39;]:
            if not _check_weight(self.module):
                _logger.warning(&#39;Module %s does not have parameter &#34;weight&#34;&#39;, self.name)
            else:
                self.module.register_parameter(&#39;old_weight&#39;, torch.nn.Parameter(self.module.weight))
                delattr(self.module, &#39;weight&#39;)
                self.module.register_buffer(&#39;weight&#39;, self.module.old_weight.data)

                # for batch normalization folding
                if self.bn_module is not None:
                    if _check_bias(self.module):
                        self.module.register_parameter(&#39;old_bias&#39;, torch.nn.Parameter(self.module.bias))
                        init_tensor = self.module.old_bias.data
                    else:
                        init_tensor = torch.zeros_like(self.bn_module.weight)
                    delattr(self.module, &#39;bias&#39;)
                    self.module.register_buffer(&#39;bias&#39;, init_tensor)
                    setattr(module, BN_FOLD_TAG, True)

    def forward(self, *inputs):
        if &#39;input&#39; in self.config[&#39;quant_types&#39;]:
            assert len(inputs) == 1, &#34;Quantization of input only supports ops with single input.&#34;
            new_inp = self.quantizer.quant_grad(
                inputs[0],
                QuantType.INPUT,
                self)
            inputs = (new_inp,)

        if &#39;weight&#39; in self.config[&#39;quant_types&#39;] and _check_weight(self.module):
            if self.bn_module is not None:
                new_weight, new_bias = self.quantizer.fold_bn(*inputs, wrapper=self)
                self.module.bias = new_bias
                self.module.weight = new_weight
            else:
                new_weight = self.module.old_weight
                self.module.weight = new_weight.data

            self.quantizer.quant_grad(
                new_weight,
                QuantType.WEIGHT,
                self, inputs[0])

        result = self.module(*inputs)

        if &#39;output&#39; in self.config[&#39;quant_types&#39;]:
            result = self.quantizer.quant_grad(
                result,
                QuantType.OUTPUT,
                self)
        return result


class QuantizerIdentityWrapper(torch.nn.Module):
    def __init__(self, module, module_name):
        &#34;&#34;&#34;
        Used to wrap modules that should be treated as torch.Identity

        Parameters
        ----------
        module : pytorch module
            the module to be wrapped
        module_name : str
            the name of the module to wrapped, wrapper module shares same name
        &#34;&#34;&#34;
        super().__init__()
        self.module = module
        self.module_name = module_name

    def forward(self, x):
        return x</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="optimization.quantization.core.quant.quantize_helper"><code class="name flex">
<span>def <span class="ident">quantize_helper</span></span>(<span>tensor, quant_type, wrapper, input_tensor=None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def quantize_helper(tensor, quant_type, wrapper, input_tensor=None, **kwargs):
    if quant_type == QuantType.INPUT:
        output = wrapper.quantizer.quantize_input(tensor, wrapper=wrapper, **kwargs)
    elif quant_type == QuantType.WEIGHT:
        output = wrapper.quantizer.quantize_weight(wrapper, input_tensor=input_tensor, **kwargs)
    elif quant_type == QuantType.OUTPUT:
        output = wrapper.quantizer.quantize_output(tensor, wrapper, **kwargs)
    else:
        raise ValueError(&#34;unrecognized QuantType.&#34;)

    return output</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="optimization.quantization.core.quant.QuantForward"><code class="flex name class">
<span>class <span class="ident">QuantForward</span></span>
</code></dt>
<dd>
<div class="desc"><p>Base class for executing quantization operations. This is for quantization algorithms
that do not need to customize gradient.</p>
<p>Initializes internal Module state, shared by both nn.Module and ScriptModule.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class QuantForward(torch.nn.Module):
    &#34;&#34;&#34;
    Base class for executing quantization operations. This is for quantization algorithms
    that do not need to customize gradient.
    &#34;&#34;&#34;

    def forward(self, tensor, quant_type, wrapper, input_tensor=None, **kwargs):
        return quantize_helper(tensor, quant_type, wrapper, input_tensor, **kwargs)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="optimization.quantization.core.quant.QuantForward.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="optimization.quantization.core.quant.QuantForward.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="optimization.quantization.core.quant.QuantForward.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, tensor, quant_type, wrapper, input_tensor=None, **kwargs) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, tensor, quant_type, wrapper, input_tensor=None, **kwargs):
    return quantize_helper(tensor, quant_type, wrapper, input_tensor, **kwargs)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="optimization.quantization.core.quant.QuantGrad"><code class="flex name class">
<span>class <span class="ident">QuantGrad</span></span>
<span>(</span><span>...)</span>
</code></dt>
<dd>
<div class="desc"><p>Records operation history and defines formulas for differentiating ops.</p>
<p>See the Note on extending the autograd engine for more details on how to use
this class: <a href="https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd">https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd</a></p>
<p>Every operation performed on :class:<code>Tensor</code> s creates a new function
object, that performs the computation, and records that it happened.
The history is retained in the form of a DAG of functions, with edges
denoting data dependencies (<code>input &lt;- output</code>). Then, when backward is
called, the graph is processed in the topological ordering, by calling
:func:<code>backward</code> methods of each :class:<code>Function</code> object, and passing
returned gradients on to next :class:<code>Function</code> s.</p>
<p>Normally, the only way users interact with functions is by creating
subclasses and defining new operations. This is a recommended way of
extending torch.autograd.</p>
<p>Examples::</p>
<pre><code>&gt;&gt;&gt; class Exp(Function):
&gt;&gt;&gt;
&gt;&gt;&gt;     @staticmethod
&gt;&gt;&gt;     def forward(ctx, i):
&gt;&gt;&gt;         result = i.exp()
&gt;&gt;&gt;         ctx.save_for_backward(result)
&gt;&gt;&gt;         return result
&gt;&gt;&gt;
&gt;&gt;&gt;     @staticmethod
&gt;&gt;&gt;     def backward(ctx, grad_output):
&gt;&gt;&gt;         result, = ctx.saved_tensors
&gt;&gt;&gt;         return grad_output * result
&gt;&gt;&gt;
&gt;&gt;&gt; #Use it by calling the apply method:
&gt;&gt;&gt; output = Exp.apply(input)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class QuantGrad(torch.autograd.Function):
    @classmethod
    def _quantize(cls, x, scale, zero_point):
        &#34;&#34;&#34;
        Reference function for quantizing x -- non-clamped.
        Parameters
        ----------
        x : Tensor
            tensor to be quantized
        scale : Tensor
            scale for quantizing x
        zero_point : Tensor
            zero_point for quantizing x
        Returns
        -------
        tensor
            quantized x without clamped
        &#34;&#34;&#34;
        return ((x / scale) + zero_point).round()

    @classmethod
    def get_bits_length(cls, config, quant_type):
        &#34;&#34;&#34;
        Get bits for quantize config
        Parameters
        ----------
        config : Dict
            the configuration for quantization
        quant_type : str
            quant type
        Returns
        -------
        int
            n-bits for quantization configuration
        &#34;&#34;&#34;
        if isinstance(config[&#34;quant_bits&#34;], int):
            return config[&#34;quant_bits&#34;]
        else:
            return config[&#34;quant_bits&#34;].get(quant_type)

    @staticmethod
    def quant_backward(tensor, grad_output, quant_type, scale, zero_point, qmin, qmax):
        &#34;&#34;&#34;
        This method should be overrided by subclass to provide customized backward function,
        default implementation is Straight-Through Estimator
        Parameters
        ----------
        tensor : Tensor
            input of quantization operation
        grad_output : Tensor
            gradient of the output of quantization operation
        scale : Tensor
            the type of quantization, it can be `QuantType.INPUT`, `QuantType.WEIGHT`,
            `QuantType.OUTPUT`, you can define different behavior for different types.
        zero_point : Tensor
            zero_point for quantizing tensor
        qmin : Tensor
            quant_min for quantizing tensor
        qmax : Tensor
            quant_max for quantizng tensor
        Returns
        -------
        tensor
            gradient of the input of quantization operation
        &#34;&#34;&#34;
        return grad_output

    @staticmethod
    def forward(ctx, tensor, quant_type, wrapper, input_tensor=None, **kwargs):
        output = quantize_helper(tensor, quant_type, wrapper, input_tensor, **kwargs)

        if hasattr(wrapper.module, &#34;layer_quant_setting&#34;):
            layer_quant_setting = wrapper.module.layer_quant_setting
            qmin, qmax = getattr(layer_quant_setting, quant_type).get_qmin_qmax()
        else:
            bits = QuantGrad.get_bits_length(wrapper.config, quant_type)
            qmin, qmax = 0, (1 &lt;&lt; bits) - 1

        scale_name, zero_point_name = quant_type.type_to_scale_zero_point_name()
        if hasattr(wrapper.module, scale_name) and hasattr(wrapper.module, zero_point_name):
            scale = getattr(wrapper.module, scale_name)
            zero_point = getattr(wrapper.module, zero_point_name)
        elif hasattr(wrapper.module, &#39;scale&#39;) and hasattr(wrapper.module, &#39;zero_point&#39;):
            scale = wrapper.module.scale
            zero_point = wrapper.module.zero_point
        else:
            scale, zero_point = None, None
        ctx.save_for_backward(tensor)
        ctx.quant_type = quant_type
        ctx.qmin, ctx.qmax = qmin, qmax
        ctx.scale = scale
        ctx.zero_point = zero_point
        return output

    @classmethod
    def backward(cls, ctx, grad_output):
        tensor = ctx.saved_variables[0]
        scale, zero_point = ctx.scale, ctx.zero_point
        quant_type = ctx.quant_type
        qmin, qmax = ctx.qmin, ctx.qmax
        output = cls.quant_backward(tensor, grad_output, quant_type, scale, zero_point, qmin, qmax)
        return output, None, None, None</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.autograd.function.Function</li>
<li>torch._C._FunctionBase</li>
<li>torch.autograd.function._ContextMethodMixin</li>
<li>torch.autograd.function._HookMixin</li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="optimization.quantization.quant.bnn_quant.ClipGrad" href="../quant/bnn_quant.html#optimization.quantization.quant.bnn_quant.ClipGrad">ClipGrad</a></li>
<li><a title="optimization.quantization.quant.qat.QATGrad" href="../quant/qat.html#optimization.quantization.quant.qat.QATGrad">QATGrad</a></li>
</ul>
<h3>Static methods</h3>
<dl>
<dt id="optimization.quantization.core.quant.QuantGrad.backward"><code class="name flex">
<span>def <span class="ident">backward</span></span>(<span>ctx, grad_output)</span>
</code></dt>
<dd>
<div class="desc"><p>Defines a formula for differentiating the operation.</p>
<p>This function is to be overridden by all subclasses.</p>
<p>It must accept a context :attr:<code>ctx</code> as the first argument, followed by
as many outputs did :func:<code>forward</code> return, and it should return as many
tensors, as there were inputs to :func:<code>forward</code>. Each argument is the
gradient w.r.t the given output, and each returned value should be the
gradient w.r.t. the corresponding input.</p>
<p>The context can be used to retrieve tensors saved during the forward
pass. It also has an attribute :attr:<code>ctx.needs_input_grad</code> as a tuple
of booleans representing whether each input needs gradient. E.g.,
:func:<code>backward</code> will have <code>ctx.needs_input_grad[0] = True</code> if the
first input to :func:<code>forward</code> needs gradient computated w.r.t. the
output.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def backward(cls, ctx, grad_output):
    tensor = ctx.saved_variables[0]
    scale, zero_point = ctx.scale, ctx.zero_point
    quant_type = ctx.quant_type
    qmin, qmax = ctx.qmin, ctx.qmax
    output = cls.quant_backward(tensor, grad_output, quant_type, scale, zero_point, qmin, qmax)
    return output, None, None, None</code></pre>
</details>
</dd>
<dt id="optimization.quantization.core.quant.QuantGrad.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>ctx, tensor, quant_type, wrapper, input_tensor=None, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Performs the operation.</p>
<p>This function is to be overridden by all subclasses.</p>
<p>It must accept a context ctx as the first argument, followed by any
number of arguments (tensors or other types).</p>
<p>The context can be used to store tensors that can be then retrieved
during the backward pass.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def forward(ctx, tensor, quant_type, wrapper, input_tensor=None, **kwargs):
    output = quantize_helper(tensor, quant_type, wrapper, input_tensor, **kwargs)

    if hasattr(wrapper.module, &#34;layer_quant_setting&#34;):
        layer_quant_setting = wrapper.module.layer_quant_setting
        qmin, qmax = getattr(layer_quant_setting, quant_type).get_qmin_qmax()
    else:
        bits = QuantGrad.get_bits_length(wrapper.config, quant_type)
        qmin, qmax = 0, (1 &lt;&lt; bits) - 1

    scale_name, zero_point_name = quant_type.type_to_scale_zero_point_name()
    if hasattr(wrapper.module, scale_name) and hasattr(wrapper.module, zero_point_name):
        scale = getattr(wrapper.module, scale_name)
        zero_point = getattr(wrapper.module, zero_point_name)
    elif hasattr(wrapper.module, &#39;scale&#39;) and hasattr(wrapper.module, &#39;zero_point&#39;):
        scale = wrapper.module.scale
        zero_point = wrapper.module.zero_point
    else:
        scale, zero_point = None, None
    ctx.save_for_backward(tensor)
    ctx.quant_type = quant_type
    ctx.qmin, ctx.qmax = qmin, qmax
    ctx.scale = scale
    ctx.zero_point = zero_point
    return output</code></pre>
</details>
</dd>
<dt id="optimization.quantization.core.quant.QuantGrad.get_bits_length"><code class="name flex">
<span>def <span class="ident">get_bits_length</span></span>(<span>config, quant_type)</span>
</code></dt>
<dd>
<div class="desc"><p>Get bits for quantize config
Parameters</p>
<hr>
<dl>
<dt><strong><code>config</code></strong> :&ensp;<code>Dict</code></dt>
<dd>the configuration for quantization</dd>
<dt><strong><code>quant_type</code></strong> :&ensp;<code>str</code></dt>
<dd>quant type</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>int</code></dt>
<dd>n-bits for quantization configuration</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@classmethod
def get_bits_length(cls, config, quant_type):
    &#34;&#34;&#34;
    Get bits for quantize config
    Parameters
    ----------
    config : Dict
        the configuration for quantization
    quant_type : str
        quant type
    Returns
    -------
    int
        n-bits for quantization configuration
    &#34;&#34;&#34;
    if isinstance(config[&#34;quant_bits&#34;], int):
        return config[&#34;quant_bits&#34;]
    else:
        return config[&#34;quant_bits&#34;].get(quant_type)</code></pre>
</details>
</dd>
<dt id="optimization.quantization.core.quant.QuantGrad.quant_backward"><code class="name flex">
<span>def <span class="ident">quant_backward</span></span>(<span>tensor, grad_output, quant_type, scale, zero_point, qmin, qmax)</span>
</code></dt>
<dd>
<div class="desc"><p>This method should be overrided by subclass to provide customized backward function,
default implementation is Straight-Through Estimator
Parameters</p>
<hr>
<dl>
<dt><strong><code>tensor</code></strong> :&ensp;<code>Tensor</code></dt>
<dd>input of quantization operation</dd>
<dt><strong><code>grad_output</code></strong> :&ensp;<code>Tensor</code></dt>
<dd>gradient of the output of quantization operation</dd>
<dt><strong><code>scale</code></strong> :&ensp;<code>Tensor</code></dt>
<dd>the type of quantization, it can be <code>QuantType.INPUT</code>, <code>QuantType.WEIGHT</code>,
<code>QuantType.OUTPUT</code>, you can define different behavior for different types.</dd>
<dt><strong><code>zero_point</code></strong> :&ensp;<code>Tensor</code></dt>
<dd>zero_point for quantizing tensor</dd>
<dt><strong><code>qmin</code></strong> :&ensp;<code>Tensor</code></dt>
<dd>quant_min for quantizing tensor</dd>
<dt><strong><code>qmax</code></strong> :&ensp;<code>Tensor</code></dt>
<dd>quant_max for quantizng tensor</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tensor</code></dt>
<dd>gradient of the input of quantization operation</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def quant_backward(tensor, grad_output, quant_type, scale, zero_point, qmin, qmax):
    &#34;&#34;&#34;
    This method should be overrided by subclass to provide customized backward function,
    default implementation is Straight-Through Estimator
    Parameters
    ----------
    tensor : Tensor
        input of quantization operation
    grad_output : Tensor
        gradient of the output of quantization operation
    scale : Tensor
        the type of quantization, it can be `QuantType.INPUT`, `QuantType.WEIGHT`,
        `QuantType.OUTPUT`, you can define different behavior for different types.
    zero_point : Tensor
        zero_point for quantizing tensor
    qmin : Tensor
        quant_min for quantizing tensor
    qmax : Tensor
        quant_max for quantizng tensor
    Returns
    -------
    tensor
        gradient of the input of quantization operation
    &#34;&#34;&#34;
    return grad_output</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="optimization.quantization.core.quant.Quantizer"><code class="flex name class">
<span>class <span class="ident">Quantizer</span></span>
<span>(</span><span>model, config_list, optimizer=None, dummy_input=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Base quantizer for pytorch quantizer</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Quantizer(Compressor):

    def __init__(self, model, config_list, optimizer=None, dummy_input=None):
        &#34;&#34;&#34;
        Base quantizer for pytorch quantizer
        &#34;&#34;&#34;
        if isinstance(model, torch.nn.DataParallel):
            model = model.module
        model_copied = copy.deepcopy(model)
        self.identity_wrappers = []
        self.conv_bn_patterns = {}
        self.find_conv_bn_patterns(model, dummy_input)
        super().__init__(model, config_list, optimizer)
        self.all_shapes = {}
        self.record_shape(model_copied, dummy_input)
        self.quant_grad = QuantGrad.apply
        if self.optimizer is not None:
            self.patch_optimizer(self.step_with_optimizer)
            for wrapper in self.get_modules_wrapper():
                if &#39;weight&#39; in wrapper.config[&#39;quant_types&#39;]:
                    self.optimizer.add_param_group({&#34;params&#34;: wrapper.module.old_weight})
                if hasattr(wrapper.module, &#34;old_bias&#34;):
                    self.optimizer.add_param_group({&#34;params&#34;: getattr(wrapper.module, &#34;old_bias&#34;)})

    def quantize_weight(self, wrapper, **kwargs):
        &#34;&#34;&#34;
        quantize should overload this method to quantize weight.
        This method is effectively hooked to :meth:`forward` of the model.
        Parameters
        ----------
        wrapper : QuantizerModuleWrapper
            the wrapper for origin module
        &#34;&#34;&#34;
        raise NotImplementedError(&#39;Quantizer must overload quantize_weight()&#39;)

    def quantize_output(self, output, wrapper, **kwargs):
        &#34;&#34;&#34;
        quantize should overload this method to quantize output.
        This method is effectively hooked to :meth:`forward` of the model.
        Parameters
        ----------
        output : Tensor
            output that needs to be quantized
        wrapper : QuantizerModuleWrapper
            the wrapper for origin module
        &#34;&#34;&#34;
        raise NotImplementedError(&#39;Quantizer must overload quantize_output()&#39;)

    def quantize_input(self, inputs, wrapper, **kwargs):
        &#34;&#34;&#34;
        quantize should overload this method to quantize input.
        This method is effectively hooked to :meth:`forward` of the model.
        Parameters
        ----------
        inputs : Tensor
            inputs that needs to be quantized
        wrapper : QuantizerModuleWrapper
            the wrapper for origin module
        &#34;&#34;&#34;
        raise NotImplementedError(&#39;Quantizer must overload quantize_input()&#39;)

    def fold_bn(self, *inputs, wrapper):
        &#34;&#34;&#34;
        Simulate batch normalization folding in the training graph. Folded weight and bias are
        returned for the following operations.

        Parameters
        ----------
        inputs : tuple of torch.Tensor
            inputs for the module
        wrapper : QuantizerModuleWrapper
            the wrapper for origin module

        Returns
        -------
        Tuple of torch.Tensor
        &#34;&#34;&#34;
        module = wrapper.module
        bn_module = wrapper.bn_module
        with torch.no_grad():
            output = module(*inputs)
            _ = bn_module(output)
        running_mean = bn_module.running_mean
        running_var = torch.sqrt(bn_module.running_var + bn_module.eps)
        bn_weight = bn_module.weight
        bn_bias = bn_module.bias
        dimensions = len(module.weight.shape)
        shape = [-1] + [1] * (dimensions - 1)
        new_weight = module.old_weight * bn_weight.reshape(shape) / running_var.reshape(shape)
        if hasattr(module, &#39;old_bias&#39;):
            new_bias = bn_bias + (module.old_bias - running_mean) / running_var * bn_weight
        else:
            new_bias = bn_bias - running_mean / running_var * bn_weight
        return new_weight, new_bias

    def _wrap_modules(self, layer, config):
        &#34;&#34;&#34;
        Create a wrapper forward function to replace the original one.
        Parameters
        ----------
        layer : LayerInfo
            the layer to instrument the mask
        config : dict
            the configuration for quantization
        &#34;&#34;&#34;
        assert &#39;quant_types&#39; in config, &#39;must provide quant_types in config&#39;
        assert isinstance(config[&#39;quant_types&#39;], list), &#39;quant_types must be list type&#39;
        assert &#39;quant_bits&#39; in config, &#39;must provide quant_bits in config&#39;
        assert isinstance(config[&#39;quant_bits&#39;], int) or isinstance(config[&#39;quant_bits&#39;], dict), &#39;quant_bits must be dict type or int type&#39;

        if isinstance(config[&#39;quant_bits&#39;], dict):
            for quant_type in config[&#39;quant_types&#39;]:
                assert quant_type in config[&#39;quant_bits&#39;], &#39;bits length for %s must be specified in quant_bits dict&#39; % quant_type

        bn_module = None
        if layer.name in self.conv_bn_patterns:
            bn_module_name = self.conv_bn_patterns[layer.name]
            for name, module in self.bound_model.named_modules():
                if name == bn_module_name:
                    bn_module = module
                    break
            assert bn_module is not None, &#34;BN module corresponding to layer {} is not found&#34;.format(layer.name)
            self.identity_wrappers.append(QuantizerIdentityWrapper(bn_module, bn_module_name))
        return QuantizerModuleWrapper(layer.module, layer.name, layer.type, config, self, bn_module)

    def _wrap_model(self):
        &#34;&#34;&#34;
        wrap all modules that needed to be compressed

        &#34;&#34;&#34;
        for wrapper in reversed(self.identity_wrappers):
            _setattr(self.bound_model, wrapper.module_name, wrapper)
        super()._wrap_model()

    def _unwrap_model(self):
        &#34;&#34;&#34;
        unwrap all modules that needed to be compressed

        &#34;&#34;&#34;
        for wrapper in self.identity_wrappers:
            _setattr(self.bound_model, wrapper.module_name, wrapper.module)
        super()._unwrap_model()

    def export_model_save(self, model, model_path, calibration_config=None, calibration_path=None, onnx_path=None,
                          input_shape=None, device=None):
        &#34;&#34;&#34;
        This method helps save pytorch model, calibration config, onnx model in quantizer.

        Parameters
        ----------
        model : pytorch model
            pytorch model to be saved
        model_path : str
            path to save pytorch
        calibration_config: dict
            (optional) config of calibration parameters
        calibration_path : str
            (optional) path to save quantize parameters after calibration
        onnx_path : str
            (optional) path to save onnx model
        input_shape : list or tuple
            input shape to onnx model
        device : torch.device
            device of the model, used to place the dummy input tensor for exporting onnx file.
            the tensor is placed on cpu if ```device``` is None
        &#34;&#34;&#34;
        torch.save(model.state_dict(), model_path)
        _logger.info(&#39;Model state_dict saved to %s&#39;, model_path)
        if calibration_path is not None:
            torch.save(calibration_config, calibration_path)
            _logger.info(&#39;Mask dict saved to %s&#39;, calibration_path)
        if onnx_path is not None:
            assert input_shape is not None, &#39;input_shape must be specified to export onnx model&#39;
            # input info needed
            if device is None:
                device = torch.device(&#39;cpu&#39;)
            input_data = torch.Tensor(*input_shape)
            torch.onnx.export(self.bound_model, input_data.to(device), onnx_path)
            _logger.info(&#39;Model in onnx with input shape %s saved to %s&#39;, input_data.shape, onnx_path)

    def export_model(self, model_path, calibration_path=None, onnx_path=None, input_shape=None, device=None):
        &#34;&#34;&#34;
        Export quantized model weights and calibration parameters

        Parameters
        ----------
        model_path : str
            path to save quantized model weight
        calibration_path : str
            (optional) path to save quantize parameters after calibration
        onnx_path : str
            (optional) path to save onnx model
        input_shape : list or tuple
            input shape to onnx model
        device : torch.device
            device of the model, used to place the dummy input tensor for exporting onnx file.
            the tensor is placed on cpu if ```device``` is None

        Returns
        -------
        Dict
        &#34;&#34;&#34;
        raise NotImplementedError(&#39;Quantizer must overload export_model()&#39;)

    def load_calibration_config(self, calibration_config):
        &#34;&#34;&#34;
        This function aims to help quantizer set quantization parameters by
        loading from a calibration_config which is exported by other quantizer
        or itself. The main usage of this function is helping quantize aware training
        quantizer set appropriate initial parameters so that the training process will
        be much more flexible and converges quickly. What&#39;s more, it can also enable
        quantizer resume quantization model by loading parameters from config.

        Parameters
        ----------
        calibration_config : dict
            dict which saves quantization parameters, quantizer can export itself
            calibration config.
            eg, calibration_config = quantizer.export_model(model_path, calibration_path)
        &#34;&#34;&#34;
        raise NotImplementedError(&#39;Quantizer must overload export_model()&#39;)

    def find_conv_bn_patterns(self, model, dummy_input):
        &#34;&#34;&#34;
        Find all Conv-BN patterns, used for batch normalization folding

        Parameters
        ----------
        model : torch.nn.Module
            model to be analyzed.
        dummy_input : tupel of torch.tensor
            inputs to the model, used for generating the torchscript
        &#34;&#34;&#34;
        if dummy_input is None:
            _logger.debug(&#34;Model inputs are not given, batch normalization folding is disabled&#34;)
            return

        graph = build_module_graph(model, dummy_input)
        for node_group in graph.nodes_py.nodes_op:
            if node_group.op_type in BN_FOLD_OP:
                successors = graph.find_successors(node_group.unique_name)
                successors = [graph.name_to_node[x] for x in successors]
                for successor in successors:
                    if successor.op_type == &#39;BatchNorm2d&#39;:
                        self.conv_bn_patterns[node_group.name] = successor.name

    def record_shape(self, model, dummy_input):
        &#34;&#34;&#34;
        Record input/output&#39;s shapes of each module to be quantized

        Parameters
        ----------
        model : torch.nn.Module
            model to be recorded.
        dummy_input : tupel of torch.tensor
            inputs to the model.
        &#34;&#34;&#34;
        def _pre_forward_hook(self, inp):
            # Only record the first tensor of the input
            return self.pre_forward(inp[0])

        def _post_forward_hook(self, _, out):
            return self.post_forward(out)

        if dummy_input is None:
            return

        all_handles = []
        all_observers = {}
        modules_to_compress = self.get_modules_to_compress()
        compress_names = [layer_info[0].name for layer_info in modules_to_compress]
        for name, module in model.named_modules():
            if name in compress_names:
                all_observers[name] = {}
                all_observers[name][&#39;input_hook&#39;] = RecordingObserver()
                all_observers[name][&#39;output_hook&#39;] = RecordingObserver()
                module.add_module(&#39;pre_forward&#39;, all_observers[name][&#39;input_hook&#39;])
                module.add_module(&#39;post_forward&#39;, all_observers[name][&#39;output_hook&#39;])
                all_handles.append(module.register_forward_pre_hook(_pre_forward_hook))
                all_handles.append(module.register_forward_hook(_post_forward_hook))
        model(dummy_input)
        for name, hooks in all_observers.items():
            # only support single input
            input_val = hooks[&#39;input_hook&#39;].tensor_val
            input_shape = input_val[0].shape if input_val else None
            output_val = hooks[&#39;output_hook&#39;].tensor_val
            output_shape = output_val[0].shape if output_val else None
            shapes = [input_shape, output_shape]
            self.all_shapes[name] = shapes
        return

    def step_with_optimizer(self):
        pass</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="optimization.common.base.compressor.Compressor" href="../../common/base/compressor.html#optimization.common.base.compressor.Compressor">Compressor</a></li>
</ul>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="optimization.quantization.quant.bnn_quant.BNNQuantizer" href="../quant/bnn_quant.html#optimization.quantization.quant.bnn_quant.BNNQuantizer">BNNQuantizer</a></li>
<li><a title="optimization.quantization.quant.dorefa_quant.DoReFaQuantizer" href="../quant/dorefa_quant.html#optimization.quantization.quant.dorefa_quant.DoReFaQuantizer">DoReFaQuantizer</a></li>
<li><a title="optimization.quantization.quant.lsq_quant.LsqQuantizer" href="../quant/lsq_quant.html#optimization.quantization.quant.lsq_quant.LsqQuantizer">LsqQuantizer</a></li>
<li><a title="optimization.quantization.quant.navie_quant.NavieQuantizer" href="../quant/navie_quant.html#optimization.quantization.quant.navie_quant.NavieQuantizer">NavieQuantizer</a></li>
<li><a title="optimization.quantization.quant.observer_quant.ObserverQuantizer" href="../quant/observer_quant.html#optimization.quantization.quant.observer_quant.ObserverQuantizer">ObserverQuantizer</a></li>
<li><a title="optimization.quantization.quant.qat.QAT_Quantizer" href="../quant/qat.html#optimization.quantization.quant.qat.QAT_Quantizer">QAT_Quantizer</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="optimization.quantization.core.quant.Quantizer.export_model"><code class="name flex">
<span>def <span class="ident">export_model</span></span>(<span>self, model_path, calibration_path=None, onnx_path=None, input_shape=None, device=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Export quantized model weights and calibration parameters</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model_path</code></strong> :&ensp;<code>str</code></dt>
<dd>path to save quantized model weight</dd>
<dt><strong><code>calibration_path</code></strong> :&ensp;<code>str</code></dt>
<dd>(optional) path to save quantize parameters after calibration</dd>
<dt><strong><code>onnx_path</code></strong> :&ensp;<code>str</code></dt>
<dd>(optional) path to save onnx model</dd>
<dt><strong><code>input_shape</code></strong> :&ensp;<code>list</code> or <code>tuple</code></dt>
<dd>input shape to onnx model</dd>
<dt><strong><code>device</code></strong> :&ensp;<code>torch.device</code></dt>
<dd>device of the model, used to place the dummy input tensor for exporting onnx file.
the tensor is placed on cpu if <code>device</code> is None</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Dict</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def export_model(self, model_path, calibration_path=None, onnx_path=None, input_shape=None, device=None):
    &#34;&#34;&#34;
    Export quantized model weights and calibration parameters

    Parameters
    ----------
    model_path : str
        path to save quantized model weight
    calibration_path : str
        (optional) path to save quantize parameters after calibration
    onnx_path : str
        (optional) path to save onnx model
    input_shape : list or tuple
        input shape to onnx model
    device : torch.device
        device of the model, used to place the dummy input tensor for exporting onnx file.
        the tensor is placed on cpu if ```device``` is None

    Returns
    -------
    Dict
    &#34;&#34;&#34;
    raise NotImplementedError(&#39;Quantizer must overload export_model()&#39;)</code></pre>
</details>
</dd>
<dt id="optimization.quantization.core.quant.Quantizer.export_model_save"><code class="name flex">
<span>def <span class="ident">export_model_save</span></span>(<span>self, model, model_path, calibration_config=None, calibration_path=None, onnx_path=None, input_shape=None, device=None)</span>
</code></dt>
<dd>
<div class="desc"><p>This method helps save pytorch model, calibration config, onnx model in quantizer.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>pytorch model</code></dt>
<dd>pytorch model to be saved</dd>
<dt><strong><code>model_path</code></strong> :&ensp;<code>str</code></dt>
<dd>path to save pytorch</dd>
<dt><strong><code>calibration_config</code></strong> :&ensp;<code>dict</code></dt>
<dd>(optional) config of calibration parameters</dd>
<dt><strong><code>calibration_path</code></strong> :&ensp;<code>str</code></dt>
<dd>(optional) path to save quantize parameters after calibration</dd>
<dt><strong><code>onnx_path</code></strong> :&ensp;<code>str</code></dt>
<dd>(optional) path to save onnx model</dd>
<dt><strong><code>input_shape</code></strong> :&ensp;<code>list</code> or <code>tuple</code></dt>
<dd>input shape to onnx model</dd>
<dt><strong><code>device</code></strong> :&ensp;<code>torch.device</code></dt>
<dd>device of the model, used to place the dummy input tensor for exporting onnx file.
the tensor is placed on cpu if <code>device</code> is None</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def export_model_save(self, model, model_path, calibration_config=None, calibration_path=None, onnx_path=None,
                      input_shape=None, device=None):
    &#34;&#34;&#34;
    This method helps save pytorch model, calibration config, onnx model in quantizer.

    Parameters
    ----------
    model : pytorch model
        pytorch model to be saved
    model_path : str
        path to save pytorch
    calibration_config: dict
        (optional) config of calibration parameters
    calibration_path : str
        (optional) path to save quantize parameters after calibration
    onnx_path : str
        (optional) path to save onnx model
    input_shape : list or tuple
        input shape to onnx model
    device : torch.device
        device of the model, used to place the dummy input tensor for exporting onnx file.
        the tensor is placed on cpu if ```device``` is None
    &#34;&#34;&#34;
    torch.save(model.state_dict(), model_path)
    _logger.info(&#39;Model state_dict saved to %s&#39;, model_path)
    if calibration_path is not None:
        torch.save(calibration_config, calibration_path)
        _logger.info(&#39;Mask dict saved to %s&#39;, calibration_path)
    if onnx_path is not None:
        assert input_shape is not None, &#39;input_shape must be specified to export onnx model&#39;
        # input info needed
        if device is None:
            device = torch.device(&#39;cpu&#39;)
        input_data = torch.Tensor(*input_shape)
        torch.onnx.export(self.bound_model, input_data.to(device), onnx_path)
        _logger.info(&#39;Model in onnx with input shape %s saved to %s&#39;, input_data.shape, onnx_path)</code></pre>
</details>
</dd>
<dt id="optimization.quantization.core.quant.Quantizer.find_conv_bn_patterns"><code class="name flex">
<span>def <span class="ident">find_conv_bn_patterns</span></span>(<span>self, model, dummy_input)</span>
</code></dt>
<dd>
<div class="desc"><p>Find all Conv-BN patterns, used for batch normalization folding</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>torch.nn.Module</code></dt>
<dd>model to be analyzed.</dd>
<dt><strong><code>dummy_input</code></strong> :&ensp;<code>tupel</code> of <code>torch.tensor</code></dt>
<dd>inputs to the model, used for generating the torchscript</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_conv_bn_patterns(self, model, dummy_input):
    &#34;&#34;&#34;
    Find all Conv-BN patterns, used for batch normalization folding

    Parameters
    ----------
    model : torch.nn.Module
        model to be analyzed.
    dummy_input : tupel of torch.tensor
        inputs to the model, used for generating the torchscript
    &#34;&#34;&#34;
    if dummy_input is None:
        _logger.debug(&#34;Model inputs are not given, batch normalization folding is disabled&#34;)
        return

    graph = build_module_graph(model, dummy_input)
    for node_group in graph.nodes_py.nodes_op:
        if node_group.op_type in BN_FOLD_OP:
            successors = graph.find_successors(node_group.unique_name)
            successors = [graph.name_to_node[x] for x in successors]
            for successor in successors:
                if successor.op_type == &#39;BatchNorm2d&#39;:
                    self.conv_bn_patterns[node_group.name] = successor.name</code></pre>
</details>
</dd>
<dt id="optimization.quantization.core.quant.Quantizer.fold_bn"><code class="name flex">
<span>def <span class="ident">fold_bn</span></span>(<span>self, *inputs, wrapper)</span>
</code></dt>
<dd>
<div class="desc"><p>Simulate batch normalization folding in the training graph. Folded weight and bias are
returned for the following operations.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>inputs</code></strong> :&ensp;<code>tuple</code> of <code>torch.Tensor</code></dt>
<dd>inputs for the module</dd>
<dt><strong><code>wrapper</code></strong> :&ensp;<code><a title="optimization.quantization.core.quant.QuantizerModuleWrapper" href="#optimization.quantization.core.quant.QuantizerModuleWrapper">QuantizerModuleWrapper</a></code></dt>
<dd>the wrapper for origin module</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Tuple</code> of <code>torch.Tensor</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fold_bn(self, *inputs, wrapper):
    &#34;&#34;&#34;
    Simulate batch normalization folding in the training graph. Folded weight and bias are
    returned for the following operations.

    Parameters
    ----------
    inputs : tuple of torch.Tensor
        inputs for the module
    wrapper : QuantizerModuleWrapper
        the wrapper for origin module

    Returns
    -------
    Tuple of torch.Tensor
    &#34;&#34;&#34;
    module = wrapper.module
    bn_module = wrapper.bn_module
    with torch.no_grad():
        output = module(*inputs)
        _ = bn_module(output)
    running_mean = bn_module.running_mean
    running_var = torch.sqrt(bn_module.running_var + bn_module.eps)
    bn_weight = bn_module.weight
    bn_bias = bn_module.bias
    dimensions = len(module.weight.shape)
    shape = [-1] + [1] * (dimensions - 1)
    new_weight = module.old_weight * bn_weight.reshape(shape) / running_var.reshape(shape)
    if hasattr(module, &#39;old_bias&#39;):
        new_bias = bn_bias + (module.old_bias - running_mean) / running_var * bn_weight
    else:
        new_bias = bn_bias - running_mean / running_var * bn_weight
    return new_weight, new_bias</code></pre>
</details>
</dd>
<dt id="optimization.quantization.core.quant.Quantizer.load_calibration_config"><code class="name flex">
<span>def <span class="ident">load_calibration_config</span></span>(<span>self, calibration_config)</span>
</code></dt>
<dd>
<div class="desc"><p>This function aims to help quantizer set quantization parameters by
loading from a calibration_config which is exported by other quantizer
or itself. The main usage of this function is helping quantize aware training
quantizer set appropriate initial parameters so that the training process will
be much more flexible and converges quickly. What's more, it can also enable
quantizer resume quantization model by loading parameters from config.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>calibration_config</code></strong> :&ensp;<code>dict</code></dt>
<dd>dict which saves quantization parameters, quantizer can export itself
calibration config.
eg, calibration_config = quantizer.export_model(model_path, calibration_path)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_calibration_config(self, calibration_config):
    &#34;&#34;&#34;
    This function aims to help quantizer set quantization parameters by
    loading from a calibration_config which is exported by other quantizer
    or itself. The main usage of this function is helping quantize aware training
    quantizer set appropriate initial parameters so that the training process will
    be much more flexible and converges quickly. What&#39;s more, it can also enable
    quantizer resume quantization model by loading parameters from config.

    Parameters
    ----------
    calibration_config : dict
        dict which saves quantization parameters, quantizer can export itself
        calibration config.
        eg, calibration_config = quantizer.export_model(model_path, calibration_path)
    &#34;&#34;&#34;
    raise NotImplementedError(&#39;Quantizer must overload export_model()&#39;)</code></pre>
</details>
</dd>
<dt id="optimization.quantization.core.quant.Quantizer.quantize_input"><code class="name flex">
<span>def <span class="ident">quantize_input</span></span>(<span>self, inputs, wrapper, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>quantize should overload this method to quantize input.
This method is effectively hooked to :meth:<code>forward</code> of the model.
Parameters</p>
<hr>
<dl>
<dt><strong><code>inputs</code></strong> :&ensp;<code>Tensor</code></dt>
<dd>inputs that needs to be quantized</dd>
<dt><strong><code>wrapper</code></strong> :&ensp;<code><a title="optimization.quantization.core.quant.QuantizerModuleWrapper" href="#optimization.quantization.core.quant.QuantizerModuleWrapper">QuantizerModuleWrapper</a></code></dt>
<dd>the wrapper for origin module</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def quantize_input(self, inputs, wrapper, **kwargs):
    &#34;&#34;&#34;
    quantize should overload this method to quantize input.
    This method is effectively hooked to :meth:`forward` of the model.
    Parameters
    ----------
    inputs : Tensor
        inputs that needs to be quantized
    wrapper : QuantizerModuleWrapper
        the wrapper for origin module
    &#34;&#34;&#34;
    raise NotImplementedError(&#39;Quantizer must overload quantize_input()&#39;)</code></pre>
</details>
</dd>
<dt id="optimization.quantization.core.quant.Quantizer.quantize_output"><code class="name flex">
<span>def <span class="ident">quantize_output</span></span>(<span>self, output, wrapper, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>quantize should overload this method to quantize output.
This method is effectively hooked to :meth:<code>forward</code> of the model.
Parameters</p>
<hr>
<dl>
<dt><strong><code>output</code></strong> :&ensp;<code>Tensor</code></dt>
<dd>output that needs to be quantized</dd>
<dt><strong><code>wrapper</code></strong> :&ensp;<code><a title="optimization.quantization.core.quant.QuantizerModuleWrapper" href="#optimization.quantization.core.quant.QuantizerModuleWrapper">QuantizerModuleWrapper</a></code></dt>
<dd>the wrapper for origin module</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def quantize_output(self, output, wrapper, **kwargs):
    &#34;&#34;&#34;
    quantize should overload this method to quantize output.
    This method is effectively hooked to :meth:`forward` of the model.
    Parameters
    ----------
    output : Tensor
        output that needs to be quantized
    wrapper : QuantizerModuleWrapper
        the wrapper for origin module
    &#34;&#34;&#34;
    raise NotImplementedError(&#39;Quantizer must overload quantize_output()&#39;)</code></pre>
</details>
</dd>
<dt id="optimization.quantization.core.quant.Quantizer.quantize_weight"><code class="name flex">
<span>def <span class="ident">quantize_weight</span></span>(<span>self, wrapper, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>quantize should overload this method to quantize weight.
This method is effectively hooked to :meth:<code>forward</code> of the model.
Parameters</p>
<hr>
<dl>
<dt><strong><code>wrapper</code></strong> :&ensp;<code><a title="optimization.quantization.core.quant.QuantizerModuleWrapper" href="#optimization.quantization.core.quant.QuantizerModuleWrapper">QuantizerModuleWrapper</a></code></dt>
<dd>the wrapper for origin module</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def quantize_weight(self, wrapper, **kwargs):
    &#34;&#34;&#34;
    quantize should overload this method to quantize weight.
    This method is effectively hooked to :meth:`forward` of the model.
    Parameters
    ----------
    wrapper : QuantizerModuleWrapper
        the wrapper for origin module
    &#34;&#34;&#34;
    raise NotImplementedError(&#39;Quantizer must overload quantize_weight()&#39;)</code></pre>
</details>
</dd>
<dt id="optimization.quantization.core.quant.Quantizer.record_shape"><code class="name flex">
<span>def <span class="ident">record_shape</span></span>(<span>self, model, dummy_input)</span>
</code></dt>
<dd>
<div class="desc"><p>Record input/output's shapes of each module to be quantized</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>torch.nn.Module</code></dt>
<dd>model to be recorded.</dd>
<dt><strong><code>dummy_input</code></strong> :&ensp;<code>tupel</code> of <code>torch.tensor</code></dt>
<dd>inputs to the model.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def record_shape(self, model, dummy_input):
    &#34;&#34;&#34;
    Record input/output&#39;s shapes of each module to be quantized

    Parameters
    ----------
    model : torch.nn.Module
        model to be recorded.
    dummy_input : tupel of torch.tensor
        inputs to the model.
    &#34;&#34;&#34;
    def _pre_forward_hook(self, inp):
        # Only record the first tensor of the input
        return self.pre_forward(inp[0])

    def _post_forward_hook(self, _, out):
        return self.post_forward(out)

    if dummy_input is None:
        return

    all_handles = []
    all_observers = {}
    modules_to_compress = self.get_modules_to_compress()
    compress_names = [layer_info[0].name for layer_info in modules_to_compress]
    for name, module in model.named_modules():
        if name in compress_names:
            all_observers[name] = {}
            all_observers[name][&#39;input_hook&#39;] = RecordingObserver()
            all_observers[name][&#39;output_hook&#39;] = RecordingObserver()
            module.add_module(&#39;pre_forward&#39;, all_observers[name][&#39;input_hook&#39;])
            module.add_module(&#39;post_forward&#39;, all_observers[name][&#39;output_hook&#39;])
            all_handles.append(module.register_forward_pre_hook(_pre_forward_hook))
            all_handles.append(module.register_forward_hook(_post_forward_hook))
    model(dummy_input)
    for name, hooks in all_observers.items():
        # only support single input
        input_val = hooks[&#39;input_hook&#39;].tensor_val
        input_shape = input_val[0].shape if input_val else None
        output_val = hooks[&#39;output_hook&#39;].tensor_val
        output_shape = output_val[0].shape if output_val else None
        shapes = [input_shape, output_shape]
        self.all_shapes[name] = shapes
    return</code></pre>
</details>
</dd>
<dt id="optimization.quantization.core.quant.Quantizer.step_with_optimizer"><code class="name flex">
<span>def <span class="ident">step_with_optimizer</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step_with_optimizer(self):
    pass</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="optimization.common.base.compressor.Compressor" href="../../common/base/compressor.html#optimization.common.base.compressor.Compressor">Compressor</a></b></code>:
<ul class="hlist">
<li><code><a title="optimization.common.base.compressor.Compressor.compress" href="../../common/base/compressor.html#optimization.common.base.compressor.Compressor.compress">compress</a></code></li>
<li><code><a title="optimization.common.base.compressor.Compressor.get_modules_to_compress" href="../../common/base/compressor.html#optimization.common.base.compressor.Compressor.get_modules_to_compress">get_modules_to_compress</a></code></li>
<li><code><a title="optimization.common.base.compressor.Compressor.get_modules_wrapper" href="../../common/base/compressor.html#optimization.common.base.compressor.Compressor.get_modules_wrapper">get_modules_wrapper</a></code></li>
<li><code><a title="optimization.common.base.compressor.Compressor.reset" href="../../common/base/compressor.html#optimization.common.base.compressor.Compressor.reset">reset</a></code></li>
<li><code><a title="optimization.common.base.compressor.Compressor.select_config" href="../../common/base/compressor.html#optimization.common.base.compressor.Compressor.select_config">select_config</a></code></li>
<li><code><a title="optimization.common.base.compressor.Compressor.set_wrappers_attribute" href="../../common/base/compressor.html#optimization.common.base.compressor.Compressor.set_wrappers_attribute">set_wrappers_attribute</a></code></li>
<li><code><a title="optimization.common.base.compressor.Compressor.update_epoch" href="../../common/base/compressor.html#optimization.common.base.compressor.Compressor.update_epoch">update_epoch</a></code></li>
<li><code><a title="optimization.common.base.compressor.Compressor.validate_config" href="../../common/base/compressor.html#optimization.common.base.compressor.Compressor.validate_config">validate_config</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="optimization.quantization.core.quant.QuantizerIdentityWrapper"><code class="flex name class">
<span>class <span class="ident">QuantizerIdentityWrapper</span></span>
<span>(</span><span>module, module_name)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Used to wrap modules that should be treated as torch.Identity</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>module</code></strong> :&ensp;<code>pytorch module</code></dt>
<dd>the module to be wrapped</dd>
<dt><strong><code>module_name</code></strong> :&ensp;<code>str</code></dt>
<dd>the name of the module to wrapped, wrapper module shares same name</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class QuantizerIdentityWrapper(torch.nn.Module):
    def __init__(self, module, module_name):
        &#34;&#34;&#34;
        Used to wrap modules that should be treated as torch.Identity

        Parameters
        ----------
        module : pytorch module
            the module to be wrapped
        module_name : str
            the name of the module to wrapped, wrapper module shares same name
        &#34;&#34;&#34;
        super().__init__()
        self.module = module
        self.module_name = module_name

    def forward(self, x):
        return x</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="optimization.quantization.core.quant.QuantizerIdentityWrapper.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="optimization.quantization.core.quant.QuantizerIdentityWrapper.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="optimization.quantization.core.quant.QuantizerIdentityWrapper.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    return x</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="optimization.quantization.core.quant.QuantizerModuleWrapper"><code class="flex name class">
<span>class <span class="ident">QuantizerModuleWrapper</span></span>
<span>(</span><span>module, module_name, module_type, config, quantizer, bn_module=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Base class for all neural network modules.</p>
<p>Your models should also subclass this class.</p>
<p>Modules can also contain other Modules, allowing to nest them in
a tree structure. You can assign the submodules as regular attributes::</p>
<pre><code>import torch.nn as nn
import torch.nn.functional as F

class Model(nn.Module):
    def __init__(self):
        super(Model, self).__init__()
        self.conv1 = nn.Conv2d(1, 20, 5)
        self.conv2 = nn.Conv2d(20, 20, 5)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        return F.relu(self.conv2(x))
</code></pre>
<p>Submodules assigned in this way will be registered, and will have their
parameters converted too when you call :meth:<code>to</code>, etc.</p>
<p>:ivar training: Boolean represents whether this module is in training or
evaluation mode.
:vartype training: bool</p>
<p>Wrap an module to enable data parallel, forward method customization and buffer registeration.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>module</code></strong> :&ensp;<code>pytorch module</code></dt>
<dd>the module user wants to compress</dd>
<dt><strong><code>config</code></strong> :&ensp;<code>dict</code></dt>
<dd>the configurations that users specify for compression</dd>
<dt><strong><code>module_name</code></strong> :&ensp;<code>str</code></dt>
<dd>the name of the module to compress, wrapper module shares same name</dd>
<dt><strong><code>module_type</code></strong> :&ensp;<code>str</code></dt>
<dd>the type of the module to compress</dd>
<dt>quantizer ：quantizer</dt>
<dt>the quantizer used to calculate mask</dt>
<dt><strong><code>bn_module</code></strong> :&ensp;<code>torch.nn.Module</code></dt>
<dd>batch norm layer corresponding to current module, used for simulating batch normalization folding</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class QuantizerModuleWrapper(torch.nn.Module):
    def __init__(self, module, module_name, module_type, config, quantizer, bn_module=None):
        &#34;&#34;&#34;
        Wrap an module to enable data parallel, forward method customization and buffer registeration.

        Parameters
        ----------
        module : pytorch module
            the module user wants to compress
        config : dict
            the configurations that users specify for compression
        module_name : str
            the name of the module to compress, wrapper module shares same name
        module_type : str
            the type of the module to compress
        quantizer ：quantizer
            the quantizer used to calculate mask
        bn_module : torch.nn.Module
            batch norm layer corresponding to current module, used for simulating batch normalization folding
        &#34;&#34;&#34;
        super().__init__()
        self.module = module
        self.name = module_name
        self.type = module_type
        self.config = config
        self.quantizer = quantizer
        self.bn_module = bn_module

        if &#39;weight&#39; in config[&#39;quant_types&#39;]:
            if not _check_weight(self.module):
                _logger.warning(&#39;Module %s does not have parameter &#34;weight&#34;&#39;, self.name)
            else:
                self.module.register_parameter(&#39;old_weight&#39;, torch.nn.Parameter(self.module.weight))
                delattr(self.module, &#39;weight&#39;)
                self.module.register_buffer(&#39;weight&#39;, self.module.old_weight.data)

                # for batch normalization folding
                if self.bn_module is not None:
                    if _check_bias(self.module):
                        self.module.register_parameter(&#39;old_bias&#39;, torch.nn.Parameter(self.module.bias))
                        init_tensor = self.module.old_bias.data
                    else:
                        init_tensor = torch.zeros_like(self.bn_module.weight)
                    delattr(self.module, &#39;bias&#39;)
                    self.module.register_buffer(&#39;bias&#39;, init_tensor)
                    setattr(module, BN_FOLD_TAG, True)

    def forward(self, *inputs):
        if &#39;input&#39; in self.config[&#39;quant_types&#39;]:
            assert len(inputs) == 1, &#34;Quantization of input only supports ops with single input.&#34;
            new_inp = self.quantizer.quant_grad(
                inputs[0],
                QuantType.INPUT,
                self)
            inputs = (new_inp,)

        if &#39;weight&#39; in self.config[&#39;quant_types&#39;] and _check_weight(self.module):
            if self.bn_module is not None:
                new_weight, new_bias = self.quantizer.fold_bn(*inputs, wrapper=self)
                self.module.bias = new_bias
                self.module.weight = new_weight
            else:
                new_weight = self.module.old_weight
                self.module.weight = new_weight.data

            self.quantizer.quant_grad(
                new_weight,
                QuantType.WEIGHT,
                self, inputs[0])

        result = self.module(*inputs)

        if &#39;output&#39; in self.config[&#39;quant_types&#39;]:
            result = self.quantizer.quant_grad(
                result,
                QuantType.OUTPUT,
                self)
        return result</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="optimization.quantization.core.quant.QuantizerModuleWrapper.dump_patches"><code class="name">var <span class="ident">dump_patches</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="optimization.quantization.core.quant.QuantizerModuleWrapper.training"><code class="name">var <span class="ident">training</span> : bool</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="optimization.quantization.core.quant.QuantizerModuleWrapper.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, *inputs) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, *inputs):
    if &#39;input&#39; in self.config[&#39;quant_types&#39;]:
        assert len(inputs) == 1, &#34;Quantization of input only supports ops with single input.&#34;
        new_inp = self.quantizer.quant_grad(
            inputs[0],
            QuantType.INPUT,
            self)
        inputs = (new_inp,)

    if &#39;weight&#39; in self.config[&#39;quant_types&#39;] and _check_weight(self.module):
        if self.bn_module is not None:
            new_weight, new_bias = self.quantizer.fold_bn(*inputs, wrapper=self)
            self.module.bias = new_bias
            self.module.weight = new_weight
        else:
            new_weight = self.module.old_weight
            self.module.weight = new_weight.data

        self.quantizer.quant_grad(
            new_weight,
            QuantType.WEIGHT,
            self, inputs[0])

    result = self.module(*inputs)

    if &#39;output&#39; in self.config[&#39;quant_types&#39;]:
        result = self.quantizer.quant_grad(
            result,
            QuantType.OUTPUT,
            self)
    return result</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="optimization.quantization.core.quant.RecordingObserver"><code class="flex name class">
<span>class <span class="ident">RecordingObserver</span></span>
<span>(</span><span>**kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>The module is mainly for debug and records the tensor values during runtime.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>dtype</code></strong></dt>
<dd>Quantized data type</dd>
<dt><strong><code>qscheme</code></strong></dt>
<dd>Quantization scheme to be used</dd>
<dt><strong><code>reduce_range</code></strong></dt>
<dd>Reduces the range of the quantized data type by 1 bit</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RecordingObserver(_RecordingObserver):

    def forward(self, x):
        val = x.cpu()
        super().forward(val)
        return x</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li>torch.quantization.observer.RecordingObserver</li>
<li>torch.quantization.observer._ObserverBase</li>
<li>torch.quantization.observer.ObserverBase</li>
<li>abc.ABC</li>
<li>torch.nn.modules.module.Module</li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="optimization.quantization.core.quant.RecordingObserver.tensor_val"><code class="name">var <span class="ident">tensor_val</span> : List[Optional[torch.Tensor]]</code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="optimization.quantization.core.quant.RecordingObserver.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, x) ‑> Callable[..., Any]</span>
</code></dt>
<dd>
<div class="desc"><p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Although the recipe for forward pass needs to be defined within
this function, one should call the :class:<code>Module</code> instance afterwards
instead of this since the former takes care of running the
registered hooks while the latter silently ignores them.</p>
</div></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, x):
    val = x.cpu()
    super().forward(val)
    return x</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="optimization.quantization.core" href="index.html">optimization.quantization.core</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="optimization.quantization.core.quant.quantize_helper" href="#optimization.quantization.core.quant.quantize_helper">quantize_helper</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="optimization.quantization.core.quant.QuantForward" href="#optimization.quantization.core.quant.QuantForward">QuantForward</a></code></h4>
<ul class="">
<li><code><a title="optimization.quantization.core.quant.QuantForward.dump_patches" href="#optimization.quantization.core.quant.QuantForward.dump_patches">dump_patches</a></code></li>
<li><code><a title="optimization.quantization.core.quant.QuantForward.forward" href="#optimization.quantization.core.quant.QuantForward.forward">forward</a></code></li>
<li><code><a title="optimization.quantization.core.quant.QuantForward.training" href="#optimization.quantization.core.quant.QuantForward.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="optimization.quantization.core.quant.QuantGrad" href="#optimization.quantization.core.quant.QuantGrad">QuantGrad</a></code></h4>
<ul class="">
<li><code><a title="optimization.quantization.core.quant.QuantGrad.backward" href="#optimization.quantization.core.quant.QuantGrad.backward">backward</a></code></li>
<li><code><a title="optimization.quantization.core.quant.QuantGrad.forward" href="#optimization.quantization.core.quant.QuantGrad.forward">forward</a></code></li>
<li><code><a title="optimization.quantization.core.quant.QuantGrad.get_bits_length" href="#optimization.quantization.core.quant.QuantGrad.get_bits_length">get_bits_length</a></code></li>
<li><code><a title="optimization.quantization.core.quant.QuantGrad.quant_backward" href="#optimization.quantization.core.quant.QuantGrad.quant_backward">quant_backward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="optimization.quantization.core.quant.Quantizer" href="#optimization.quantization.core.quant.Quantizer">Quantizer</a></code></h4>
<ul class="">
<li><code><a title="optimization.quantization.core.quant.Quantizer.export_model" href="#optimization.quantization.core.quant.Quantizer.export_model">export_model</a></code></li>
<li><code><a title="optimization.quantization.core.quant.Quantizer.export_model_save" href="#optimization.quantization.core.quant.Quantizer.export_model_save">export_model_save</a></code></li>
<li><code><a title="optimization.quantization.core.quant.Quantizer.find_conv_bn_patterns" href="#optimization.quantization.core.quant.Quantizer.find_conv_bn_patterns">find_conv_bn_patterns</a></code></li>
<li><code><a title="optimization.quantization.core.quant.Quantizer.fold_bn" href="#optimization.quantization.core.quant.Quantizer.fold_bn">fold_bn</a></code></li>
<li><code><a title="optimization.quantization.core.quant.Quantizer.load_calibration_config" href="#optimization.quantization.core.quant.Quantizer.load_calibration_config">load_calibration_config</a></code></li>
<li><code><a title="optimization.quantization.core.quant.Quantizer.quantize_input" href="#optimization.quantization.core.quant.Quantizer.quantize_input">quantize_input</a></code></li>
<li><code><a title="optimization.quantization.core.quant.Quantizer.quantize_output" href="#optimization.quantization.core.quant.Quantizer.quantize_output">quantize_output</a></code></li>
<li><code><a title="optimization.quantization.core.quant.Quantizer.quantize_weight" href="#optimization.quantization.core.quant.Quantizer.quantize_weight">quantize_weight</a></code></li>
<li><code><a title="optimization.quantization.core.quant.Quantizer.record_shape" href="#optimization.quantization.core.quant.Quantizer.record_shape">record_shape</a></code></li>
<li><code><a title="optimization.quantization.core.quant.Quantizer.step_with_optimizer" href="#optimization.quantization.core.quant.Quantizer.step_with_optimizer">step_with_optimizer</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="optimization.quantization.core.quant.QuantizerIdentityWrapper" href="#optimization.quantization.core.quant.QuantizerIdentityWrapper">QuantizerIdentityWrapper</a></code></h4>
<ul class="">
<li><code><a title="optimization.quantization.core.quant.QuantizerIdentityWrapper.dump_patches" href="#optimization.quantization.core.quant.QuantizerIdentityWrapper.dump_patches">dump_patches</a></code></li>
<li><code><a title="optimization.quantization.core.quant.QuantizerIdentityWrapper.forward" href="#optimization.quantization.core.quant.QuantizerIdentityWrapper.forward">forward</a></code></li>
<li><code><a title="optimization.quantization.core.quant.QuantizerIdentityWrapper.training" href="#optimization.quantization.core.quant.QuantizerIdentityWrapper.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="optimization.quantization.core.quant.QuantizerModuleWrapper" href="#optimization.quantization.core.quant.QuantizerModuleWrapper">QuantizerModuleWrapper</a></code></h4>
<ul class="">
<li><code><a title="optimization.quantization.core.quant.QuantizerModuleWrapper.dump_patches" href="#optimization.quantization.core.quant.QuantizerModuleWrapper.dump_patches">dump_patches</a></code></li>
<li><code><a title="optimization.quantization.core.quant.QuantizerModuleWrapper.forward" href="#optimization.quantization.core.quant.QuantizerModuleWrapper.forward">forward</a></code></li>
<li><code><a title="optimization.quantization.core.quant.QuantizerModuleWrapper.training" href="#optimization.quantization.core.quant.QuantizerModuleWrapper.training">training</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="optimization.quantization.core.quant.RecordingObserver" href="#optimization.quantization.core.quant.RecordingObserver">RecordingObserver</a></code></h4>
<ul class="">
<li><code><a title="optimization.quantization.core.quant.RecordingObserver.forward" href="#optimization.quantization.core.quant.RecordingObserver.forward">forward</a></code></li>
<li><code><a title="optimization.quantization.core.quant.RecordingObserver.tensor_val" href="#optimization.quantization.core.quant.RecordingObserver.tensor_val">tensor_val</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>