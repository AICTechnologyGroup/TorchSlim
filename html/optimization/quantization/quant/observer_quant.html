<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>optimization.quantization.quant.observer_quant API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>optimization.quantization.quant.observer_quant</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import logging
import torch
from optimization.quantization.core import Quantizer, QuantForward
from collections import defaultdict
from optimization.common.base import QuantizerSchema
from schema import Schema, And, Or, Optional
from torch.quantization import default_weight_observer, default_histogram_observer

logger = logging.getLogger(__name__)


class ObserverQuantizer(Quantizer):
    &#34;&#34;&#34;This quantizer uses observers to record weight/output statistics to get quantization information.
    The whole process can be divided into three steps:

    1. It will register observers to the place where quantization would happen (just like registering hooks).
    2. The observers would record tensors&#39; statistics during calibration.
    3. Scale &amp; zero point would be obtained after calibration.

    Note that the observer type, tensor dtype and quantization qscheme are hard coded for now. Their customization
    are under development and will be ready soon.
    &#34;&#34;&#34;
    def __init__(self, model, config_list, optimizer=None):
        super().__init__(model, config_list, optimizer)
        assert not model.training, &#34;Currently the observer quantizer only works in evaluation mode.&#34;
        self.quant_grad = QuantForward()
        self.device = next(model.parameters()).device
        modules_to_compress = self.get_modules_to_compress()
        all_observers = defaultdict(dict)
        weight_qmin, weight_qmax = -127, 127
        output_qmin, output_qmax = 0, 127 
        self.compressed = False

        for layer, config in modules_to_compress:
            layer_name = layer.name
            module = layer.module
            if &#34;weight&#34; in config.get(&#34;quant_types&#34;, []):
                all_observers[layer_name][&#34;weight&#34;] = default_weight_observer()
                setattr(module, &#34;weight_qmax&#34;, weight_qmax)
                setattr(module, &#34;weight_qmin&#34;, weight_qmin)
            if &#34;input&#34; in config.get(&#34;quant_types&#34;, []):
                all_observers[layer_name][&#34;input&#34;] = default_histogram_observer()
                setattr(module, &#34;input_qmax&#34;, output_qmax)
                setattr(module, &#34;input_qmin&#34;, output_qmin)
            if &#34;output&#34; in config.get(&#34;quant_types&#34;, []):
                all_observers[layer_name][&#34;output&#34;] = default_histogram_observer()
                setattr(module, &#34;output_qmax&#34;, output_qmax)
                setattr(module, &#34;output_qmin&#34;, output_qmin)
        self.all_observers = all_observers
        self.bound_model.to(self.device)

    def validate_config(self, model, config_list):
        schema = QuantizerSchema([{
            Optional(&#39;quant_types&#39;): Schema([lambda x: x in [&#39;weight&#39;, &#39;output&#39;, &#39;input&#39;]]),
            Optional(&#39;quant_bits&#39;): Or(And(int, lambda n: n == 8), Schema({
                Optional(&#39;weight&#39;): And(int, lambda n: n == 8),
                Optional(&#39;output&#39;): And(int, lambda n: n == 8),
                Optional(&#39;input&#39;): And(int, lambda n: n == 8),
            })),
            Optional(&#39;op_types&#39;): [str],
            Optional(&#39;op_names&#39;): [str]
        }], model, logger)

        schema.validate(config_list)

    def record(self, wrapper, quant_type, tensor):
        name = wrapper.name
        observer = self.all_observers[name][quant_type]
        observer(tensor.cpu())

    def calculate_qparams(self, name, quant_type):
        observer = self.all_observers[name][quant_type]
        scale, zero_point = observer.calculate_qparams()
        return scale, zero_point

    def _quantize(self, x, scale, zero_point, qmin, qmax):
        x = x / scale + zero_point
        x = torch.clamp(x, qmin, qmax)
        x = torch.round(x)
        x = (x - zero_point) * scale
        return x

    def quantize_input(self, inputs, wrapper, **kwargs):
        if self.compressed:
            module = wrapper.module
            inputs = self._quantize(inputs,
                                      module.input_scale,
                                      module.input_zero_point,
                                      module.input_qmin,
                                      module.input_qmax)
        else:
            self.record(wrapper, &#39;input&#39;, inputs)
        return inputs

    def quantize_weight(self, wrapper, **kwargs):
        if self.compressed:
            return
        weight = wrapper.module.weight
        self.record(wrapper, &#39;weight&#39;, weight)

    def quantize_output(self, output, wrapper, **kwargs):
        if self.compressed:
            module = wrapper.module
            new_output = self._quantize(output,
                                       module.output_scale,
                                       module.output_zero_point,
                                       module.output_qmin,
                                       module.output_qmax)
        else:
            self.record(wrapper, &#39;output&#39;, output)
            new_output = output
        return new_output

    def compress(self):
        &#34;&#34;&#34;
        Calculate quantization information of each tensor. Note that the inference of
        the compressed model will no longer update the corresponding. Instead, the quantization
        process will be simulated, which is used to test the accuracy of the quantization.
        &#34;&#34;&#34;
        modules_to_compress = self.get_modules_to_compress()
        for layer, config in modules_to_compress:
            module = layer.module
            if &#34;weight&#34; in config.get(&#34;quant_types&#34;, []):
                scale, zero_point = self.calculate_qparams(layer.name, &#39;weight&#39;)
                module.register_buffer(&#39;weight_scale&#39;, scale.to(self.device))
                module.register_buffer(&#39;weight_zero_point&#39;, zero_point.to(self.device))
                weight = module.weight
                quantized_weight = self._quantize(weight,
                                            module.weight_scale,
                                            module.weight_zero_point,
                                            module.weight_qmin,
                                            module.weight_qmax)
                delattr(module, &#39;weight&#39;)
                module.register_buffer(&#39;weight&#39;, quantized_weight)
            if &#34;input&#34; in config.get(&#34;quant_types&#34;, []):
                scale, zero_point = self.calculate_qparams(layer.name, &#39;input&#39;)
                module.register_buffer(&#39;input_scale&#39;, scale.to(self.device))
                module.register_buffer(&#39;input_zero_point&#39;, zero_point.to(self.device))
            if &#34;output&#34; in config.get(&#34;quant_types&#34;, []):
                scale, zero_point = self.calculate_qparams(layer.name, &#39;output&#39;)
                module.register_buffer(&#39;output_scale&#39;, scale.to(self.device))
                module.register_buffer(&#39;output_zero_point&#39;, zero_point.to(self.device))
        self.compressed = True
        super().compress()

    def export_model(self, model_path, calibration_path=None, onnx_path=None, input_shape=None, device=None):
        &#34;&#34;&#34;
        Export quantized model weights and calibration parameters(optional)

        Parameters
        ----------
        model_path : str
            path to save quantized model weight
        calibration_path : str
            (optional) path to save quantize parameters after calibration
        onnx_path : str
            (optional) path to save onnx model
        input_shape : list or tuple
            input shape to onnx model
        device : torch.device
            device of the model, used to place the dummy input tensor for exporting onnx file.
            the tensor is placed on cpu if ```device``` is None

        Returns
        -------
        Dict
        &#34;&#34;&#34;
        assert model_path is not None, &#39;model_path must be specified&#39;
        self._unwrap_model()
        calibration_config = {}

        for name, module in self.bound_model.named_modules():
            if hasattr(module, &#39;weight_scale&#39;) or hasattr(module, &#39;input_scale&#39;) or hasattr(module, &#39;output_scale&#39;):
                calibration_config[name] = {}
            if hasattr(module, &#39;weight_scale&#39;):
                calibration_config[name][&#39;weight_bits&#39;] = 8
                val = float(module.weight_scale * module.weight_qmax)
                calibration_config[name][&#39;tracked_max_weight&#39;] = val
                calibration_config[name][&#39;tracked_min_weight&#39;] = -val
                calibration_config[name][&#39;tracked_qmin_weight&#39;] = -127
                calibration_config[name][&#39;tracked_qmax_weight&#39;] = 127
                weight = module.weight
                quantized_weight = self._quantize(weight,
                                            module.weight_scale,
                                            module.weight_zero_point,
                                            module.weight_qmin,
                                            module.weight_qmax)
                delattr(module, &#39;weight&#39;)
                module.register_parameter(&#39;weight&#39;, torch.nn.Parameter(quantized_weight))
            if hasattr(module, &#39;input_scale&#39;):
                calibration_config[name][&#39;input_bits&#39;] = 8
                max_input = float(module.input_scale * (module.input_qmax - module.input_zero_point))
                min_input = float(module.input_scale * (module.input_qmin - module.input_zero_point))
                calibration_config[name][&#39;tracked_min_input&#39;] = min_input
                calibration_config[name][&#39;tracked_max_input&#39;] = max_input
                calibration_config[name][&#39;tracked_qmin_input&#39;] = 0
                calibration_config[name][&#39;tracked_qmax_input&#39;] = 127
            if hasattr(module, &#39;output_scale&#39;):
                calibration_config[name][&#39;output_bits&#39;] = 8
                max_input = float(module.output_scale * (module.output_qmax - module.output_zero_point))
                min_input = float(module.output_scale * (module.output_qmin - module.output_zero_point))
                calibration_config[name][&#39;tracked_min_output&#39;] = min_input
                calibration_config[name][&#39;tracked_max_output&#39;] = max_input
                calibration_config[name][&#39;tracked_qmin_output&#39;] = 0
                calibration_config[name][&#39;tracked_qmax_output&#39;] = 127
            self._del_simulated_attr(module)

        self.export_model_save(self.bound_model, model_path, calibration_config, calibration_path, onnx_path,
                               input_shape, device)

        return calibration_config

    def _del_simulated_attr(self, module):
        &#34;&#34;&#34;
        delete redundant parameters in quantize module
        &#34;&#34;&#34;
        del_attr_list = [&#39;old_weight&#39;, &#39;steps&#39;, &#39;weight_qmax&#39;, &#39;weight_qmin&#39;, &#39;input_qmax&#39;, &#39;input_qmin&#39;,
                         &#39;output_qmax&#39;, &#39;output_qmin&#39;, &#39;weight_scale&#39;, &#39;weight_zero_point&#39;, &#39;input_scale&#39;,
                         &#39;input_zero_point&#39;, &#39;output_scale&#39;, &#39;output_zero_point&#39;]
        for attr in del_attr_list:
            if hasattr(module, attr):
                delattr(module, attr)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="optimization.quantization.quant.observer_quant.ObserverQuantizer"><code class="flex name class">
<span>class <span class="ident">ObserverQuantizer</span></span>
<span>(</span><span>model, config_list, optimizer=None)</span>
</code></dt>
<dd>
<div class="desc"><p>This quantizer uses observers to record weight/output statistics to get quantization information.
The whole process can be divided into three steps:</p>
<ol>
<li>It will register observers to the place where quantization would happen (just like registering hooks).</li>
<li>The observers would record tensors' statistics during calibration.</li>
<li>Scale &amp; zero point would be obtained after calibration.</li>
</ol>
<p>Note that the observer type, tensor dtype and quantization qscheme are hard coded for now. Their customization
are under development and will be ready soon.</p>
<p>Base quantizer for pytorch quantizer</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ObserverQuantizer(Quantizer):
    &#34;&#34;&#34;This quantizer uses observers to record weight/output statistics to get quantization information.
    The whole process can be divided into three steps:

    1. It will register observers to the place where quantization would happen (just like registering hooks).
    2. The observers would record tensors&#39; statistics during calibration.
    3. Scale &amp; zero point would be obtained after calibration.

    Note that the observer type, tensor dtype and quantization qscheme are hard coded for now. Their customization
    are under development and will be ready soon.
    &#34;&#34;&#34;
    def __init__(self, model, config_list, optimizer=None):
        super().__init__(model, config_list, optimizer)
        assert not model.training, &#34;Currently the observer quantizer only works in evaluation mode.&#34;
        self.quant_grad = QuantForward()
        self.device = next(model.parameters()).device
        modules_to_compress = self.get_modules_to_compress()
        all_observers = defaultdict(dict)
        weight_qmin, weight_qmax = -127, 127
        output_qmin, output_qmax = 0, 127 
        self.compressed = False

        for layer, config in modules_to_compress:
            layer_name = layer.name
            module = layer.module
            if &#34;weight&#34; in config.get(&#34;quant_types&#34;, []):
                all_observers[layer_name][&#34;weight&#34;] = default_weight_observer()
                setattr(module, &#34;weight_qmax&#34;, weight_qmax)
                setattr(module, &#34;weight_qmin&#34;, weight_qmin)
            if &#34;input&#34; in config.get(&#34;quant_types&#34;, []):
                all_observers[layer_name][&#34;input&#34;] = default_histogram_observer()
                setattr(module, &#34;input_qmax&#34;, output_qmax)
                setattr(module, &#34;input_qmin&#34;, output_qmin)
            if &#34;output&#34; in config.get(&#34;quant_types&#34;, []):
                all_observers[layer_name][&#34;output&#34;] = default_histogram_observer()
                setattr(module, &#34;output_qmax&#34;, output_qmax)
                setattr(module, &#34;output_qmin&#34;, output_qmin)
        self.all_observers = all_observers
        self.bound_model.to(self.device)

    def validate_config(self, model, config_list):
        schema = QuantizerSchema([{
            Optional(&#39;quant_types&#39;): Schema([lambda x: x in [&#39;weight&#39;, &#39;output&#39;, &#39;input&#39;]]),
            Optional(&#39;quant_bits&#39;): Or(And(int, lambda n: n == 8), Schema({
                Optional(&#39;weight&#39;): And(int, lambda n: n == 8),
                Optional(&#39;output&#39;): And(int, lambda n: n == 8),
                Optional(&#39;input&#39;): And(int, lambda n: n == 8),
            })),
            Optional(&#39;op_types&#39;): [str],
            Optional(&#39;op_names&#39;): [str]
        }], model, logger)

        schema.validate(config_list)

    def record(self, wrapper, quant_type, tensor):
        name = wrapper.name
        observer = self.all_observers[name][quant_type]
        observer(tensor.cpu())

    def calculate_qparams(self, name, quant_type):
        observer = self.all_observers[name][quant_type]
        scale, zero_point = observer.calculate_qparams()
        return scale, zero_point

    def _quantize(self, x, scale, zero_point, qmin, qmax):
        x = x / scale + zero_point
        x = torch.clamp(x, qmin, qmax)
        x = torch.round(x)
        x = (x - zero_point) * scale
        return x

    def quantize_input(self, inputs, wrapper, **kwargs):
        if self.compressed:
            module = wrapper.module
            inputs = self._quantize(inputs,
                                      module.input_scale,
                                      module.input_zero_point,
                                      module.input_qmin,
                                      module.input_qmax)
        else:
            self.record(wrapper, &#39;input&#39;, inputs)
        return inputs

    def quantize_weight(self, wrapper, **kwargs):
        if self.compressed:
            return
        weight = wrapper.module.weight
        self.record(wrapper, &#39;weight&#39;, weight)

    def quantize_output(self, output, wrapper, **kwargs):
        if self.compressed:
            module = wrapper.module
            new_output = self._quantize(output,
                                       module.output_scale,
                                       module.output_zero_point,
                                       module.output_qmin,
                                       module.output_qmax)
        else:
            self.record(wrapper, &#39;output&#39;, output)
            new_output = output
        return new_output

    def compress(self):
        &#34;&#34;&#34;
        Calculate quantization information of each tensor. Note that the inference of
        the compressed model will no longer update the corresponding. Instead, the quantization
        process will be simulated, which is used to test the accuracy of the quantization.
        &#34;&#34;&#34;
        modules_to_compress = self.get_modules_to_compress()
        for layer, config in modules_to_compress:
            module = layer.module
            if &#34;weight&#34; in config.get(&#34;quant_types&#34;, []):
                scale, zero_point = self.calculate_qparams(layer.name, &#39;weight&#39;)
                module.register_buffer(&#39;weight_scale&#39;, scale.to(self.device))
                module.register_buffer(&#39;weight_zero_point&#39;, zero_point.to(self.device))
                weight = module.weight
                quantized_weight = self._quantize(weight,
                                            module.weight_scale,
                                            module.weight_zero_point,
                                            module.weight_qmin,
                                            module.weight_qmax)
                delattr(module, &#39;weight&#39;)
                module.register_buffer(&#39;weight&#39;, quantized_weight)
            if &#34;input&#34; in config.get(&#34;quant_types&#34;, []):
                scale, zero_point = self.calculate_qparams(layer.name, &#39;input&#39;)
                module.register_buffer(&#39;input_scale&#39;, scale.to(self.device))
                module.register_buffer(&#39;input_zero_point&#39;, zero_point.to(self.device))
            if &#34;output&#34; in config.get(&#34;quant_types&#34;, []):
                scale, zero_point = self.calculate_qparams(layer.name, &#39;output&#39;)
                module.register_buffer(&#39;output_scale&#39;, scale.to(self.device))
                module.register_buffer(&#39;output_zero_point&#39;, zero_point.to(self.device))
        self.compressed = True
        super().compress()

    def export_model(self, model_path, calibration_path=None, onnx_path=None, input_shape=None, device=None):
        &#34;&#34;&#34;
        Export quantized model weights and calibration parameters(optional)

        Parameters
        ----------
        model_path : str
            path to save quantized model weight
        calibration_path : str
            (optional) path to save quantize parameters after calibration
        onnx_path : str
            (optional) path to save onnx model
        input_shape : list or tuple
            input shape to onnx model
        device : torch.device
            device of the model, used to place the dummy input tensor for exporting onnx file.
            the tensor is placed on cpu if ```device``` is None

        Returns
        -------
        Dict
        &#34;&#34;&#34;
        assert model_path is not None, &#39;model_path must be specified&#39;
        self._unwrap_model()
        calibration_config = {}

        for name, module in self.bound_model.named_modules():
            if hasattr(module, &#39;weight_scale&#39;) or hasattr(module, &#39;input_scale&#39;) or hasattr(module, &#39;output_scale&#39;):
                calibration_config[name] = {}
            if hasattr(module, &#39;weight_scale&#39;):
                calibration_config[name][&#39;weight_bits&#39;] = 8
                val = float(module.weight_scale * module.weight_qmax)
                calibration_config[name][&#39;tracked_max_weight&#39;] = val
                calibration_config[name][&#39;tracked_min_weight&#39;] = -val
                calibration_config[name][&#39;tracked_qmin_weight&#39;] = -127
                calibration_config[name][&#39;tracked_qmax_weight&#39;] = 127
                weight = module.weight
                quantized_weight = self._quantize(weight,
                                            module.weight_scale,
                                            module.weight_zero_point,
                                            module.weight_qmin,
                                            module.weight_qmax)
                delattr(module, &#39;weight&#39;)
                module.register_parameter(&#39;weight&#39;, torch.nn.Parameter(quantized_weight))
            if hasattr(module, &#39;input_scale&#39;):
                calibration_config[name][&#39;input_bits&#39;] = 8
                max_input = float(module.input_scale * (module.input_qmax - module.input_zero_point))
                min_input = float(module.input_scale * (module.input_qmin - module.input_zero_point))
                calibration_config[name][&#39;tracked_min_input&#39;] = min_input
                calibration_config[name][&#39;tracked_max_input&#39;] = max_input
                calibration_config[name][&#39;tracked_qmin_input&#39;] = 0
                calibration_config[name][&#39;tracked_qmax_input&#39;] = 127
            if hasattr(module, &#39;output_scale&#39;):
                calibration_config[name][&#39;output_bits&#39;] = 8
                max_input = float(module.output_scale * (module.output_qmax - module.output_zero_point))
                min_input = float(module.output_scale * (module.output_qmin - module.output_zero_point))
                calibration_config[name][&#39;tracked_min_output&#39;] = min_input
                calibration_config[name][&#39;tracked_max_output&#39;] = max_input
                calibration_config[name][&#39;tracked_qmin_output&#39;] = 0
                calibration_config[name][&#39;tracked_qmax_output&#39;] = 127
            self._del_simulated_attr(module)

        self.export_model_save(self.bound_model, model_path, calibration_config, calibration_path, onnx_path,
                               input_shape, device)

        return calibration_config

    def _del_simulated_attr(self, module):
        &#34;&#34;&#34;
        delete redundant parameters in quantize module
        &#34;&#34;&#34;
        del_attr_list = [&#39;old_weight&#39;, &#39;steps&#39;, &#39;weight_qmax&#39;, &#39;weight_qmin&#39;, &#39;input_qmax&#39;, &#39;input_qmin&#39;,
                         &#39;output_qmax&#39;, &#39;output_qmin&#39;, &#39;weight_scale&#39;, &#39;weight_zero_point&#39;, &#39;input_scale&#39;,
                         &#39;input_zero_point&#39;, &#39;output_scale&#39;, &#39;output_zero_point&#39;]
        for attr in del_attr_list:
            if hasattr(module, attr):
                delattr(module, attr)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="optimization.quantization.core.quant.Quantizer" href="../core/quant.html#optimization.quantization.core.quant.Quantizer">Quantizer</a></li>
<li><a title="optimization.common.base.compressor.Compressor" href="../../common/base/compressor.html#optimization.common.base.compressor.Compressor">Compressor</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="optimization.quantization.quant.observer_quant.ObserverQuantizer.calculate_qparams"><code class="name flex">
<span>def <span class="ident">calculate_qparams</span></span>(<span>self, name, quant_type)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calculate_qparams(self, name, quant_type):
    observer = self.all_observers[name][quant_type]
    scale, zero_point = observer.calculate_qparams()
    return scale, zero_point</code></pre>
</details>
</dd>
<dt id="optimization.quantization.quant.observer_quant.ObserverQuantizer.compress"><code class="name flex">
<span>def <span class="ident">compress</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate quantization information of each tensor. Note that the inference of
the compressed model will no longer update the corresponding. Instead, the quantization
process will be simulated, which is used to test the accuracy of the quantization.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def compress(self):
    &#34;&#34;&#34;
    Calculate quantization information of each tensor. Note that the inference of
    the compressed model will no longer update the corresponding. Instead, the quantization
    process will be simulated, which is used to test the accuracy of the quantization.
    &#34;&#34;&#34;
    modules_to_compress = self.get_modules_to_compress()
    for layer, config in modules_to_compress:
        module = layer.module
        if &#34;weight&#34; in config.get(&#34;quant_types&#34;, []):
            scale, zero_point = self.calculate_qparams(layer.name, &#39;weight&#39;)
            module.register_buffer(&#39;weight_scale&#39;, scale.to(self.device))
            module.register_buffer(&#39;weight_zero_point&#39;, zero_point.to(self.device))
            weight = module.weight
            quantized_weight = self._quantize(weight,
                                        module.weight_scale,
                                        module.weight_zero_point,
                                        module.weight_qmin,
                                        module.weight_qmax)
            delattr(module, &#39;weight&#39;)
            module.register_buffer(&#39;weight&#39;, quantized_weight)
        if &#34;input&#34; in config.get(&#34;quant_types&#34;, []):
            scale, zero_point = self.calculate_qparams(layer.name, &#39;input&#39;)
            module.register_buffer(&#39;input_scale&#39;, scale.to(self.device))
            module.register_buffer(&#39;input_zero_point&#39;, zero_point.to(self.device))
        if &#34;output&#34; in config.get(&#34;quant_types&#34;, []):
            scale, zero_point = self.calculate_qparams(layer.name, &#39;output&#39;)
            module.register_buffer(&#39;output_scale&#39;, scale.to(self.device))
            module.register_buffer(&#39;output_zero_point&#39;, zero_point.to(self.device))
    self.compressed = True
    super().compress()</code></pre>
</details>
</dd>
<dt id="optimization.quantization.quant.observer_quant.ObserverQuantizer.export_model"><code class="name flex">
<span>def <span class="ident">export_model</span></span>(<span>self, model_path, calibration_path=None, onnx_path=None, input_shape=None, device=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Export quantized model weights and calibration parameters(optional)</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model_path</code></strong> :&ensp;<code>str</code></dt>
<dd>path to save quantized model weight</dd>
<dt><strong><code>calibration_path</code></strong> :&ensp;<code>str</code></dt>
<dd>(optional) path to save quantize parameters after calibration</dd>
<dt><strong><code>onnx_path</code></strong> :&ensp;<code>str</code></dt>
<dd>(optional) path to save onnx model</dd>
<dt><strong><code>input_shape</code></strong> :&ensp;<code>list</code> or <code>tuple</code></dt>
<dd>input shape to onnx model</dd>
<dt><strong><code>device</code></strong> :&ensp;<code>torch.device</code></dt>
<dd>device of the model, used to place the dummy input tensor for exporting onnx file.
the tensor is placed on cpu if <code>device</code> is None</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Dict</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def export_model(self, model_path, calibration_path=None, onnx_path=None, input_shape=None, device=None):
    &#34;&#34;&#34;
    Export quantized model weights and calibration parameters(optional)

    Parameters
    ----------
    model_path : str
        path to save quantized model weight
    calibration_path : str
        (optional) path to save quantize parameters after calibration
    onnx_path : str
        (optional) path to save onnx model
    input_shape : list or tuple
        input shape to onnx model
    device : torch.device
        device of the model, used to place the dummy input tensor for exporting onnx file.
        the tensor is placed on cpu if ```device``` is None

    Returns
    -------
    Dict
    &#34;&#34;&#34;
    assert model_path is not None, &#39;model_path must be specified&#39;
    self._unwrap_model()
    calibration_config = {}

    for name, module in self.bound_model.named_modules():
        if hasattr(module, &#39;weight_scale&#39;) or hasattr(module, &#39;input_scale&#39;) or hasattr(module, &#39;output_scale&#39;):
            calibration_config[name] = {}
        if hasattr(module, &#39;weight_scale&#39;):
            calibration_config[name][&#39;weight_bits&#39;] = 8
            val = float(module.weight_scale * module.weight_qmax)
            calibration_config[name][&#39;tracked_max_weight&#39;] = val
            calibration_config[name][&#39;tracked_min_weight&#39;] = -val
            calibration_config[name][&#39;tracked_qmin_weight&#39;] = -127
            calibration_config[name][&#39;tracked_qmax_weight&#39;] = 127
            weight = module.weight
            quantized_weight = self._quantize(weight,
                                        module.weight_scale,
                                        module.weight_zero_point,
                                        module.weight_qmin,
                                        module.weight_qmax)
            delattr(module, &#39;weight&#39;)
            module.register_parameter(&#39;weight&#39;, torch.nn.Parameter(quantized_weight))
        if hasattr(module, &#39;input_scale&#39;):
            calibration_config[name][&#39;input_bits&#39;] = 8
            max_input = float(module.input_scale * (module.input_qmax - module.input_zero_point))
            min_input = float(module.input_scale * (module.input_qmin - module.input_zero_point))
            calibration_config[name][&#39;tracked_min_input&#39;] = min_input
            calibration_config[name][&#39;tracked_max_input&#39;] = max_input
            calibration_config[name][&#39;tracked_qmin_input&#39;] = 0
            calibration_config[name][&#39;tracked_qmax_input&#39;] = 127
        if hasattr(module, &#39;output_scale&#39;):
            calibration_config[name][&#39;output_bits&#39;] = 8
            max_input = float(module.output_scale * (module.output_qmax - module.output_zero_point))
            min_input = float(module.output_scale * (module.output_qmin - module.output_zero_point))
            calibration_config[name][&#39;tracked_min_output&#39;] = min_input
            calibration_config[name][&#39;tracked_max_output&#39;] = max_input
            calibration_config[name][&#39;tracked_qmin_output&#39;] = 0
            calibration_config[name][&#39;tracked_qmax_output&#39;] = 127
        self._del_simulated_attr(module)

    self.export_model_save(self.bound_model, model_path, calibration_config, calibration_path, onnx_path,
                           input_shape, device)

    return calibration_config</code></pre>
</details>
</dd>
<dt id="optimization.quantization.quant.observer_quant.ObserverQuantizer.record"><code class="name flex">
<span>def <span class="ident">record</span></span>(<span>self, wrapper, quant_type, tensor)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def record(self, wrapper, quant_type, tensor):
    name = wrapper.name
    observer = self.all_observers[name][quant_type]
    observer(tensor.cpu())</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="optimization.quantization.core.quant.Quantizer" href="../core/quant.html#optimization.quantization.core.quant.Quantizer">Quantizer</a></b></code>:
<ul class="hlist">
<li><code><a title="optimization.quantization.core.quant.Quantizer.export_model_save" href="../core/quant.html#optimization.quantization.core.quant.Quantizer.export_model_save">export_model_save</a></code></li>
<li><code><a title="optimization.quantization.core.quant.Quantizer.find_conv_bn_patterns" href="../core/quant.html#optimization.quantization.core.quant.Quantizer.find_conv_bn_patterns">find_conv_bn_patterns</a></code></li>
<li><code><a title="optimization.quantization.core.quant.Quantizer.fold_bn" href="../core/quant.html#optimization.quantization.core.quant.Quantizer.fold_bn">fold_bn</a></code></li>
<li><code><a title="optimization.quantization.core.quant.Quantizer.get_modules_to_compress" href="../../common/base/compressor.html#optimization.common.base.compressor.Compressor.get_modules_to_compress">get_modules_to_compress</a></code></li>
<li><code><a title="optimization.quantization.core.quant.Quantizer.get_modules_wrapper" href="../../common/base/compressor.html#optimization.common.base.compressor.Compressor.get_modules_wrapper">get_modules_wrapper</a></code></li>
<li><code><a title="optimization.quantization.core.quant.Quantizer.load_calibration_config" href="../core/quant.html#optimization.quantization.core.quant.Quantizer.load_calibration_config">load_calibration_config</a></code></li>
<li><code><a title="optimization.quantization.core.quant.Quantizer.quantize_input" href="../core/quant.html#optimization.quantization.core.quant.Quantizer.quantize_input">quantize_input</a></code></li>
<li><code><a title="optimization.quantization.core.quant.Quantizer.quantize_output" href="../core/quant.html#optimization.quantization.core.quant.Quantizer.quantize_output">quantize_output</a></code></li>
<li><code><a title="optimization.quantization.core.quant.Quantizer.quantize_weight" href="../core/quant.html#optimization.quantization.core.quant.Quantizer.quantize_weight">quantize_weight</a></code></li>
<li><code><a title="optimization.quantization.core.quant.Quantizer.record_shape" href="../core/quant.html#optimization.quantization.core.quant.Quantizer.record_shape">record_shape</a></code></li>
<li><code><a title="optimization.quantization.core.quant.Quantizer.reset" href="../../common/base/compressor.html#optimization.common.base.compressor.Compressor.reset">reset</a></code></li>
<li><code><a title="optimization.quantization.core.quant.Quantizer.select_config" href="../../common/base/compressor.html#optimization.common.base.compressor.Compressor.select_config">select_config</a></code></li>
<li><code><a title="optimization.quantization.core.quant.Quantizer.set_wrappers_attribute" href="../../common/base/compressor.html#optimization.common.base.compressor.Compressor.set_wrappers_attribute">set_wrappers_attribute</a></code></li>
<li><code><a title="optimization.quantization.core.quant.Quantizer.update_epoch" href="../../common/base/compressor.html#optimization.common.base.compressor.Compressor.update_epoch">update_epoch</a></code></li>
<li><code><a title="optimization.quantization.core.quant.Quantizer.validate_config" href="../../common/base/compressor.html#optimization.common.base.compressor.Compressor.validate_config">validate_config</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="optimization.quantization.quant" href="index.html">optimization.quantization.quant</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="optimization.quantization.quant.observer_quant.ObserverQuantizer" href="#optimization.quantization.quant.observer_quant.ObserverQuantizer">ObserverQuantizer</a></code></h4>
<ul class="">
<li><code><a title="optimization.quantization.quant.observer_quant.ObserverQuantizer.calculate_qparams" href="#optimization.quantization.quant.observer_quant.ObserverQuantizer.calculate_qparams">calculate_qparams</a></code></li>
<li><code><a title="optimization.quantization.quant.observer_quant.ObserverQuantizer.compress" href="#optimization.quantization.quant.observer_quant.ObserverQuantizer.compress">compress</a></code></li>
<li><code><a title="optimization.quantization.quant.observer_quant.ObserverQuantizer.export_model" href="#optimization.quantization.quant.observer_quant.ObserverQuantizer.export_model">export_model</a></code></li>
<li><code><a title="optimization.quantization.quant.observer_quant.ObserverQuantizer.record" href="#optimization.quantization.quant.observer_quant.ObserverQuantizer.record">record</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>