<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>optimization.quantization.quant.qat API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>optimization.quantization.quant.qat</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import torch
import logging
from schema import Optional, And, Or, Schema
from optimization.quantization.core import Quantizer, QuantGrad, LayerQuantSetting, QuantType, QuantDtype, QuantScheme, get_quant_shape, get_min_max_value, calculate_qmin_qmax
from optimization.quantization.core import PER_CHANNEL_QUANT_SCHEME, BN_FOLD_TAG
from optimization.common.base import QuantizerSchema


logger = logging.getLogger(__name__)



def update_ema(biased_ema, value, decay):
    &#34;&#34;&#34;
    calculate biased stat and unbiased stat in each step using exponential moving average method

    Parameters
    ----------
    biased_ema : float
        previous stat value
    value : float
        current stat value
    decay : float
        the weight of previous stat value, larger means smoother curve

    Returns
    -------
    float, float
    &#34;&#34;&#34;
    biased_ema = biased_ema * decay + (1 - decay) * value
    return biased_ema

def update_quantization_param(bits, rmin, rmax, dtype, schema):
    &#34;&#34;&#34;
    calculate the `zero_point` and `scale`.

    Parameters
    ----------
    bits : int
        quantization bits length
    rmin : Tensor
        min value of real value
    rmax : Tensor
        max value of real value
    dtype : QuantDtype
        quantized data type
    scheme : QuantScheme
        quantization scheme to be used
    Returns
    -------
    float, float
    &#34;&#34;&#34;
    rmin = torch.min(rmin, torch.zeros_like(rmin))
    rmax = torch.max(rmax, torch.zeros_like(rmax))

    zero_point = torch.zeros_like(rmin)

    qmin, qmax = calculate_qmin_qmax(bits, dtype)
    if schema in [QuantScheme.PER_TENSOR_SYMMETRIC, QuantScheme.PER_CHANNEL_SYMMETRIC]:
        abs_max = torch.max(torch.abs(rmin), torch.abs(rmax))
        scale = abs_max / (float(qmax - qmin) / 2)
        if dtype == QuantDtype.UINT:
            zero_point_val = (qmin + qmax) // 2
            zero_point = zero_point.new_full(zero_point.size(), zero_point_val)
    else:
        scale = (rmax - rmin) / float(qmax - qmin)
        zero_point = qmin - torch.round(rmin / scale)
    zero_point = torch.clamp(zero_point, qmin, qmax)
    
    return scale, zero_point


class QATGrad(QuantGrad):
    @staticmethod
    def quant_backward(tensor, grad_output, quant_type, scale, zero_point, qmin, qmax):
        tensor_q = QuantGrad._quantize(tensor, grad_output, quant_type, scale)
        mask = (tensor_q &lt; qmin) | (tensor_q &gt; qmax)
        grad_output[mask] = 0
        return grad_output

class QAT_Quantizer(Quantizer):
    &#34;&#34;&#34;Quantizer defined in:
    Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference
    http://openaccess.thecvf.com/content_cvpr_2018/papers/Jacob_Quantization_and_Training_CVPR_2018_paper.pdf
    &#34;&#34;&#34;
    def __init__(self, model, config_list, optimizer, dummy_input=None):
        &#34;&#34;&#34;
        Parameters
        ----------
        layer : LayerInfo
            the layer to quantize
        config_list : list of dict
            list of configurations for quantization
            supported keys for dict:
                - quant_types : list of string
                    type of quantization you want to apply, currently support &#39;weight&#39;, &#39;input&#39;, &#39;output&#39;
                - quant_bits : int or dict of {str : int}
                    bits length of quantization, key is the quantization type, value is the length, eg. {&#39;weight&#39;, 8},
                    when the type is int, all quantization types share same bits length
                - quant_start_step : int
                    disable quantization until model are run by certain number of steps, this allows the network to enter a more stable
                    state where output quantization ranges do not exclude a signiÔ¨Åcant fraction of values, default value is 0
                - op_types : list of string
                    types of nn.module you want to apply quantization, eg. &#39;Conv2d&#39;
                - dummy_input : tuple of tensor
                    inputs to the model, which are used to get the graph of the module. The graph is used to find
                    Conv-Bn patterns. And then the batch normalization folding would be enabled. If dummy_input is not
                    given, the batch normalization folding would be disabled.
        &#34;&#34;&#34;
        assert isinstance(optimizer, torch.optim.Optimizer), &#34;unrecognized optimizer type&#34;
        super().__init__(model, config_list, optimizer, dummy_input)
        self.quant_grad = QATGrad.apply
        modules_to_compress = self.get_modules_to_compress()
        device = next(model.parameters()).device
        self.bound_model.register_buffer(&#34;steps&#34;, torch.tensor(1))
        for layer, config in modules_to_compress:
            module = layer.module
            name = layer.name
            assert name in self.all_shapes, &#34;Could not found shapes for layer {}&#34;.format(name)
            input_shape, output_shape = self.all_shapes[name]
            layer_quant_setting = LayerQuantSetting(config)
            layer_quant_setting.ema_decay = 0.99
            quant_start_step = config.get(&#39;quant_start_step&#39;, 0)
            layer_quant_setting.quant_start_step = quant_start_step
            if isinstance(module, torch.nn.Linear):
                if &#34;input&#34; in config.get(&#34;quant_types&#34;, []) and \
                        layer_quant_setting.input.quant_scheme in PER_CHANNEL_QUANT_SCHEME:
                    if len(input_shape) != 2:
                        logger.warning(&#34;When quantize torch.nn.Linear, make sure that the rank of the inputs &#34;
                                       &#34;of the layer is 2. Skip quantization of layer %s.&#34;, name)
                        continue
                if &#34;output&#34; in config.get(&#34;quant_types&#34;, []) and \
                        layer_quant_setting.output.quant_scheme in PER_CHANNEL_QUANT_SCHEME:
                    if len(output_shape) != 2:
                        logger.warning(&#34;When quantize torch.nn.Linear, make sure that the rank of the outputs &#34;
                                       &#34;of the layer is 2. Skip quantization of layer %s.&#34;, name)
                        continue

            if &#34;weight&#34; in config.get(&#34;quant_types&#34;, []):
                quant_shape = get_quant_shape(module.weight.shape, QuantType.WEIGHT, layer_quant_setting.weight.quant_scheme)
                module.register_buffer(&#39;weight_scale&#39;, torch.zeros(quant_shape))
                module.register_buffer(&#39;weight_zero_point&#39;, torch.zeros(quant_shape))

            if &#34;input&#34; in config.get(&#34;quant_types&#34;, []):
                quant_shape = get_quant_shape(input_shape, QuantType.INPUT, layer_quant_setting.input.quant_scheme)
                module.register_buffer(&#39;tracked_min_input&#39;, torch.zeros(quant_shape))
                module.register_buffer(&#39;tracked_max_input&#39;, torch.zeros(quant_shape))
                module.register_buffer(&#39;input_scale&#39;, torch.zeros(quant_shape))
                module.register_buffer(&#39;input_zero_point&#39;, torch.zeros(quant_shape))

            if &#34;output&#34; in config.get(&#34;quant_types&#34;, []):
                quant_shape = get_quant_shape(output_shape, QuantType.OUTPUT, layer_quant_setting.output.quant_scheme)
                module.register_buffer(&#39;tracked_min_output&#39;, torch.zeros(quant_shape))
                module.register_buffer(&#39;tracked_max_output&#39;, torch.zeros(quant_shape))
                module.register_buffer(&#39;output_scale&#39;, torch.zeros(quant_shape))
                module.register_buffer(&#39;output_zero_point&#39;, torch.zeros(quant_shape))

            setattr(module, &#34;layer_quant_setting&#34;, layer_quant_setting)
        self.bound_model.to(device)

    def _del_simulated_attr(self, module):
        &#34;&#34;&#34;
        delete redundant parameters in quantize module
        &#34;&#34;&#34;
        del_attr_list = [&#39;old_weight&#39;, &#39;old_bias&#39;, &#39;ema_decay&#39;, &#39;tracked_min_output&#39;, &#39;tracked_max_output&#39;,
                         &#39;tracked_min_input&#39;, &#39;tracked_max_input&#39;, &#39;BN_FOLD_TAG&#39;,
                         &#39;weight_scale&#39;, &#39;weight_zero_point&#39;, &#39;input_scale&#39;, &#39;input_zero_point&#39;,
                         &#39;output_scale&#39;, &#39;output_zero_point&#39;, &#39;layer_quant_setting&#39;]
        for attr in del_attr_list:
            if hasattr(module, attr):
                delattr(module, attr)

    def validate_config(self, model, config_list):
        &#34;&#34;&#34;
        Parameters
        ----------
        model : torch.nn.Module
            Model to be pruned
        config_list : list of dict
            List of configurations
        &#34;&#34;&#34;
        SUPPORTED_OPS = [&#39;Conv2d&#39;, &#39;Linear&#39;, &#39;ReLU&#39;, &#39;ReLU6&#39;]
        schema = QuantizerSchema([{
            Optional(&#39;quant_types&#39;): Schema([lambda x: x in [&#39;weight&#39;, &#39;output&#39;, &#39;input&#39;]]),
            Optional(&#39;quant_bits&#39;): Or(And(int, lambda n: 0 &lt; n &lt; 32), Schema({
                Optional(&#39;input&#39;): And(int, lambda n: 0 &lt; n &lt; 32),
                Optional(&#39;weight&#39;): And(int, lambda n: 0 &lt; n &lt; 32),
                Optional(&#39;output&#39;): And(int, lambda n: 0 &lt; n &lt; 32),
            })),
            Optional(&#39;quant_scheme&#39;): Or(lambda x: x in QuantScheme, Schema({
                Optional(&#39;input&#39;): lambda x: x in QuantScheme,
                Optional(&#39;weight&#39;): lambda x: x in QuantScheme,
                Optional(&#39;output&#39;): lambda x: x in QuantScheme
            })),
            Optional(&#39;quant_dtype&#39;): Or(lambda x: x in QuantDtype, Schema({
                Optional(&#39;input&#39;): lambda x: x in QuantDtype,
                Optional(&#39;weight&#39;): lambda x: x in QuantDtype,
                Optional(&#39;output&#39;): lambda x: x in QuantDtype
            })),
            Optional(&#39;quant_start_step&#39;): And(int, lambda n: n &gt;= 0),
            Optional(&#39;op_types&#39;): [And(str, lambda n: n in SUPPORTED_OPS)],
            Optional(&#39;op_names&#39;): [str],
            Optional(&#39;exclude&#39;): bool
        }], model, logger)

        schema.validate(config_list)

    def _quantize(self, real_value, scale, zero_point, qmin, qmax):
        &#34;&#34;&#34;
        quantize real value.

        Parameters
        ----------
        real_value : torch.Tensor
            the real value to be quantized
        scale : torch.Tensor
            quantization scale
        zero_point : torch.Tensor
            quantization zero point
        qmin : int
            lower bound of the int range
        qmax : int
            upper bound of the int range

        Returns
        -------
        Tensor
        &#34;&#34;&#34;
        transformed_val = zero_point + real_value / scale
        clamped_val = torch.clamp(transformed_val, qmin, qmax)
        quantized_val = torch.round(clamped_val)
        return quantized_val

    def _dequantize(self, quantized_val, scale, zero_point):
        &#34;&#34;&#34;
        dequantize quantized value.
        Because we simulate quantization in training process, all the computations still happen as float point computations, which means we
        first quantize tensors then dequantize them. For more details, please refer to the paper.

        Parameters
        ----------
        quantized_val : torch.Tensor
            the quantized value to be de-quantized
        scale : torch.Tensor
            quantization scale
        zero_point : torch.Tensor
            quantization zero point

        Returns
        -------
        Tensor
        &#34;&#34;&#34;
        real_val = scale * (quantized_val - zero_point)
        return real_val

    def quantize_weight(self, wrapper, **kwargs):
        module = wrapper.module
        weight = module.weight
        layer_quant_setting = module.layer_quant_setting
        tensor_quant_setting = layer_quant_setting.weight

        # layer-wise settings
        quant_start_step = layer_quant_setting.quant_start_step

        # tensor-wise settings
        dtype = tensor_quant_setting.quant_dtype
        scheme = tensor_quant_setting.quant_scheme
        qmin, qmax = tensor_quant_setting.get_qmin_qmax()
        bits = tensor_quant_setting.bits

        # In evaluation mode, we only quantize weight without updating statistics
        if not wrapper.training:
            scale, zero_point = module.weight_scale, module.weight_zero_point
            weight = self._quantize(weight, scale, zero_point, qmin, qmax)
            weight = self._dequantize(weight, scale, zero_point)
            module.weight = weight
            return weight

        if quant_start_step &gt; int(self.bound_model.steps):
            return weight

        current_min, current_max = get_min_max_value(weight, QuantType.WEIGHT, scheme)
        scale, zero_point = update_quantization_param(bits, current_min, current_max, dtype, scheme)
        module.weight_scale.copy_(scale)
        module.weight_zero_point.copy_(zero_point)
        weight = self._quantize(weight, scale, zero_point, qmin, qmax)
        weight = self._dequantize(weight, scale, zero_point)
        wrapper.module.weight = weight
        return weight

    def quantize_input(self, inputs, wrapper, **kwargs):
        module = wrapper.module

        layer_quant_setting = module.layer_quant_setting
        tensor_quant_setting = layer_quant_setting.input

        quant_start_step = layer_quant_setting.quant_start_step
        ema_decay = layer_quant_setting.ema_decay

        dtype = tensor_quant_setting.quant_dtype
        scheme = tensor_quant_setting.quant_scheme
        qmin, qmax = tensor_quant_setting.get_qmin_qmax()
        bits = tensor_quant_setting.bits

        if not wrapper.training:
            scale = module.input_scale
            zero_point = module.input_zero_point
            inputs = self._quantize(inputs, scale, zero_point, qmin, qmax)
            inputs = self._dequantize(inputs, scale, zero_point)
            return inputs

        current_min, current_max = get_min_max_value(inputs, QuantType.INPUT, scheme)

        if int(self.bound_model.steps) == 1:
            module.tracked_min_input.copy_(current_min)
            module.tracked_max_input.copy_(current_max)

        tracked_min_input = update_ema(module.tracked_min_input, current_min, ema_decay)
        tracked_max_input = update_ema(module.tracked_max_input, current_max, ema_decay)
        module.tracked_min_input.copy_(tracked_min_input)
        module.tracked_max_input.copy_(tracked_max_input)

        if quant_start_step &gt; int(self.bound_model.steps):
            return inputs

        scale, zero_point = update_quantization_param(
            bits, module.tracked_min_input, module.tracked_max_input, dtype, scheme)
        module.input_scale.copy_(scale)
        module.input_zero_point.copy_(zero_point)

        inputs = self._quantize(inputs, scale, zero_point, qmin, qmax)
        inputs = self._dequantize(inputs, scale, zero_point)
        return inputs

    def quantize_output(self, output, wrapper, **kwargs):
        module = wrapper.module
        layer_quant_setting = module.layer_quant_setting
        tensor_quant_setting = layer_quant_setting.output

        quant_start_step = layer_quant_setting.quant_start_step
        ema_decay = layer_quant_setting.ema_decay

        dtype = tensor_quant_setting.quant_dtype
        scheme = tensor_quant_setting.quant_scheme
        qmin, qmax = tensor_quant_setting.get_qmin_qmax()
        bits = tensor_quant_setting.bits

        if not wrapper.training:
            scale = module.output_scale
            zero_point = module.output_zero_point
            output = self._quantize(output, scale, zero_point, qmin, qmax)
            output = self._dequantize(output, scale, zero_point)
            return output

        current_min, current_max = get_min_max_value(output, QuantType.OUTPUT, scheme)

        if int(self.bound_model.steps) == 1:
            module.tracked_min_output.copy_(current_min)
            module.tracked_max_output.copy_(current_max)

        tracked_min_output = update_ema(module.tracked_min_output, current_min, ema_decay)
        tracked_max_output = update_ema(module.tracked_max_output, current_max, ema_decay)
        module.tracked_min_output.copy_(tracked_min_output)
        module.tracked_max_output.copy_(tracked_max_output)

        if quant_start_step &gt; int(self.bound_model.steps):
            return output

        scale, zero_point = update_quantization_param(
            bits, module.tracked_min_output, module.tracked_max_output, dtype, scheme)
        module.output_scale.copy_(scale)
        module.output_zero_point.copy_(zero_point)

        output = self._quantize(output, scale, zero_point, qmin, qmax)
        output = self._dequantize(output, scale, zero_point)
        return output

    def load_calibration_config(self, calibration_config):
        modules_to_compress = self.get_modules_to_compress()
        for layer, _ in modules_to_compress:
            name, module = layer.name, layer.module
            if name not in calibration_config:
                if module.layer_quant_setting.weight or module.layer_quant_setting.input or module.layer_quant_setting.output:
                    logger.warning(f&#34;Can not find module {name}&#39;s parameter in input config.&#34;)
                continue
            if module.layer_quant_setting.weight:
                assert calibration_config[name][&#39;weight_bits&#39;] == module.layer_quant_setting.weight.bits, \
                    f&#34;weight bits of module {name} fail to match&#34;
            if module.layer_quant_setting.input:
                assert calibration_config[name][&#39;input_bits&#39;] == module.layer_quant_setting.input.bits, \
                    f&#34;input bits of module {name} fail to match&#34;
                module.tracked_min_input.data = torch.tensor([calibration_config[name][&#39;tracked_min_input&#39;]])
                module.tracked_max_input.data = torch.tensor([calibration_config[name][&#39;tracked_max_input&#39;]])
            if module.layer_quant_setting.output:
                assert calibration_config[name][&#39;output_bits&#39;] == module.layer_quant_setting.output.bits, \
                    f&#34;output bits of module {name} fail to match&#34;
                module.tracked_min_output.data = torch.tensor([calibration_config[name][&#39;tracked_min_output&#39;]])
                module.tracked_max_output.data = torch.tensor([calibration_config[name][&#39;tracked_max_output&#39;]])

    def export_model(self, model_path, calibration_path=None, onnx_path=None, input_shape=None, device=None):
        &#34;&#34;&#34;
        Export quantized model weights and calibration parameters(optional)

        Parameters
        ----------
        model_path : str
            path to save quantized model weight
        calibration_path : str
            (optional) path to save quantize parameters after calibration
        onnx_path : str
            (optional) path to save onnx model
        input_shape : list or tuple
            input shape to onnx model
        device : torch.device
            device of the model, used to place the dummy input tensor for exporting onnx file.
            the tensor is placed on cpu if ```device``` is None

        Returns
        -------
        Dict
        &#34;&#34;&#34;
        assert model_path is not None, &#39;model_path must be specified&#39;
        self._unwrap_model()
        calibration_config = {}

        modules_to_compress = self.get_modules_to_compress()
        for layer, _ in modules_to_compress:
            name, module = layer.name, layer.module
            if hasattr(module.layer_quant_setting, &#39;weight&#39;) or hasattr(module.layer_quant_setting, &#39;output&#39;):
                calibration_config[name] = {}
            if module.layer_quant_setting.weight:
                calibration_config[name][&#39;weight_bits&#39;] = int(module.layer_quant_setting.weight.bits)
                calibration_config[name][&#39;weight_scale&#39;] = module.weight_scale
                calibration_config[name][&#39;weight_zero_point&#39;] = module.weight_zero_point

                actual_weight = getattr(module, &#39;old_weight&#39;, None)
                if actual_weight is None:
                    logger.warning(&#34;Can not recover weight for layer %s. &#34;
                                   &#34;This may lead to a wrong accuracy performance on the backend.&#34;, name)
                delattr(module, &#39;weight&#39;)
                module.register_parameter(&#39;weight&#39;, actual_weight)
                if hasattr(module, BN_FOLD_TAG):
                    actual_bias = getattr(module, &#39;old_bias&#39;, None)
                    delattr(module, &#39;bias&#39;)
                    if actual_bias is not None:
                        module.register_parameter(&#39;bias&#39;, actual_bias)
                    else:
                        setattr(module, &#39;bias&#39;, None)

            if module.layer_quant_setting.input:
                calibration_config[name][&#39;input_bits&#39;] = int(module.layer_quant_setting.input.bits)
                calibration_config[name][&#39;tracked_min_input&#39;] = float(module.tracked_min_input)
                calibration_config[name][&#39;tracked_max_input&#39;] = float(module.tracked_max_input)

            if module.layer_quant_setting.output:
                calibration_config[name][&#39;output_bits&#39;] = int(module.layer_quant_setting.output.bits)
                calibration_config[name][&#39;tracked_min_output&#39;] = float(module.tracked_min_output)
                calibration_config[name][&#39;tracked_max_output&#39;] = float(module.tracked_max_output)
            self._del_simulated_attr(module)

        self.export_model_save(self.bound_model, model_path, calibration_config, calibration_path, onnx_path, input_shape, device)

        return calibration_config

    def step_with_optimizer(self):
        &#34;&#34;&#34;
        override `compressor` `step` method, quantization only happens after certain number of steps
        &#34;&#34;&#34;
        self.bound_model.steps.add_(1)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="optimization.quantization.quant.qat.update_ema"><code class="name flex">
<span>def <span class="ident">update_ema</span></span>(<span>biased_ema, value, decay)</span>
</code></dt>
<dd>
<div class="desc"><p>calculate biased stat and unbiased stat in each step using exponential moving average method</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>biased_ema</code></strong> :&ensp;<code>float</code></dt>
<dd>previous stat value</dd>
<dt><strong><code>value</code></strong> :&ensp;<code>float</code></dt>
<dd>current stat value</dd>
<dt><strong><code>decay</code></strong> :&ensp;<code>float</code></dt>
<dd>the weight of previous stat value, larger means smoother curve</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float, float</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_ema(biased_ema, value, decay):
    &#34;&#34;&#34;
    calculate biased stat and unbiased stat in each step using exponential moving average method

    Parameters
    ----------
    biased_ema : float
        previous stat value
    value : float
        current stat value
    decay : float
        the weight of previous stat value, larger means smoother curve

    Returns
    -------
    float, float
    &#34;&#34;&#34;
    biased_ema = biased_ema * decay + (1 - decay) * value
    return biased_ema</code></pre>
</details>
</dd>
<dt id="optimization.quantization.quant.qat.update_quantization_param"><code class="name flex">
<span>def <span class="ident">update_quantization_param</span></span>(<span>bits, rmin, rmax, dtype, schema)</span>
</code></dt>
<dd>
<div class="desc"><p>calculate the <code>zero_point</code> and <code>scale</code>.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>bits</code></strong> :&ensp;<code>int</code></dt>
<dd>quantization bits length</dd>
<dt><strong><code>rmin</code></strong> :&ensp;<code>Tensor</code></dt>
<dd>min value of real value</dd>
<dt><strong><code>rmax</code></strong> :&ensp;<code>Tensor</code></dt>
<dd>max value of real value</dd>
<dt><strong><code>dtype</code></strong> :&ensp;<code>QuantDtype</code></dt>
<dd>quantized data type</dd>
<dt><strong><code>scheme</code></strong> :&ensp;<code>QuantScheme</code></dt>
<dd>quantization scheme to be used</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>float, float</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_quantization_param(bits, rmin, rmax, dtype, schema):
    &#34;&#34;&#34;
    calculate the `zero_point` and `scale`.

    Parameters
    ----------
    bits : int
        quantization bits length
    rmin : Tensor
        min value of real value
    rmax : Tensor
        max value of real value
    dtype : QuantDtype
        quantized data type
    scheme : QuantScheme
        quantization scheme to be used
    Returns
    -------
    float, float
    &#34;&#34;&#34;
    rmin = torch.min(rmin, torch.zeros_like(rmin))
    rmax = torch.max(rmax, torch.zeros_like(rmax))

    zero_point = torch.zeros_like(rmin)

    qmin, qmax = calculate_qmin_qmax(bits, dtype)
    if schema in [QuantScheme.PER_TENSOR_SYMMETRIC, QuantScheme.PER_CHANNEL_SYMMETRIC]:
        abs_max = torch.max(torch.abs(rmin), torch.abs(rmax))
        scale = abs_max / (float(qmax - qmin) / 2)
        if dtype == QuantDtype.UINT:
            zero_point_val = (qmin + qmax) // 2
            zero_point = zero_point.new_full(zero_point.size(), zero_point_val)
    else:
        scale = (rmax - rmin) / float(qmax - qmin)
        zero_point = qmin - torch.round(rmin / scale)
    zero_point = torch.clamp(zero_point, qmin, qmax)
    
    return scale, zero_point</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="optimization.quantization.quant.qat.QATGrad"><code class="flex name class">
<span>class <span class="ident">QATGrad</span></span>
<span>(</span><span>...)</span>
</code></dt>
<dd>
<div class="desc"><p>Records operation history and defines formulas for differentiating ops.</p>
<p>See the Note on extending the autograd engine for more details on how to use
this class: <a href="https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd">https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd</a></p>
<p>Every operation performed on :class:<code>Tensor</code> s creates a new function
object, that performs the computation, and records that it happened.
The history is retained in the form of a DAG of functions, with edges
denoting data dependencies (<code>input &lt;- output</code>). Then, when backward is
called, the graph is processed in the topological ordering, by calling
:func:<code>backward</code> methods of each :class:<code>Function</code> object, and passing
returned gradients on to next :class:<code>Function</code> s.</p>
<p>Normally, the only way users interact with functions is by creating
subclasses and defining new operations. This is a recommended way of
extending torch.autograd.</p>
<p>Examples::</p>
<pre><code>&gt;&gt;&gt; class Exp(Function):
&gt;&gt;&gt;
&gt;&gt;&gt;     @staticmethod
&gt;&gt;&gt;     def forward(ctx, i):
&gt;&gt;&gt;         result = i.exp()
&gt;&gt;&gt;         ctx.save_for_backward(result)
&gt;&gt;&gt;         return result
&gt;&gt;&gt;
&gt;&gt;&gt;     @staticmethod
&gt;&gt;&gt;     def backward(ctx, grad_output):
&gt;&gt;&gt;         result, = ctx.saved_tensors
&gt;&gt;&gt;         return grad_output * result
&gt;&gt;&gt;
&gt;&gt;&gt; #Use it by calling the apply method:
&gt;&gt;&gt; output = Exp.apply(input)
</code></pre></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class QATGrad(QuantGrad):
    @staticmethod
    def quant_backward(tensor, grad_output, quant_type, scale, zero_point, qmin, qmax):
        tensor_q = QuantGrad._quantize(tensor, grad_output, quant_type, scale)
        mask = (tensor_q &lt; qmin) | (tensor_q &gt; qmax)
        grad_output[mask] = 0
        return grad_output</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="optimization.quantization.core.quant.QuantGrad" href="../core/quant.html#optimization.quantization.core.quant.QuantGrad">QuantGrad</a></li>
<li>torch.autograd.function.Function</li>
<li>torch._C._FunctionBase</li>
<li>torch.autograd.function._ContextMethodMixin</li>
<li>torch.autograd.function._HookMixin</li>
</ul>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="optimization.quantization.core.quant.QuantGrad" href="../core/quant.html#optimization.quantization.core.quant.QuantGrad">QuantGrad</a></b></code>:
<ul class="hlist">
<li><code><a title="optimization.quantization.core.quant.QuantGrad.backward" href="../core/quant.html#optimization.quantization.core.quant.QuantGrad.backward">backward</a></code></li>
<li><code><a title="optimization.quantization.core.quant.QuantGrad.forward" href="../core/quant.html#optimization.quantization.core.quant.QuantGrad.forward">forward</a></code></li>
<li><code><a title="optimization.quantization.core.quant.QuantGrad.get_bits_length" href="../core/quant.html#optimization.quantization.core.quant.QuantGrad.get_bits_length">get_bits_length</a></code></li>
<li><code><a title="optimization.quantization.core.quant.QuantGrad.quant_backward" href="../core/quant.html#optimization.quantization.core.quant.QuantGrad.quant_backward">quant_backward</a></code></li>
</ul>
</li>
</ul>
</dd>
<dt id="optimization.quantization.quant.qat.QAT_Quantizer"><code class="flex name class">
<span>class <span class="ident">QAT_Quantizer</span></span>
<span>(</span><span>model, config_list, optimizer, dummy_input=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Quantizer defined in:
Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference
<a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Jacob_Quantization_and_Training_CVPR_2018_paper.pdf">http://openaccess.thecvf.com/content_cvpr_2018/papers/Jacob_Quantization_and_Training_CVPR_2018_paper.pdf</a></p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>layer</code></strong> :&ensp;<code>LayerInfo</code></dt>
<dd>the layer to quantize</dd>
<dt><strong><code>config_list</code></strong> :&ensp;<code>list</code> of <code>dict</code></dt>
<dd>list of configurations for quantization
supported keys for dict:
- quant_types : list of string
type of quantization you want to apply, currently support 'weight', 'input', 'output'
- quant_bits : int or dict of {str : int}
bits length of quantization, key is the quantization type, value is the length, eg. {'weight', 8},
when the type is int, all quantization types share same bits length
- quant_start_step : int
disable quantization until model are run by certain number of steps, this allows the network to enter a more stable
state where output quantization ranges do not exclude a signiÔ¨Åcant fraction of values, default value is 0
- op_types : list of string
types of nn.module you want to apply quantization, eg. 'Conv2d'
- dummy_input : tuple of tensor
inputs to the model, which are used to get the graph of the module. The graph is used to find
Conv-Bn patterns. And then the batch normalization folding would be enabled. If dummy_input is not
given, the batch normalization folding would be disabled.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class QAT_Quantizer(Quantizer):
    &#34;&#34;&#34;Quantizer defined in:
    Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference
    http://openaccess.thecvf.com/content_cvpr_2018/papers/Jacob_Quantization_and_Training_CVPR_2018_paper.pdf
    &#34;&#34;&#34;
    def __init__(self, model, config_list, optimizer, dummy_input=None):
        &#34;&#34;&#34;
        Parameters
        ----------
        layer : LayerInfo
            the layer to quantize
        config_list : list of dict
            list of configurations for quantization
            supported keys for dict:
                - quant_types : list of string
                    type of quantization you want to apply, currently support &#39;weight&#39;, &#39;input&#39;, &#39;output&#39;
                - quant_bits : int or dict of {str : int}
                    bits length of quantization, key is the quantization type, value is the length, eg. {&#39;weight&#39;, 8},
                    when the type is int, all quantization types share same bits length
                - quant_start_step : int
                    disable quantization until model are run by certain number of steps, this allows the network to enter a more stable
                    state where output quantization ranges do not exclude a signiÔ¨Åcant fraction of values, default value is 0
                - op_types : list of string
                    types of nn.module you want to apply quantization, eg. &#39;Conv2d&#39;
                - dummy_input : tuple of tensor
                    inputs to the model, which are used to get the graph of the module. The graph is used to find
                    Conv-Bn patterns. And then the batch normalization folding would be enabled. If dummy_input is not
                    given, the batch normalization folding would be disabled.
        &#34;&#34;&#34;
        assert isinstance(optimizer, torch.optim.Optimizer), &#34;unrecognized optimizer type&#34;
        super().__init__(model, config_list, optimizer, dummy_input)
        self.quant_grad = QATGrad.apply
        modules_to_compress = self.get_modules_to_compress()
        device = next(model.parameters()).device
        self.bound_model.register_buffer(&#34;steps&#34;, torch.tensor(1))
        for layer, config in modules_to_compress:
            module = layer.module
            name = layer.name
            assert name in self.all_shapes, &#34;Could not found shapes for layer {}&#34;.format(name)
            input_shape, output_shape = self.all_shapes[name]
            layer_quant_setting = LayerQuantSetting(config)
            layer_quant_setting.ema_decay = 0.99
            quant_start_step = config.get(&#39;quant_start_step&#39;, 0)
            layer_quant_setting.quant_start_step = quant_start_step
            if isinstance(module, torch.nn.Linear):
                if &#34;input&#34; in config.get(&#34;quant_types&#34;, []) and \
                        layer_quant_setting.input.quant_scheme in PER_CHANNEL_QUANT_SCHEME:
                    if len(input_shape) != 2:
                        logger.warning(&#34;When quantize torch.nn.Linear, make sure that the rank of the inputs &#34;
                                       &#34;of the layer is 2. Skip quantization of layer %s.&#34;, name)
                        continue
                if &#34;output&#34; in config.get(&#34;quant_types&#34;, []) and \
                        layer_quant_setting.output.quant_scheme in PER_CHANNEL_QUANT_SCHEME:
                    if len(output_shape) != 2:
                        logger.warning(&#34;When quantize torch.nn.Linear, make sure that the rank of the outputs &#34;
                                       &#34;of the layer is 2. Skip quantization of layer %s.&#34;, name)
                        continue

            if &#34;weight&#34; in config.get(&#34;quant_types&#34;, []):
                quant_shape = get_quant_shape(module.weight.shape, QuantType.WEIGHT, layer_quant_setting.weight.quant_scheme)
                module.register_buffer(&#39;weight_scale&#39;, torch.zeros(quant_shape))
                module.register_buffer(&#39;weight_zero_point&#39;, torch.zeros(quant_shape))

            if &#34;input&#34; in config.get(&#34;quant_types&#34;, []):
                quant_shape = get_quant_shape(input_shape, QuantType.INPUT, layer_quant_setting.input.quant_scheme)
                module.register_buffer(&#39;tracked_min_input&#39;, torch.zeros(quant_shape))
                module.register_buffer(&#39;tracked_max_input&#39;, torch.zeros(quant_shape))
                module.register_buffer(&#39;input_scale&#39;, torch.zeros(quant_shape))
                module.register_buffer(&#39;input_zero_point&#39;, torch.zeros(quant_shape))

            if &#34;output&#34; in config.get(&#34;quant_types&#34;, []):
                quant_shape = get_quant_shape(output_shape, QuantType.OUTPUT, layer_quant_setting.output.quant_scheme)
                module.register_buffer(&#39;tracked_min_output&#39;, torch.zeros(quant_shape))
                module.register_buffer(&#39;tracked_max_output&#39;, torch.zeros(quant_shape))
                module.register_buffer(&#39;output_scale&#39;, torch.zeros(quant_shape))
                module.register_buffer(&#39;output_zero_point&#39;, torch.zeros(quant_shape))

            setattr(module, &#34;layer_quant_setting&#34;, layer_quant_setting)
        self.bound_model.to(device)

    def _del_simulated_attr(self, module):
        &#34;&#34;&#34;
        delete redundant parameters in quantize module
        &#34;&#34;&#34;
        del_attr_list = [&#39;old_weight&#39;, &#39;old_bias&#39;, &#39;ema_decay&#39;, &#39;tracked_min_output&#39;, &#39;tracked_max_output&#39;,
                         &#39;tracked_min_input&#39;, &#39;tracked_max_input&#39;, &#39;BN_FOLD_TAG&#39;,
                         &#39;weight_scale&#39;, &#39;weight_zero_point&#39;, &#39;input_scale&#39;, &#39;input_zero_point&#39;,
                         &#39;output_scale&#39;, &#39;output_zero_point&#39;, &#39;layer_quant_setting&#39;]
        for attr in del_attr_list:
            if hasattr(module, attr):
                delattr(module, attr)

    def validate_config(self, model, config_list):
        &#34;&#34;&#34;
        Parameters
        ----------
        model : torch.nn.Module
            Model to be pruned
        config_list : list of dict
            List of configurations
        &#34;&#34;&#34;
        SUPPORTED_OPS = [&#39;Conv2d&#39;, &#39;Linear&#39;, &#39;ReLU&#39;, &#39;ReLU6&#39;]
        schema = QuantizerSchema([{
            Optional(&#39;quant_types&#39;): Schema([lambda x: x in [&#39;weight&#39;, &#39;output&#39;, &#39;input&#39;]]),
            Optional(&#39;quant_bits&#39;): Or(And(int, lambda n: 0 &lt; n &lt; 32), Schema({
                Optional(&#39;input&#39;): And(int, lambda n: 0 &lt; n &lt; 32),
                Optional(&#39;weight&#39;): And(int, lambda n: 0 &lt; n &lt; 32),
                Optional(&#39;output&#39;): And(int, lambda n: 0 &lt; n &lt; 32),
            })),
            Optional(&#39;quant_scheme&#39;): Or(lambda x: x in QuantScheme, Schema({
                Optional(&#39;input&#39;): lambda x: x in QuantScheme,
                Optional(&#39;weight&#39;): lambda x: x in QuantScheme,
                Optional(&#39;output&#39;): lambda x: x in QuantScheme
            })),
            Optional(&#39;quant_dtype&#39;): Or(lambda x: x in QuantDtype, Schema({
                Optional(&#39;input&#39;): lambda x: x in QuantDtype,
                Optional(&#39;weight&#39;): lambda x: x in QuantDtype,
                Optional(&#39;output&#39;): lambda x: x in QuantDtype
            })),
            Optional(&#39;quant_start_step&#39;): And(int, lambda n: n &gt;= 0),
            Optional(&#39;op_types&#39;): [And(str, lambda n: n in SUPPORTED_OPS)],
            Optional(&#39;op_names&#39;): [str],
            Optional(&#39;exclude&#39;): bool
        }], model, logger)

        schema.validate(config_list)

    def _quantize(self, real_value, scale, zero_point, qmin, qmax):
        &#34;&#34;&#34;
        quantize real value.

        Parameters
        ----------
        real_value : torch.Tensor
            the real value to be quantized
        scale : torch.Tensor
            quantization scale
        zero_point : torch.Tensor
            quantization zero point
        qmin : int
            lower bound of the int range
        qmax : int
            upper bound of the int range

        Returns
        -------
        Tensor
        &#34;&#34;&#34;
        transformed_val = zero_point + real_value / scale
        clamped_val = torch.clamp(transformed_val, qmin, qmax)
        quantized_val = torch.round(clamped_val)
        return quantized_val

    def _dequantize(self, quantized_val, scale, zero_point):
        &#34;&#34;&#34;
        dequantize quantized value.
        Because we simulate quantization in training process, all the computations still happen as float point computations, which means we
        first quantize tensors then dequantize them. For more details, please refer to the paper.

        Parameters
        ----------
        quantized_val : torch.Tensor
            the quantized value to be de-quantized
        scale : torch.Tensor
            quantization scale
        zero_point : torch.Tensor
            quantization zero point

        Returns
        -------
        Tensor
        &#34;&#34;&#34;
        real_val = scale * (quantized_val - zero_point)
        return real_val

    def quantize_weight(self, wrapper, **kwargs):
        module = wrapper.module
        weight = module.weight
        layer_quant_setting = module.layer_quant_setting
        tensor_quant_setting = layer_quant_setting.weight

        # layer-wise settings
        quant_start_step = layer_quant_setting.quant_start_step

        # tensor-wise settings
        dtype = tensor_quant_setting.quant_dtype
        scheme = tensor_quant_setting.quant_scheme
        qmin, qmax = tensor_quant_setting.get_qmin_qmax()
        bits = tensor_quant_setting.bits

        # In evaluation mode, we only quantize weight without updating statistics
        if not wrapper.training:
            scale, zero_point = module.weight_scale, module.weight_zero_point
            weight = self._quantize(weight, scale, zero_point, qmin, qmax)
            weight = self._dequantize(weight, scale, zero_point)
            module.weight = weight
            return weight

        if quant_start_step &gt; int(self.bound_model.steps):
            return weight

        current_min, current_max = get_min_max_value(weight, QuantType.WEIGHT, scheme)
        scale, zero_point = update_quantization_param(bits, current_min, current_max, dtype, scheme)
        module.weight_scale.copy_(scale)
        module.weight_zero_point.copy_(zero_point)
        weight = self._quantize(weight, scale, zero_point, qmin, qmax)
        weight = self._dequantize(weight, scale, zero_point)
        wrapper.module.weight = weight
        return weight

    def quantize_input(self, inputs, wrapper, **kwargs):
        module = wrapper.module

        layer_quant_setting = module.layer_quant_setting
        tensor_quant_setting = layer_quant_setting.input

        quant_start_step = layer_quant_setting.quant_start_step
        ema_decay = layer_quant_setting.ema_decay

        dtype = tensor_quant_setting.quant_dtype
        scheme = tensor_quant_setting.quant_scheme
        qmin, qmax = tensor_quant_setting.get_qmin_qmax()
        bits = tensor_quant_setting.bits

        if not wrapper.training:
            scale = module.input_scale
            zero_point = module.input_zero_point
            inputs = self._quantize(inputs, scale, zero_point, qmin, qmax)
            inputs = self._dequantize(inputs, scale, zero_point)
            return inputs

        current_min, current_max = get_min_max_value(inputs, QuantType.INPUT, scheme)

        if int(self.bound_model.steps) == 1:
            module.tracked_min_input.copy_(current_min)
            module.tracked_max_input.copy_(current_max)

        tracked_min_input = update_ema(module.tracked_min_input, current_min, ema_decay)
        tracked_max_input = update_ema(module.tracked_max_input, current_max, ema_decay)
        module.tracked_min_input.copy_(tracked_min_input)
        module.tracked_max_input.copy_(tracked_max_input)

        if quant_start_step &gt; int(self.bound_model.steps):
            return inputs

        scale, zero_point = update_quantization_param(
            bits, module.tracked_min_input, module.tracked_max_input, dtype, scheme)
        module.input_scale.copy_(scale)
        module.input_zero_point.copy_(zero_point)

        inputs = self._quantize(inputs, scale, zero_point, qmin, qmax)
        inputs = self._dequantize(inputs, scale, zero_point)
        return inputs

    def quantize_output(self, output, wrapper, **kwargs):
        module = wrapper.module
        layer_quant_setting = module.layer_quant_setting
        tensor_quant_setting = layer_quant_setting.output

        quant_start_step = layer_quant_setting.quant_start_step
        ema_decay = layer_quant_setting.ema_decay

        dtype = tensor_quant_setting.quant_dtype
        scheme = tensor_quant_setting.quant_scheme
        qmin, qmax = tensor_quant_setting.get_qmin_qmax()
        bits = tensor_quant_setting.bits

        if not wrapper.training:
            scale = module.output_scale
            zero_point = module.output_zero_point
            output = self._quantize(output, scale, zero_point, qmin, qmax)
            output = self._dequantize(output, scale, zero_point)
            return output

        current_min, current_max = get_min_max_value(output, QuantType.OUTPUT, scheme)

        if int(self.bound_model.steps) == 1:
            module.tracked_min_output.copy_(current_min)
            module.tracked_max_output.copy_(current_max)

        tracked_min_output = update_ema(module.tracked_min_output, current_min, ema_decay)
        tracked_max_output = update_ema(module.tracked_max_output, current_max, ema_decay)
        module.tracked_min_output.copy_(tracked_min_output)
        module.tracked_max_output.copy_(tracked_max_output)

        if quant_start_step &gt; int(self.bound_model.steps):
            return output

        scale, zero_point = update_quantization_param(
            bits, module.tracked_min_output, module.tracked_max_output, dtype, scheme)
        module.output_scale.copy_(scale)
        module.output_zero_point.copy_(zero_point)

        output = self._quantize(output, scale, zero_point, qmin, qmax)
        output = self._dequantize(output, scale, zero_point)
        return output

    def load_calibration_config(self, calibration_config):
        modules_to_compress = self.get_modules_to_compress()
        for layer, _ in modules_to_compress:
            name, module = layer.name, layer.module
            if name not in calibration_config:
                if module.layer_quant_setting.weight or module.layer_quant_setting.input or module.layer_quant_setting.output:
                    logger.warning(f&#34;Can not find module {name}&#39;s parameter in input config.&#34;)
                continue
            if module.layer_quant_setting.weight:
                assert calibration_config[name][&#39;weight_bits&#39;] == module.layer_quant_setting.weight.bits, \
                    f&#34;weight bits of module {name} fail to match&#34;
            if module.layer_quant_setting.input:
                assert calibration_config[name][&#39;input_bits&#39;] == module.layer_quant_setting.input.bits, \
                    f&#34;input bits of module {name} fail to match&#34;
                module.tracked_min_input.data = torch.tensor([calibration_config[name][&#39;tracked_min_input&#39;]])
                module.tracked_max_input.data = torch.tensor([calibration_config[name][&#39;tracked_max_input&#39;]])
            if module.layer_quant_setting.output:
                assert calibration_config[name][&#39;output_bits&#39;] == module.layer_quant_setting.output.bits, \
                    f&#34;output bits of module {name} fail to match&#34;
                module.tracked_min_output.data = torch.tensor([calibration_config[name][&#39;tracked_min_output&#39;]])
                module.tracked_max_output.data = torch.tensor([calibration_config[name][&#39;tracked_max_output&#39;]])

    def export_model(self, model_path, calibration_path=None, onnx_path=None, input_shape=None, device=None):
        &#34;&#34;&#34;
        Export quantized model weights and calibration parameters(optional)

        Parameters
        ----------
        model_path : str
            path to save quantized model weight
        calibration_path : str
            (optional) path to save quantize parameters after calibration
        onnx_path : str
            (optional) path to save onnx model
        input_shape : list or tuple
            input shape to onnx model
        device : torch.device
            device of the model, used to place the dummy input tensor for exporting onnx file.
            the tensor is placed on cpu if ```device``` is None

        Returns
        -------
        Dict
        &#34;&#34;&#34;
        assert model_path is not None, &#39;model_path must be specified&#39;
        self._unwrap_model()
        calibration_config = {}

        modules_to_compress = self.get_modules_to_compress()
        for layer, _ in modules_to_compress:
            name, module = layer.name, layer.module
            if hasattr(module.layer_quant_setting, &#39;weight&#39;) or hasattr(module.layer_quant_setting, &#39;output&#39;):
                calibration_config[name] = {}
            if module.layer_quant_setting.weight:
                calibration_config[name][&#39;weight_bits&#39;] = int(module.layer_quant_setting.weight.bits)
                calibration_config[name][&#39;weight_scale&#39;] = module.weight_scale
                calibration_config[name][&#39;weight_zero_point&#39;] = module.weight_zero_point

                actual_weight = getattr(module, &#39;old_weight&#39;, None)
                if actual_weight is None:
                    logger.warning(&#34;Can not recover weight for layer %s. &#34;
                                   &#34;This may lead to a wrong accuracy performance on the backend.&#34;, name)
                delattr(module, &#39;weight&#39;)
                module.register_parameter(&#39;weight&#39;, actual_weight)
                if hasattr(module, BN_FOLD_TAG):
                    actual_bias = getattr(module, &#39;old_bias&#39;, None)
                    delattr(module, &#39;bias&#39;)
                    if actual_bias is not None:
                        module.register_parameter(&#39;bias&#39;, actual_bias)
                    else:
                        setattr(module, &#39;bias&#39;, None)

            if module.layer_quant_setting.input:
                calibration_config[name][&#39;input_bits&#39;] = int(module.layer_quant_setting.input.bits)
                calibration_config[name][&#39;tracked_min_input&#39;] = float(module.tracked_min_input)
                calibration_config[name][&#39;tracked_max_input&#39;] = float(module.tracked_max_input)

            if module.layer_quant_setting.output:
                calibration_config[name][&#39;output_bits&#39;] = int(module.layer_quant_setting.output.bits)
                calibration_config[name][&#39;tracked_min_output&#39;] = float(module.tracked_min_output)
                calibration_config[name][&#39;tracked_max_output&#39;] = float(module.tracked_max_output)
            self._del_simulated_attr(module)

        self.export_model_save(self.bound_model, model_path, calibration_config, calibration_path, onnx_path, input_shape, device)

        return calibration_config

    def step_with_optimizer(self):
        &#34;&#34;&#34;
        override `compressor` `step` method, quantization only happens after certain number of steps
        &#34;&#34;&#34;
        self.bound_model.steps.add_(1)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="optimization.quantization.core.quant.Quantizer" href="../core/quant.html#optimization.quantization.core.quant.Quantizer">Quantizer</a></li>
<li><a title="optimization.common.base.compressor.Compressor" href="../../common/base/compressor.html#optimization.common.base.compressor.Compressor">Compressor</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="optimization.quantization.quant.qat.QAT_Quantizer.export_model"><code class="name flex">
<span>def <span class="ident">export_model</span></span>(<span>self, model_path, calibration_path=None, onnx_path=None, input_shape=None, device=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Export quantized model weights and calibration parameters(optional)</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model_path</code></strong> :&ensp;<code>str</code></dt>
<dd>path to save quantized model weight</dd>
<dt><strong><code>calibration_path</code></strong> :&ensp;<code>str</code></dt>
<dd>(optional) path to save quantize parameters after calibration</dd>
<dt><strong><code>onnx_path</code></strong> :&ensp;<code>str</code></dt>
<dd>(optional) path to save onnx model</dd>
<dt><strong><code>input_shape</code></strong> :&ensp;<code>list</code> or <code>tuple</code></dt>
<dd>input shape to onnx model</dd>
<dt><strong><code>device</code></strong> :&ensp;<code>torch.device</code></dt>
<dd>device of the model, used to place the dummy input tensor for exporting onnx file.
the tensor is placed on cpu if <code>device</code> is None</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>Dict</code></dt>
<dd>&nbsp;</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def export_model(self, model_path, calibration_path=None, onnx_path=None, input_shape=None, device=None):
    &#34;&#34;&#34;
    Export quantized model weights and calibration parameters(optional)

    Parameters
    ----------
    model_path : str
        path to save quantized model weight
    calibration_path : str
        (optional) path to save quantize parameters after calibration
    onnx_path : str
        (optional) path to save onnx model
    input_shape : list or tuple
        input shape to onnx model
    device : torch.device
        device of the model, used to place the dummy input tensor for exporting onnx file.
        the tensor is placed on cpu if ```device``` is None

    Returns
    -------
    Dict
    &#34;&#34;&#34;
    assert model_path is not None, &#39;model_path must be specified&#39;
    self._unwrap_model()
    calibration_config = {}

    modules_to_compress = self.get_modules_to_compress()
    for layer, _ in modules_to_compress:
        name, module = layer.name, layer.module
        if hasattr(module.layer_quant_setting, &#39;weight&#39;) or hasattr(module.layer_quant_setting, &#39;output&#39;):
            calibration_config[name] = {}
        if module.layer_quant_setting.weight:
            calibration_config[name][&#39;weight_bits&#39;] = int(module.layer_quant_setting.weight.bits)
            calibration_config[name][&#39;weight_scale&#39;] = module.weight_scale
            calibration_config[name][&#39;weight_zero_point&#39;] = module.weight_zero_point

            actual_weight = getattr(module, &#39;old_weight&#39;, None)
            if actual_weight is None:
                logger.warning(&#34;Can not recover weight for layer %s. &#34;
                               &#34;This may lead to a wrong accuracy performance on the backend.&#34;, name)
            delattr(module, &#39;weight&#39;)
            module.register_parameter(&#39;weight&#39;, actual_weight)
            if hasattr(module, BN_FOLD_TAG):
                actual_bias = getattr(module, &#39;old_bias&#39;, None)
                delattr(module, &#39;bias&#39;)
                if actual_bias is not None:
                    module.register_parameter(&#39;bias&#39;, actual_bias)
                else:
                    setattr(module, &#39;bias&#39;, None)

        if module.layer_quant_setting.input:
            calibration_config[name][&#39;input_bits&#39;] = int(module.layer_quant_setting.input.bits)
            calibration_config[name][&#39;tracked_min_input&#39;] = float(module.tracked_min_input)
            calibration_config[name][&#39;tracked_max_input&#39;] = float(module.tracked_max_input)

        if module.layer_quant_setting.output:
            calibration_config[name][&#39;output_bits&#39;] = int(module.layer_quant_setting.output.bits)
            calibration_config[name][&#39;tracked_min_output&#39;] = float(module.tracked_min_output)
            calibration_config[name][&#39;tracked_max_output&#39;] = float(module.tracked_max_output)
        self._del_simulated_attr(module)

    self.export_model_save(self.bound_model, model_path, calibration_config, calibration_path, onnx_path, input_shape, device)

    return calibration_config</code></pre>
</details>
</dd>
<dt id="optimization.quantization.quant.qat.QAT_Quantizer.step_with_optimizer"><code class="name flex">
<span>def <span class="ident">step_with_optimizer</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>override <code>compressor</code> <code>step</code> method, quantization only happens after certain number of steps</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step_with_optimizer(self):
    &#34;&#34;&#34;
    override `compressor` `step` method, quantization only happens after certain number of steps
    &#34;&#34;&#34;
    self.bound_model.steps.add_(1)</code></pre>
</details>
</dd>
<dt id="optimization.quantization.quant.qat.QAT_Quantizer.validate_config"><code class="name flex">
<span>def <span class="ident">validate_config</span></span>(<span>self, model, config_list)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>torch.nn.Module</code></dt>
<dd>Model to be pruned</dd>
<dt><strong><code>config_list</code></strong> :&ensp;<code>list</code> of <code>dict</code></dt>
<dd>List of configurations</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def validate_config(self, model, config_list):
    &#34;&#34;&#34;
    Parameters
    ----------
    model : torch.nn.Module
        Model to be pruned
    config_list : list of dict
        List of configurations
    &#34;&#34;&#34;
    SUPPORTED_OPS = [&#39;Conv2d&#39;, &#39;Linear&#39;, &#39;ReLU&#39;, &#39;ReLU6&#39;]
    schema = QuantizerSchema([{
        Optional(&#39;quant_types&#39;): Schema([lambda x: x in [&#39;weight&#39;, &#39;output&#39;, &#39;input&#39;]]),
        Optional(&#39;quant_bits&#39;): Or(And(int, lambda n: 0 &lt; n &lt; 32), Schema({
            Optional(&#39;input&#39;): And(int, lambda n: 0 &lt; n &lt; 32),
            Optional(&#39;weight&#39;): And(int, lambda n: 0 &lt; n &lt; 32),
            Optional(&#39;output&#39;): And(int, lambda n: 0 &lt; n &lt; 32),
        })),
        Optional(&#39;quant_scheme&#39;): Or(lambda x: x in QuantScheme, Schema({
            Optional(&#39;input&#39;): lambda x: x in QuantScheme,
            Optional(&#39;weight&#39;): lambda x: x in QuantScheme,
            Optional(&#39;output&#39;): lambda x: x in QuantScheme
        })),
        Optional(&#39;quant_dtype&#39;): Or(lambda x: x in QuantDtype, Schema({
            Optional(&#39;input&#39;): lambda x: x in QuantDtype,
            Optional(&#39;weight&#39;): lambda x: x in QuantDtype,
            Optional(&#39;output&#39;): lambda x: x in QuantDtype
        })),
        Optional(&#39;quant_start_step&#39;): And(int, lambda n: n &gt;= 0),
        Optional(&#39;op_types&#39;): [And(str, lambda n: n in SUPPORTED_OPS)],
        Optional(&#39;op_names&#39;): [str],
        Optional(&#39;exclude&#39;): bool
    }], model, logger)

    schema.validate(config_list)</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="optimization.quantization.core.quant.Quantizer" href="../core/quant.html#optimization.quantization.core.quant.Quantizer">Quantizer</a></b></code>:
<ul class="hlist">
<li><code><a title="optimization.quantization.core.quant.Quantizer.compress" href="../../common/base/compressor.html#optimization.common.base.compressor.Compressor.compress">compress</a></code></li>
<li><code><a title="optimization.quantization.core.quant.Quantizer.export_model_save" href="../core/quant.html#optimization.quantization.core.quant.Quantizer.export_model_save">export_model_save</a></code></li>
<li><code><a title="optimization.quantization.core.quant.Quantizer.find_conv_bn_patterns" href="../core/quant.html#optimization.quantization.core.quant.Quantizer.find_conv_bn_patterns">find_conv_bn_patterns</a></code></li>
<li><code><a title="optimization.quantization.core.quant.Quantizer.fold_bn" href="../core/quant.html#optimization.quantization.core.quant.Quantizer.fold_bn">fold_bn</a></code></li>
<li><code><a title="optimization.quantization.core.quant.Quantizer.get_modules_to_compress" href="../../common/base/compressor.html#optimization.common.base.compressor.Compressor.get_modules_to_compress">get_modules_to_compress</a></code></li>
<li><code><a title="optimization.quantization.core.quant.Quantizer.get_modules_wrapper" href="../../common/base/compressor.html#optimization.common.base.compressor.Compressor.get_modules_wrapper">get_modules_wrapper</a></code></li>
<li><code><a title="optimization.quantization.core.quant.Quantizer.load_calibration_config" href="../core/quant.html#optimization.quantization.core.quant.Quantizer.load_calibration_config">load_calibration_config</a></code></li>
<li><code><a title="optimization.quantization.core.quant.Quantizer.quantize_input" href="../core/quant.html#optimization.quantization.core.quant.Quantizer.quantize_input">quantize_input</a></code></li>
<li><code><a title="optimization.quantization.core.quant.Quantizer.quantize_output" href="../core/quant.html#optimization.quantization.core.quant.Quantizer.quantize_output">quantize_output</a></code></li>
<li><code><a title="optimization.quantization.core.quant.Quantizer.quantize_weight" href="../core/quant.html#optimization.quantization.core.quant.Quantizer.quantize_weight">quantize_weight</a></code></li>
<li><code><a title="optimization.quantization.core.quant.Quantizer.record_shape" href="../core/quant.html#optimization.quantization.core.quant.Quantizer.record_shape">record_shape</a></code></li>
<li><code><a title="optimization.quantization.core.quant.Quantizer.reset" href="../../common/base/compressor.html#optimization.common.base.compressor.Compressor.reset">reset</a></code></li>
<li><code><a title="optimization.quantization.core.quant.Quantizer.select_config" href="../../common/base/compressor.html#optimization.common.base.compressor.Compressor.select_config">select_config</a></code></li>
<li><code><a title="optimization.quantization.core.quant.Quantizer.set_wrappers_attribute" href="../../common/base/compressor.html#optimization.common.base.compressor.Compressor.set_wrappers_attribute">set_wrappers_attribute</a></code></li>
<li><code><a title="optimization.quantization.core.quant.Quantizer.update_epoch" href="../../common/base/compressor.html#optimization.common.base.compressor.Compressor.update_epoch">update_epoch</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="optimization.quantization.quant" href="index.html">optimization.quantization.quant</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="optimization.quantization.quant.qat.update_ema" href="#optimization.quantization.quant.qat.update_ema">update_ema</a></code></li>
<li><code><a title="optimization.quantization.quant.qat.update_quantization_param" href="#optimization.quantization.quant.qat.update_quantization_param">update_quantization_param</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="optimization.quantization.quant.qat.QATGrad" href="#optimization.quantization.quant.qat.QATGrad">QATGrad</a></code></h4>
</li>
<li>
<h4><code><a title="optimization.quantization.quant.qat.QAT_Quantizer" href="#optimization.quantization.quant.qat.QAT_Quantizer">QAT_Quantizer</a></code></h4>
<ul class="">
<li><code><a title="optimization.quantization.quant.qat.QAT_Quantizer.export_model" href="#optimization.quantization.quant.qat.QAT_Quantizer.export_model">export_model</a></code></li>
<li><code><a title="optimization.quantization.quant.qat.QAT_Quantizer.step_with_optimizer" href="#optimization.quantization.quant.qat.QAT_Quantizer.step_with_optimizer">step_with_optimizer</a></code></li>
<li><code><a title="optimization.quantization.quant.qat.QAT_Quantizer.validate_config" href="#optimization.quantization.quant.qat.QAT_Quantizer.validate_config">validate_config</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>