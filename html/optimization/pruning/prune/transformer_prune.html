<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>optimization.pruning.prune.transformer_prune API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>optimization.pruning.prune.transformer_prune</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import logging
from schema import And, Optional

from optimization.common.graph import TorchModuleGraph
from optimization.pruning.core import AttentionWeightDependency
from optimization.common.base import CompressorSchema
from optimization.pruning.core import Pruner
from optimization.pruning.core import L1WeightHeadMasker, L2WeightHeadMasker, L1ActivationHeadMasker, L2ActivationHeadMasker, TaylorFOHeadMasker

__all__ = [&#39;TransformerHeadPruner&#39;]

MASKER_DICT = {
    &#39;l1_weight&#39;: L1WeightHeadMasker,
    &#39;l2_weight&#39;: L2WeightHeadMasker,
    &#39;l1_activation&#39;: L1ActivationHeadMasker,
    &#39;l2_activation&#39;: L2ActivationHeadMasker,
    &#39;taylorfo&#39;: TaylorFOHeadMasker
}

logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)


class TransformerHeadPruner(Pruner):
    &#34;&#34;&#34;
    A pruner specialized for pruning attention heads in models belong to the transformer family.

    Parameters
    ----------
    model : torch.nn.Module
        Model to be pruned. Expect a model from transformers library (e.g., BertModel).
        This pruner can work with other customized transformer models, but some ranking modes might fail.
    config_list : list
        Supported keys:
            - sparsity : This is to specify the sparsity operations to be compressed to.
            - op_types : Optional. Operation types to prune. (Should be &#39;Linear&#39; for this pruner.)
            - op_names : Optional. Operation names to prune.
    head_hidden_dim : int
        Dimension of the hidden dimension of each attention head. (e.g., 64 for BERT)
        We assume that this head_hidden_dim is constant across the entire model.
    attention_name_groups : list (Optional)
        List of groups of names for weights of each attention layer. Each element should be a four-element list, with
        the first three corresponding to Q_proj, K_proj, V_proj (in any order) and the last one being output_proj.
    dummy_input : torch.Tensor (Optional)
        Input to model&#39;s forward method, used to infer module grouping if attention_name_groups is not specified.
        This tensor is used by the underlying torch.jit.trace to infer the module graph.
    ranking_criterion : str
        The criterion for ranking attention heads. Currently we support:
            - l1_weight: l1 norm of Q_proj, K_proj, and V_proj
            - l2_weight: l2 norm of Q_proj, K_proj, and V_proj
            - l1_activation: l1 norm of the output of attention computation
            - l2_activation: l2 norm of the output of attention computation
            - taylorfo: l1 norm of the output of attention computation * gradient for this output
                        (check more details in the masker documentation)
    global_sort : bool
        Whether rank the heads globally or locally before deciding heads to prune.
    num_iterations : int
        Number of pruning iterations. Defaults to 1 (ont-shot pruning). If num_iterations &gt; 1, the pruner will split
        the sparsity specified in config_list uniformly and assign a fraction to each pruning iteration.
    epochs_per_iteration : int
        Number of finetuning epochs before the next pruning iteration.
        Only used when num_iterations &gt; 1.
        If num_iterations is 1, then no finetuning is performed by the pruner after pruning.
    optimizer: torch.optim.Optimizer
        Optimizer used to train model
    trainer: function
        Function used to finetune the model between pruning iterations.
        Only used when  num_iterations &gt; 1 or ranking_criterion is &#39;taylorfo&#39;.
        Users should write this function as a normal function to train the PyTorch model and include
        `model, optimizer, criterion, epoch` as function arguments. Note that the trainer is also used for collecting
        gradients for pruning if ranking_criterion is &#39;taylorfo&#39;. In that case, ``epoch=None`` will be passed.
    criterion: function
        Function used to calculate the loss between the target and the output.
        Only used when  num_iterations &gt; 1 or ranking_criterion is &#39;taylorfo&#39;.
        For example, you can use ``torch.nn.CrossEntropyLoss()`` as input.
    forward_runner: function
        Function used to perform a &#34;dry run&#34; on the model on the entire train/validation dataset in order to collect
        data for pruning required by the criteria &#39;l1_activation&#39; or &#39;l2_activation&#39;.
        Only used when ranking_criterion is &#39;l1_activation&#39; or &#39;l2_activation&#39;.
        Users should write this function as a normal function that accepts a PyTorch model and runs forward on the model
        using the entire train/validation dataset. This function is not expected to perform any backpropagation or
        parameter updates.
    &#34;&#34;&#34;
    def __init__(self, model, config_list,head_hidden_dim, attention_name_groups=None, dummy_input=None,
                 ranking_criterion=&#39;l1_weight&#39;, global_sort=False, num_iterations=1, epochs_per_iteration=1,
                 optimizer=None, trainer=None, criterion=None, forward_runner=None,
                 **algo_kwargs):
        super().__init__(model, config_list)

        self.head_hidden_dim = int(head_hidden_dim)
        self.attention_name_groups = attention_name_groups
        self.dummy_input = dummy_input
        self.ranking_criterion = ranking_criterion
        assert self.ranking_criterion in [&#39;l1_weight&#39;, &#39;l2_weight&#39;, &#39;l1_activation&#39;, &#39;l2_activation&#39;, &#39;taylorfo&#39;], \
            &#34;Unsupported ranking criteria.&#34;
        self.global_sort = global_sort
        self.num_iterations = int(num_iterations)
        assert self.num_iterations &gt;= 1, &#34;num_iterations must be greater than or equal to 1&#34;
        self.epochs_per_iteration = int(epochs_per_iteration)
        self._optimizer = optimizer
        self._trainer = trainer
        self._criterion = criterion
        self._forward_runner = forward_runner
        if self.ranking_criterion in [&#39;taylorfo&#39;] or num_iterations &gt; 1:
            assert self._trainer is not None
            assert self._optimizer is not None
        if self.ranking_criterion in [&#39;l1_activation&#39;, &#39;l2_activation&#39;]:
            assert self._forward_runner is not None

        self.masking_groups = []
        if self.attention_name_groups is not None:
            logger.info(&#34;Note: weights for the same attention layer are grouped using the given attention_name_groups.&#34;)
            self.group_weights_by_name()
        else:
            assert self.dummy_input is not None
            logger.info(&#34;Note: weights for the same attention layer are grouped using model graph.&#34;)
            self._unwrap_model()
            self.group_weight_names_by_graph()
            self._wrap_model()

        self.validate_weight_groups()

        self._unwrap_model()
        self.remove_ungrouped_modules()
        self._wrap_model()

        self.masker = MASKER_DICT[ranking_criterion](model, self, self.head_hidden_dim, **algo_kwargs)
        self.pruned_heads = {i: set() for i in range(len(self.masking_groups))}

    def group_weights_by_name(self):
        &#34;&#34;&#34;
        Populate self.masking_groups using the groups specified by user in attention_name_groups.
        &#34;&#34;&#34;
        assert len(self.masking_groups) == 0
        name2group = {}
        for layer_idx, layer in enumerate(self.attention_name_groups):
            errmsg = &#39;Each name group must contain 4 weights, with the first three corresponding to Q_proj, K_proj, &#39; \
                     &#39;V_proj (in any order) and the last one being output_proj.&#39;
            assert len(layer) == 4, errmsg
            self.masking_groups.append([])
            for weight in layer:
                name2group[weight] = layer_idx

        for wrapper in self.get_modules_wrapper():
            if wrapper.name in name2group:
                wrapper.group_idx = name2group[wrapper.name]
                self.masking_groups[name2group[wrapper.name]].append(wrapper)

        logger.info(&#39;Grouping updated:&#39;)
        logger.info([[x.name for x in group] for group in self.masking_groups])

    def group_weight_names_by_graph(self):
        &#34;&#34;&#34;
        Populate self.attention_name_groups by running inference on the module graph.
        Currently, the group inferred AttentionWeightDependency is limited to a set of four weights, with the first
        three corresponding to Q_proj, K_proj, V_proj (in any order) and the last one being output_proj.
        &#34;&#34;&#34;
        try:
            module_graph = TorchModuleGraph(self.bound_model, self.dummy_input)
            dependency_tracer = AttentionWeightDependency(traced_model=module_graph.trace)
            self.attention_name_groups = dependency_tracer.dependency_sets
            self.group_weights_by_name()

        except Exception as e:
            raise RuntimeError(&#39;Graph trace failed: please check dummy_input, or specify attention_name_groups.\n&#39;
                               &#39;Exception message: &#39; + str(e))

    def validate_weight_groups(self):
        &#34;&#34;&#34;
        Sanity checks:
            - Q, K, V projection weights in each groups must have the same shape
            - output projection weight shape must match total hidden dimension (inferred from Q, K, V projection)
            - Four weights in a group must have the same sparsity in their config
            - If global_sort is specified, all weights must have the same sparsity
            - head_hidden_dim must be a divisor of the output dimension of the projection weights (i.e., the resulting
              head number must be an integer)
        &#34;&#34;&#34;
        errmsg = &#39;Attention weight group sanity check not passed&#39;
        sparsity = None
        for group in self.masking_groups:
            if len(group) == 0:
                continue
            assert len(group) == 4, errmsg + &#39;: each group must have four weights&#39;
            assert group[0].module.weight.size() == group[1].module.weight.size() and \
                group[1].module.weight.size() == group[2].module.weight.size(), \
                errmsg + &#39;: the dimensions of Q, K, V projection matrices must be the same &#39;
            assert group[0].module.weight.size()[0] == group[3].module.weight.size()[1], \
                errmsg + &#39;: the dimension of attention results must match with input for output projection&#39;
            assert group[0].config[&#39;sparsity&#39;] == group[1].config[&#39;sparsity&#39;] == \
                   group[2].config[&#39;sparsity&#39;] == group[3].config[&#39;sparsity&#39;], \
                errmsg + &#39;: the sparsity of matrices in the same layer must be the same&#39;
            if sparsity is None:
                sparsity = group[0].config[&#39;sparsity&#39;]
            if self.global_sort:
                assert sparsity == group[0].config[&#39;sparsity&#39;], \
                    errmsg + &#39;: for global_sort=True, the sparsity for all modules must be the same&#39;
            assert group[0].module.weight.size(0) % self.head_hidden_dim == 0, \
                errmsg + &#39;: head_hidden_dim must be a divisor of the output dimension of the projection weights&#39;

    def remove_ungrouped_modules(self):
        &#34;&#34;&#34;
        Remove non-attention weights that might be mistakenly captured by a simplified config_list.
        Also update the corresponding list of layer information (self.modules_to_compress)
        &#34;&#34;&#34;
        care_of_modules = set([x for layer in self.masking_groups for x in layer])

        modules_wrapper_new, modules_to_compress_new = [], []
        for wrapper, layer_info in zip(self.modules_wrapper, self.modules_to_compress):
            if wrapper in care_of_modules:
                modules_wrapper_new.append(wrapper)
                modules_to_compress_new.append(layer_info)

        self.modules_wrapper = modules_wrapper_new
        self.modules_to_compress = modules_to_compress_new

    def validate_config(self, model, config_list):
        &#34;&#34;&#34;
        Parameters
        ----------
        model : torch.nn.Module
            Model to be pruned
        config_list : list
            List on pruning configs
        &#34;&#34;&#34;
        schema = CompressorSchema([{
            &#39;sparsity&#39;: And(float, lambda n: 0 &lt; n &lt; 1),
            Optional(&#39;op_types&#39;): [str],
            Optional(&#39;op_names&#39;): [str]
        }], model, logger)

        schema.validate(config_list)

    def compress(self):
        for pruning_iter in range(self.num_iterations):
            if self.ranking_criterion in [&#39;l1_activation&#39;, &#39;l2_activation&#39;]:
                training = self.bound_model.training
                self.bound_model.eval()
                self._forward_runner(self.bound_model)       # dry run, forward only
                self.update_mask()
                self.bound_model.train(training)
            elif self.ranking_criterion in [&#39;taylorfo&#39;]:
                self._trainer(self.bound_model, optimizer=self._optimizer, criterion=self._criterion, epoch=None)
                self.update_mask()
            else:
                self.update_mask()
            if self.num_iterations &gt; 1 and pruning_iter != self.num_iterations - 1:
                for e in range(self.epochs_per_iteration):
                    self._trainer(self.bound_model, optimizer=self._optimizer, criterion=self._criterion, epoch=e+1)
                self.masker.reset()

            logger.info(&#39;Pruned heads after iteration %i&#39;, pruning_iter)
            logger.info(self.pruned_heads)

    def update_mask(self):
        &#34;&#34;&#34;
        Calculate and update masks for each masking group. If global_sort is set, the masks for all groups are
        calculated altogether, and then the groups are updated individually.
        &#34;&#34;&#34;
        masks_for_all_groups = None
        if self.global_sort:
            masks_for_all_groups = self._calc_mask_global()
            assert len(masks_for_all_groups) == len(self.masking_groups)
        for group_idx, layer_weight_group in enumerate(self.masking_groups):
            if self.global_sort:
                masks = masks_for_all_groups[group_idx]
            else:
                masks = self._calc_mask(layer_weight_group)
            if masks is not None:
                for i, mask in enumerate(masks):
                    for mask_type in mask:
                        assert hasattr(layer_weight_group[i], mask_type), \
                            &#34;there is no attribute &#39;%s&#39; in wrapper on %s&#34; % (mask_type, layer_weight_group[i])
                        setattr(layer_weight_group[i], mask_type, mask[mask_type])
                        logger.debug(f&#39;mask updated: {layer_weight_group[i].name} {mask_type}&#39;)

    def _calc_mask(self, weight_group):
        &#34;&#34;&#34;
        Calculate mask for each group using only layer-local information.
        When global_sort is set for the pruner, _calc_mask_global should be called instead of this function.

        Parameters
        ----------
        weight_group : list
            A list of four wrappers generated by self.group_weights_by_name().

        Returns
        -------
        masks : list
            A four element list corresponding to the masks for each element in the four-element weight group.
            Each element in masks is a dict with keys &#34;weight_mask&#34; and &#34;bias_mask&#34; (optional).
            masks can be None if the underlying masker returns None. This means that the mask calculation fails.
            The calling function can try recalculate the mask at a later time. Note that the calling function might need
            to call masker.reset() before attempting to recalculate the mask.
        &#34;&#34;&#34;
        iter_sparsity = weight_group[0].config[&#39;sparsity&#39;] / self.num_iterations
        masks = self.masker.calc_mask(sparsity=iter_sparsity, weight_group=weight_group)

        return masks

    def _calc_mask_global(self):
        &#34;&#34;&#34;
        Calculate mask for all groups using global information.

        Returns
        -------
        masks_list : list
            A list corresponding to the masks for each weight group in self.masking_groups. Each element in the
            returned mask_list is a four-element list corresponding to the masks for each element in a four-element
            weight group.
        &#34;&#34;&#34;
        if len(self.get_modules_wrapper()) == 0:
            return []

        overall_sparsity = self.get_modules_wrapper()[0].config[&#39;sparsity&#39;] / self.num_iterations
        n_heads_total = 0
        for group in self.masking_groups:
            if len(group) != 0:
                q_proj, _, _, _ = group
                n_heads_total += int(q_proj.module.weight.size()[0] / self.head_hidden_dim)
        n_heads_to_prune = int(n_heads_total * overall_sparsity)

        return self.masker.calc_mask_global(n_heads_to_prune)

    def calc_mask(self, wrapper, **kwargs):
        raise RuntimeError(&#34;Applications should directly call TransformerHeadPruner&#39;s update_mask() method.&#34;)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="optimization.pruning.prune.transformer_prune.TransformerHeadPruner"><code class="flex name class">
<span>class <span class="ident">TransformerHeadPruner</span></span>
<span>(</span><span>model, config_list, head_hidden_dim, attention_name_groups=None, dummy_input=None, ranking_criterion='l1_weight', global_sort=False, num_iterations=1, epochs_per_iteration=1, optimizer=None, trainer=None, criterion=None, forward_runner=None, **algo_kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>A pruner specialized for pruning attention heads in models belong to the transformer family.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>torch.nn.Module</code></dt>
<dd>Model to be pruned. Expect a model from transformers library (e.g., BertModel).
This pruner can work with other customized transformer models, but some ranking modes might fail.</dd>
<dt><strong><code>config_list</code></strong> :&ensp;<code>list</code></dt>
<dd>Supported keys:
- sparsity : This is to specify the sparsity operations to be compressed to.
- op_types : Optional. Operation types to prune. (Should be 'Linear' for this pruner.)
- op_names : Optional. Operation names to prune.</dd>
<dt><strong><code>head_hidden_dim</code></strong> :&ensp;<code>int</code></dt>
<dd>Dimension of the hidden dimension of each attention head. (e.g., 64 for BERT)
We assume that this head_hidden_dim is constant across the entire model.</dd>
<dt><strong><code>attention_name_groups</code></strong> :&ensp;<code>list (Optional)</code></dt>
<dd>List of groups of names for weights of each attention layer. Each element should be a four-element list, with
the first three corresponding to Q_proj, K_proj, V_proj (in any order) and the last one being output_proj.</dd>
<dt><strong><code>dummy_input</code></strong> :&ensp;<code>torch.Tensor (Optional)</code></dt>
<dd>Input to model's forward method, used to infer module grouping if attention_name_groups is not specified.
This tensor is used by the underlying torch.jit.trace to infer the module graph.</dd>
<dt><strong><code>ranking_criterion</code></strong> :&ensp;<code>str</code></dt>
<dd>The criterion for ranking attention heads. Currently we support:
- l1_weight: l1 norm of Q_proj, K_proj, and V_proj
- l2_weight: l2 norm of Q_proj, K_proj, and V_proj
- l1_activation: l1 norm of the output of attention computation
- l2_activation: l2 norm of the output of attention computation
- taylorfo: l1 norm of the output of attention computation * gradient for this output
(check more details in the masker documentation)</dd>
<dt><strong><code>global_sort</code></strong> :&ensp;<code>bool</code></dt>
<dd>Whether rank the heads globally or locally before deciding heads to prune.</dd>
<dt><strong><code>num_iterations</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of pruning iterations. Defaults to 1 (ont-shot pruning). If num_iterations &gt; 1, the pruner will split
the sparsity specified in config_list uniformly and assign a fraction to each pruning iteration.</dd>
<dt><strong><code>epochs_per_iteration</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of finetuning epochs before the next pruning iteration.
Only used when num_iterations &gt; 1.
If num_iterations is 1, then no finetuning is performed by the pruner after pruning.</dd>
<dt><strong><code>optimizer</code></strong> :&ensp;<code>torch.optim.Optimizer</code></dt>
<dd>Optimizer used to train model</dd>
<dt><strong><code>trainer</code></strong> :&ensp;<code>function</code></dt>
<dd>Function used to finetune the model between pruning iterations.
Only used when
num_iterations &gt; 1 or ranking_criterion is 'taylorfo'.
Users should write this function as a normal function to train the PyTorch model and include
<code>model, optimizer, criterion, epoch</code> as function arguments. Note that the trainer is also used for collecting
gradients for pruning if ranking_criterion is 'taylorfo'. In that case, <code>epoch=None</code> will be passed.</dd>
<dt><strong><code>criterion</code></strong> :&ensp;<code>function</code></dt>
<dd>Function used to calculate the loss between the target and the output.
Only used when
num_iterations &gt; 1 or ranking_criterion is 'taylorfo'.
For example, you can use <code>torch.nn.CrossEntropyLoss()</code> as input.</dd>
<dt><strong><code>forward_runner</code></strong> :&ensp;<code>function</code></dt>
<dd>Function used to perform a "dry run" on the model on the entire train/validation dataset in order to collect
data for pruning required by the criteria 'l1_activation' or 'l2_activation'.
Only used when ranking_criterion is 'l1_activation' or 'l2_activation'.
Users should write this function as a normal function that accepts a PyTorch model and runs forward on the model
using the entire train/validation dataset. This function is not expected to perform any backpropagation or
parameter updates.</dd>
</dl>
<p>Record necessary info in class members</p>
<h2 id="parameters_1">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>pytorch model</code></dt>
<dd>the model user wants to compress</dd>
<dt><strong><code>config_list</code></strong> :&ensp;<code>list</code></dt>
<dd>the configurations that users specify for compression</dd>
<dt><strong><code>optimizer</code></strong> :&ensp;<code>pytorch optimizer</code></dt>
<dd>optimizer used to train the model</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class TransformerHeadPruner(Pruner):
    &#34;&#34;&#34;
    A pruner specialized for pruning attention heads in models belong to the transformer family.

    Parameters
    ----------
    model : torch.nn.Module
        Model to be pruned. Expect a model from transformers library (e.g., BertModel).
        This pruner can work with other customized transformer models, but some ranking modes might fail.
    config_list : list
        Supported keys:
            - sparsity : This is to specify the sparsity operations to be compressed to.
            - op_types : Optional. Operation types to prune. (Should be &#39;Linear&#39; for this pruner.)
            - op_names : Optional. Operation names to prune.
    head_hidden_dim : int
        Dimension of the hidden dimension of each attention head. (e.g., 64 for BERT)
        We assume that this head_hidden_dim is constant across the entire model.
    attention_name_groups : list (Optional)
        List of groups of names for weights of each attention layer. Each element should be a four-element list, with
        the first three corresponding to Q_proj, K_proj, V_proj (in any order) and the last one being output_proj.
    dummy_input : torch.Tensor (Optional)
        Input to model&#39;s forward method, used to infer module grouping if attention_name_groups is not specified.
        This tensor is used by the underlying torch.jit.trace to infer the module graph.
    ranking_criterion : str
        The criterion for ranking attention heads. Currently we support:
            - l1_weight: l1 norm of Q_proj, K_proj, and V_proj
            - l2_weight: l2 norm of Q_proj, K_proj, and V_proj
            - l1_activation: l1 norm of the output of attention computation
            - l2_activation: l2 norm of the output of attention computation
            - taylorfo: l1 norm of the output of attention computation * gradient for this output
                        (check more details in the masker documentation)
    global_sort : bool
        Whether rank the heads globally or locally before deciding heads to prune.
    num_iterations : int
        Number of pruning iterations. Defaults to 1 (ont-shot pruning). If num_iterations &gt; 1, the pruner will split
        the sparsity specified in config_list uniformly and assign a fraction to each pruning iteration.
    epochs_per_iteration : int
        Number of finetuning epochs before the next pruning iteration.
        Only used when num_iterations &gt; 1.
        If num_iterations is 1, then no finetuning is performed by the pruner after pruning.
    optimizer: torch.optim.Optimizer
        Optimizer used to train model
    trainer: function
        Function used to finetune the model between pruning iterations.
        Only used when  num_iterations &gt; 1 or ranking_criterion is &#39;taylorfo&#39;.
        Users should write this function as a normal function to train the PyTorch model and include
        `model, optimizer, criterion, epoch` as function arguments. Note that the trainer is also used for collecting
        gradients for pruning if ranking_criterion is &#39;taylorfo&#39;. In that case, ``epoch=None`` will be passed.
    criterion: function
        Function used to calculate the loss between the target and the output.
        Only used when  num_iterations &gt; 1 or ranking_criterion is &#39;taylorfo&#39;.
        For example, you can use ``torch.nn.CrossEntropyLoss()`` as input.
    forward_runner: function
        Function used to perform a &#34;dry run&#34; on the model on the entire train/validation dataset in order to collect
        data for pruning required by the criteria &#39;l1_activation&#39; or &#39;l2_activation&#39;.
        Only used when ranking_criterion is &#39;l1_activation&#39; or &#39;l2_activation&#39;.
        Users should write this function as a normal function that accepts a PyTorch model and runs forward on the model
        using the entire train/validation dataset. This function is not expected to perform any backpropagation or
        parameter updates.
    &#34;&#34;&#34;
    def __init__(self, model, config_list,head_hidden_dim, attention_name_groups=None, dummy_input=None,
                 ranking_criterion=&#39;l1_weight&#39;, global_sort=False, num_iterations=1, epochs_per_iteration=1,
                 optimizer=None, trainer=None, criterion=None, forward_runner=None,
                 **algo_kwargs):
        super().__init__(model, config_list)

        self.head_hidden_dim = int(head_hidden_dim)
        self.attention_name_groups = attention_name_groups
        self.dummy_input = dummy_input
        self.ranking_criterion = ranking_criterion
        assert self.ranking_criterion in [&#39;l1_weight&#39;, &#39;l2_weight&#39;, &#39;l1_activation&#39;, &#39;l2_activation&#39;, &#39;taylorfo&#39;], \
            &#34;Unsupported ranking criteria.&#34;
        self.global_sort = global_sort
        self.num_iterations = int(num_iterations)
        assert self.num_iterations &gt;= 1, &#34;num_iterations must be greater than or equal to 1&#34;
        self.epochs_per_iteration = int(epochs_per_iteration)
        self._optimizer = optimizer
        self._trainer = trainer
        self._criterion = criterion
        self._forward_runner = forward_runner
        if self.ranking_criterion in [&#39;taylorfo&#39;] or num_iterations &gt; 1:
            assert self._trainer is not None
            assert self._optimizer is not None
        if self.ranking_criterion in [&#39;l1_activation&#39;, &#39;l2_activation&#39;]:
            assert self._forward_runner is not None

        self.masking_groups = []
        if self.attention_name_groups is not None:
            logger.info(&#34;Note: weights for the same attention layer are grouped using the given attention_name_groups.&#34;)
            self.group_weights_by_name()
        else:
            assert self.dummy_input is not None
            logger.info(&#34;Note: weights for the same attention layer are grouped using model graph.&#34;)
            self._unwrap_model()
            self.group_weight_names_by_graph()
            self._wrap_model()

        self.validate_weight_groups()

        self._unwrap_model()
        self.remove_ungrouped_modules()
        self._wrap_model()

        self.masker = MASKER_DICT[ranking_criterion](model, self, self.head_hidden_dim, **algo_kwargs)
        self.pruned_heads = {i: set() for i in range(len(self.masking_groups))}

    def group_weights_by_name(self):
        &#34;&#34;&#34;
        Populate self.masking_groups using the groups specified by user in attention_name_groups.
        &#34;&#34;&#34;
        assert len(self.masking_groups) == 0
        name2group = {}
        for layer_idx, layer in enumerate(self.attention_name_groups):
            errmsg = &#39;Each name group must contain 4 weights, with the first three corresponding to Q_proj, K_proj, &#39; \
                     &#39;V_proj (in any order) and the last one being output_proj.&#39;
            assert len(layer) == 4, errmsg
            self.masking_groups.append([])
            for weight in layer:
                name2group[weight] = layer_idx

        for wrapper in self.get_modules_wrapper():
            if wrapper.name in name2group:
                wrapper.group_idx = name2group[wrapper.name]
                self.masking_groups[name2group[wrapper.name]].append(wrapper)

        logger.info(&#39;Grouping updated:&#39;)
        logger.info([[x.name for x in group] for group in self.masking_groups])

    def group_weight_names_by_graph(self):
        &#34;&#34;&#34;
        Populate self.attention_name_groups by running inference on the module graph.
        Currently, the group inferred AttentionWeightDependency is limited to a set of four weights, with the first
        three corresponding to Q_proj, K_proj, V_proj (in any order) and the last one being output_proj.
        &#34;&#34;&#34;
        try:
            module_graph = TorchModuleGraph(self.bound_model, self.dummy_input)
            dependency_tracer = AttentionWeightDependency(traced_model=module_graph.trace)
            self.attention_name_groups = dependency_tracer.dependency_sets
            self.group_weights_by_name()

        except Exception as e:
            raise RuntimeError(&#39;Graph trace failed: please check dummy_input, or specify attention_name_groups.\n&#39;
                               &#39;Exception message: &#39; + str(e))

    def validate_weight_groups(self):
        &#34;&#34;&#34;
        Sanity checks:
            - Q, K, V projection weights in each groups must have the same shape
            - output projection weight shape must match total hidden dimension (inferred from Q, K, V projection)
            - Four weights in a group must have the same sparsity in their config
            - If global_sort is specified, all weights must have the same sparsity
            - head_hidden_dim must be a divisor of the output dimension of the projection weights (i.e., the resulting
              head number must be an integer)
        &#34;&#34;&#34;
        errmsg = &#39;Attention weight group sanity check not passed&#39;
        sparsity = None
        for group in self.masking_groups:
            if len(group) == 0:
                continue
            assert len(group) == 4, errmsg + &#39;: each group must have four weights&#39;
            assert group[0].module.weight.size() == group[1].module.weight.size() and \
                group[1].module.weight.size() == group[2].module.weight.size(), \
                errmsg + &#39;: the dimensions of Q, K, V projection matrices must be the same &#39;
            assert group[0].module.weight.size()[0] == group[3].module.weight.size()[1], \
                errmsg + &#39;: the dimension of attention results must match with input for output projection&#39;
            assert group[0].config[&#39;sparsity&#39;] == group[1].config[&#39;sparsity&#39;] == \
                   group[2].config[&#39;sparsity&#39;] == group[3].config[&#39;sparsity&#39;], \
                errmsg + &#39;: the sparsity of matrices in the same layer must be the same&#39;
            if sparsity is None:
                sparsity = group[0].config[&#39;sparsity&#39;]
            if self.global_sort:
                assert sparsity == group[0].config[&#39;sparsity&#39;], \
                    errmsg + &#39;: for global_sort=True, the sparsity for all modules must be the same&#39;
            assert group[0].module.weight.size(0) % self.head_hidden_dim == 0, \
                errmsg + &#39;: head_hidden_dim must be a divisor of the output dimension of the projection weights&#39;

    def remove_ungrouped_modules(self):
        &#34;&#34;&#34;
        Remove non-attention weights that might be mistakenly captured by a simplified config_list.
        Also update the corresponding list of layer information (self.modules_to_compress)
        &#34;&#34;&#34;
        care_of_modules = set([x for layer in self.masking_groups for x in layer])

        modules_wrapper_new, modules_to_compress_new = [], []
        for wrapper, layer_info in zip(self.modules_wrapper, self.modules_to_compress):
            if wrapper in care_of_modules:
                modules_wrapper_new.append(wrapper)
                modules_to_compress_new.append(layer_info)

        self.modules_wrapper = modules_wrapper_new
        self.modules_to_compress = modules_to_compress_new

    def validate_config(self, model, config_list):
        &#34;&#34;&#34;
        Parameters
        ----------
        model : torch.nn.Module
            Model to be pruned
        config_list : list
            List on pruning configs
        &#34;&#34;&#34;
        schema = CompressorSchema([{
            &#39;sparsity&#39;: And(float, lambda n: 0 &lt; n &lt; 1),
            Optional(&#39;op_types&#39;): [str],
            Optional(&#39;op_names&#39;): [str]
        }], model, logger)

        schema.validate(config_list)

    def compress(self):
        for pruning_iter in range(self.num_iterations):
            if self.ranking_criterion in [&#39;l1_activation&#39;, &#39;l2_activation&#39;]:
                training = self.bound_model.training
                self.bound_model.eval()
                self._forward_runner(self.bound_model)       # dry run, forward only
                self.update_mask()
                self.bound_model.train(training)
            elif self.ranking_criterion in [&#39;taylorfo&#39;]:
                self._trainer(self.bound_model, optimizer=self._optimizer, criterion=self._criterion, epoch=None)
                self.update_mask()
            else:
                self.update_mask()
            if self.num_iterations &gt; 1 and pruning_iter != self.num_iterations - 1:
                for e in range(self.epochs_per_iteration):
                    self._trainer(self.bound_model, optimizer=self._optimizer, criterion=self._criterion, epoch=e+1)
                self.masker.reset()

            logger.info(&#39;Pruned heads after iteration %i&#39;, pruning_iter)
            logger.info(self.pruned_heads)

    def update_mask(self):
        &#34;&#34;&#34;
        Calculate and update masks for each masking group. If global_sort is set, the masks for all groups are
        calculated altogether, and then the groups are updated individually.
        &#34;&#34;&#34;
        masks_for_all_groups = None
        if self.global_sort:
            masks_for_all_groups = self._calc_mask_global()
            assert len(masks_for_all_groups) == len(self.masking_groups)
        for group_idx, layer_weight_group in enumerate(self.masking_groups):
            if self.global_sort:
                masks = masks_for_all_groups[group_idx]
            else:
                masks = self._calc_mask(layer_weight_group)
            if masks is not None:
                for i, mask in enumerate(masks):
                    for mask_type in mask:
                        assert hasattr(layer_weight_group[i], mask_type), \
                            &#34;there is no attribute &#39;%s&#39; in wrapper on %s&#34; % (mask_type, layer_weight_group[i])
                        setattr(layer_weight_group[i], mask_type, mask[mask_type])
                        logger.debug(f&#39;mask updated: {layer_weight_group[i].name} {mask_type}&#39;)

    def _calc_mask(self, weight_group):
        &#34;&#34;&#34;
        Calculate mask for each group using only layer-local information.
        When global_sort is set for the pruner, _calc_mask_global should be called instead of this function.

        Parameters
        ----------
        weight_group : list
            A list of four wrappers generated by self.group_weights_by_name().

        Returns
        -------
        masks : list
            A four element list corresponding to the masks for each element in the four-element weight group.
            Each element in masks is a dict with keys &#34;weight_mask&#34; and &#34;bias_mask&#34; (optional).
            masks can be None if the underlying masker returns None. This means that the mask calculation fails.
            The calling function can try recalculate the mask at a later time. Note that the calling function might need
            to call masker.reset() before attempting to recalculate the mask.
        &#34;&#34;&#34;
        iter_sparsity = weight_group[0].config[&#39;sparsity&#39;] / self.num_iterations
        masks = self.masker.calc_mask(sparsity=iter_sparsity, weight_group=weight_group)

        return masks

    def _calc_mask_global(self):
        &#34;&#34;&#34;
        Calculate mask for all groups using global information.

        Returns
        -------
        masks_list : list
            A list corresponding to the masks for each weight group in self.masking_groups. Each element in the
            returned mask_list is a four-element list corresponding to the masks for each element in a four-element
            weight group.
        &#34;&#34;&#34;
        if len(self.get_modules_wrapper()) == 0:
            return []

        overall_sparsity = self.get_modules_wrapper()[0].config[&#39;sparsity&#39;] / self.num_iterations
        n_heads_total = 0
        for group in self.masking_groups:
            if len(group) != 0:
                q_proj, _, _, _ = group
                n_heads_total += int(q_proj.module.weight.size()[0] / self.head_hidden_dim)
        n_heads_to_prune = int(n_heads_total * overall_sparsity)

        return self.masker.calc_mask_global(n_heads_to_prune)

    def calc_mask(self, wrapper, **kwargs):
        raise RuntimeError(&#34;Applications should directly call TransformerHeadPruner&#39;s update_mask() method.&#34;)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="optimization.pruning.core.pruner.Pruner" href="../core/pruner.html#optimization.pruning.core.pruner.Pruner">Pruner</a></li>
<li><a title="optimization.common.base.compressor.Compressor" href="../../common/base/compressor.html#optimization.common.base.compressor.Compressor">Compressor</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="optimization.pruning.prune.transformer_prune.TransformerHeadPruner.group_weight_names_by_graph"><code class="name flex">
<span>def <span class="ident">group_weight_names_by_graph</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Populate self.attention_name_groups by running inference on the module graph.
Currently, the group inferred AttentionWeightDependency is limited to a set of four weights, with the first
three corresponding to Q_proj, K_proj, V_proj (in any order) and the last one being output_proj.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def group_weight_names_by_graph(self):
    &#34;&#34;&#34;
    Populate self.attention_name_groups by running inference on the module graph.
    Currently, the group inferred AttentionWeightDependency is limited to a set of four weights, with the first
    three corresponding to Q_proj, K_proj, V_proj (in any order) and the last one being output_proj.
    &#34;&#34;&#34;
    try:
        module_graph = TorchModuleGraph(self.bound_model, self.dummy_input)
        dependency_tracer = AttentionWeightDependency(traced_model=module_graph.trace)
        self.attention_name_groups = dependency_tracer.dependency_sets
        self.group_weights_by_name()

    except Exception as e:
        raise RuntimeError(&#39;Graph trace failed: please check dummy_input, or specify attention_name_groups.\n&#39;
                           &#39;Exception message: &#39; + str(e))</code></pre>
</details>
</dd>
<dt id="optimization.pruning.prune.transformer_prune.TransformerHeadPruner.group_weights_by_name"><code class="name flex">
<span>def <span class="ident">group_weights_by_name</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Populate self.masking_groups using the groups specified by user in attention_name_groups.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def group_weights_by_name(self):
    &#34;&#34;&#34;
    Populate self.masking_groups using the groups specified by user in attention_name_groups.
    &#34;&#34;&#34;
    assert len(self.masking_groups) == 0
    name2group = {}
    for layer_idx, layer in enumerate(self.attention_name_groups):
        errmsg = &#39;Each name group must contain 4 weights, with the first three corresponding to Q_proj, K_proj, &#39; \
                 &#39;V_proj (in any order) and the last one being output_proj.&#39;
        assert len(layer) == 4, errmsg
        self.masking_groups.append([])
        for weight in layer:
            name2group[weight] = layer_idx

    for wrapper in self.get_modules_wrapper():
        if wrapper.name in name2group:
            wrapper.group_idx = name2group[wrapper.name]
            self.masking_groups[name2group[wrapper.name]].append(wrapper)

    logger.info(&#39;Grouping updated:&#39;)
    logger.info([[x.name for x in group] for group in self.masking_groups])</code></pre>
</details>
</dd>
<dt id="optimization.pruning.prune.transformer_prune.TransformerHeadPruner.remove_ungrouped_modules"><code class="name flex">
<span>def <span class="ident">remove_ungrouped_modules</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Remove non-attention weights that might be mistakenly captured by a simplified config_list.
Also update the corresponding list of layer information (self.modules_to_compress)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def remove_ungrouped_modules(self):
    &#34;&#34;&#34;
    Remove non-attention weights that might be mistakenly captured by a simplified config_list.
    Also update the corresponding list of layer information (self.modules_to_compress)
    &#34;&#34;&#34;
    care_of_modules = set([x for layer in self.masking_groups for x in layer])

    modules_wrapper_new, modules_to_compress_new = [], []
    for wrapper, layer_info in zip(self.modules_wrapper, self.modules_to_compress):
        if wrapper in care_of_modules:
            modules_wrapper_new.append(wrapper)
            modules_to_compress_new.append(layer_info)

    self.modules_wrapper = modules_wrapper_new
    self.modules_to_compress = modules_to_compress_new</code></pre>
</details>
</dd>
<dt id="optimization.pruning.prune.transformer_prune.TransformerHeadPruner.update_mask"><code class="name flex">
<span>def <span class="ident">update_mask</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate and update masks for each masking group. If global_sort is set, the masks for all groups are
calculated altogether, and then the groups are updated individually.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update_mask(self):
    &#34;&#34;&#34;
    Calculate and update masks for each masking group. If global_sort is set, the masks for all groups are
    calculated altogether, and then the groups are updated individually.
    &#34;&#34;&#34;
    masks_for_all_groups = None
    if self.global_sort:
        masks_for_all_groups = self._calc_mask_global()
        assert len(masks_for_all_groups) == len(self.masking_groups)
    for group_idx, layer_weight_group in enumerate(self.masking_groups):
        if self.global_sort:
            masks = masks_for_all_groups[group_idx]
        else:
            masks = self._calc_mask(layer_weight_group)
        if masks is not None:
            for i, mask in enumerate(masks):
                for mask_type in mask:
                    assert hasattr(layer_weight_group[i], mask_type), \
                        &#34;there is no attribute &#39;%s&#39; in wrapper on %s&#34; % (mask_type, layer_weight_group[i])
                    setattr(layer_weight_group[i], mask_type, mask[mask_type])
                    logger.debug(f&#39;mask updated: {layer_weight_group[i].name} {mask_type}&#39;)</code></pre>
</details>
</dd>
<dt id="optimization.pruning.prune.transformer_prune.TransformerHeadPruner.validate_config"><code class="name flex">
<span>def <span class="ident">validate_config</span></span>(<span>self, model, config_list)</span>
</code></dt>
<dd>
<div class="desc"><h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>model</code></strong> :&ensp;<code>torch.nn.Module</code></dt>
<dd>Model to be pruned</dd>
<dt><strong><code>config_list</code></strong> :&ensp;<code>list</code></dt>
<dd>List on pruning configs</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def validate_config(self, model, config_list):
    &#34;&#34;&#34;
    Parameters
    ----------
    model : torch.nn.Module
        Model to be pruned
    config_list : list
        List on pruning configs
    &#34;&#34;&#34;
    schema = CompressorSchema([{
        &#39;sparsity&#39;: And(float, lambda n: 0 &lt; n &lt; 1),
        Optional(&#39;op_types&#39;): [str],
        Optional(&#39;op_names&#39;): [str]
    }], model, logger)

    schema.validate(config_list)</code></pre>
</details>
</dd>
<dt id="optimization.pruning.prune.transformer_prune.TransformerHeadPruner.validate_weight_groups"><code class="name flex">
<span>def <span class="ident">validate_weight_groups</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Sanity checks:
- Q, K, V projection weights in each groups must have the same shape
- output projection weight shape must match total hidden dimension (inferred from Q, K, V projection)
- Four weights in a group must have the same sparsity in their config
- If global_sort is specified, all weights must have the same sparsity
- head_hidden_dim must be a divisor of the output dimension of the projection weights (i.e., the resulting
head number must be an integer)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def validate_weight_groups(self):
    &#34;&#34;&#34;
    Sanity checks:
        - Q, K, V projection weights in each groups must have the same shape
        - output projection weight shape must match total hidden dimension (inferred from Q, K, V projection)
        - Four weights in a group must have the same sparsity in their config
        - If global_sort is specified, all weights must have the same sparsity
        - head_hidden_dim must be a divisor of the output dimension of the projection weights (i.e., the resulting
          head number must be an integer)
    &#34;&#34;&#34;
    errmsg = &#39;Attention weight group sanity check not passed&#39;
    sparsity = None
    for group in self.masking_groups:
        if len(group) == 0:
            continue
        assert len(group) == 4, errmsg + &#39;: each group must have four weights&#39;
        assert group[0].module.weight.size() == group[1].module.weight.size() and \
            group[1].module.weight.size() == group[2].module.weight.size(), \
            errmsg + &#39;: the dimensions of Q, K, V projection matrices must be the same &#39;
        assert group[0].module.weight.size()[0] == group[3].module.weight.size()[1], \
            errmsg + &#39;: the dimension of attention results must match with input for output projection&#39;
        assert group[0].config[&#39;sparsity&#39;] == group[1].config[&#39;sparsity&#39;] == \
               group[2].config[&#39;sparsity&#39;] == group[3].config[&#39;sparsity&#39;], \
            errmsg + &#39;: the sparsity of matrices in the same layer must be the same&#39;
        if sparsity is None:
            sparsity = group[0].config[&#39;sparsity&#39;]
        if self.global_sort:
            assert sparsity == group[0].config[&#39;sparsity&#39;], \
                errmsg + &#39;: for global_sort=True, the sparsity for all modules must be the same&#39;
        assert group[0].module.weight.size(0) % self.head_hidden_dim == 0, \
            errmsg + &#39;: head_hidden_dim must be a divisor of the output dimension of the projection weights&#39;</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="optimization.pruning.core.pruner.Pruner" href="../core/pruner.html#optimization.pruning.core.pruner.Pruner">Pruner</a></b></code>:
<ul class="hlist">
<li><code><a title="optimization.pruning.core.pruner.Pruner.calc_mask" href="../core/pruner.html#optimization.pruning.core.pruner.Pruner.calc_mask">calc_mask</a></code></li>
<li><code><a title="optimization.pruning.core.pruner.Pruner.compress" href="../../common/base/compressor.html#optimization.common.base.compressor.Compressor.compress">compress</a></code></li>
<li><code><a title="optimization.pruning.core.pruner.Pruner.export_model" href="../core/pruner.html#optimization.pruning.core.pruner.Pruner.export_model">export_model</a></code></li>
<li><code><a title="optimization.pruning.core.pruner.Pruner.get_modules_to_compress" href="../../common/base/compressor.html#optimization.common.base.compressor.Compressor.get_modules_to_compress">get_modules_to_compress</a></code></li>
<li><code><a title="optimization.pruning.core.pruner.Pruner.get_modules_wrapper" href="../../common/base/compressor.html#optimization.common.base.compressor.Compressor.get_modules_wrapper">get_modules_wrapper</a></code></li>
<li><code><a title="optimization.pruning.core.pruner.Pruner.get_pruned_weights" href="../core/pruner.html#optimization.pruning.core.pruner.Pruner.get_pruned_weights">get_pruned_weights</a></code></li>
<li><code><a title="optimization.pruning.core.pruner.Pruner.load_model_state_dict" href="../core/pruner.html#optimization.pruning.core.pruner.Pruner.load_model_state_dict">load_model_state_dict</a></code></li>
<li><code><a title="optimization.pruning.core.pruner.Pruner.reset" href="../../common/base/compressor.html#optimization.common.base.compressor.Compressor.reset">reset</a></code></li>
<li><code><a title="optimization.pruning.core.pruner.Pruner.select_config" href="../../common/base/compressor.html#optimization.common.base.compressor.Compressor.select_config">select_config</a></code></li>
<li><code><a title="optimization.pruning.core.pruner.Pruner.set_wrappers_attribute" href="../../common/base/compressor.html#optimization.common.base.compressor.Compressor.set_wrappers_attribute">set_wrappers_attribute</a></code></li>
<li><code><a title="optimization.pruning.core.pruner.Pruner.update_epoch" href="../../common/base/compressor.html#optimization.common.base.compressor.Compressor.update_epoch">update_epoch</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="optimization.pruning.prune" href="index.html">optimization.pruning.prune</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="optimization.pruning.prune.transformer_prune.TransformerHeadPruner" href="#optimization.pruning.prune.transformer_prune.TransformerHeadPruner">TransformerHeadPruner</a></code></h4>
<ul class="">
<li><code><a title="optimization.pruning.prune.transformer_prune.TransformerHeadPruner.group_weight_names_by_graph" href="#optimization.pruning.prune.transformer_prune.TransformerHeadPruner.group_weight_names_by_graph">group_weight_names_by_graph</a></code></li>
<li><code><a title="optimization.pruning.prune.transformer_prune.TransformerHeadPruner.group_weights_by_name" href="#optimization.pruning.prune.transformer_prune.TransformerHeadPruner.group_weights_by_name">group_weights_by_name</a></code></li>
<li><code><a title="optimization.pruning.prune.transformer_prune.TransformerHeadPruner.remove_ungrouped_modules" href="#optimization.pruning.prune.transformer_prune.TransformerHeadPruner.remove_ungrouped_modules">remove_ungrouped_modules</a></code></li>
<li><code><a title="optimization.pruning.prune.transformer_prune.TransformerHeadPruner.update_mask" href="#optimization.pruning.prune.transformer_prune.TransformerHeadPruner.update_mask">update_mask</a></code></li>
<li><code><a title="optimization.pruning.prune.transformer_prune.TransformerHeadPruner.validate_config" href="#optimization.pruning.prune.transformer_prune.TransformerHeadPruner.validate_config">validate_config</a></code></li>
<li><code><a title="optimization.pruning.prune.transformer_prune.TransformerHeadPruner.validate_weight_groups" href="#optimization.pruning.prune.transformer_prune.TransformerHeadPruner.validate_weight_groups">validate_weight_groups</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>